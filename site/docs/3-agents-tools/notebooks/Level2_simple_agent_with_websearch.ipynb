{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level 2: Simple Agent with Web Search\n",
    "\n",
    "This notebook will introduce how to build a simple agent using Llama Stack's agent framework, enhanced with a single tool: the builtin web search tool. This capability will  allow the agent to retrieve up to date external information beyond the limits of its training data. This is an important step toward developing a more capable and autonomous agent. Make sure you have setup your environment OK in [Level 1](Level1_getting_started_with_Llama_Stack.ipynb)\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial will walk you through how to build your own AI agent who can search the web:\n",
    "\n",
    "1. Configure a Llama Stack agent.\n",
    "2. Enhance the agent by providing it access to a specific tool\n",
    "2. Interact with the agent and tests its use of the web search tool.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting this notebook, ensure that you have:\n",
    "- Followed the instructions in the [Setup Guide](./Level1_getting_started_with_Llama_Stack.ipynb) notebook. \n",
    "- A Tavily API key. It is critical for this notebook to run correctly. You can register for one at [https://tavily.com/](https://tavily.com/).\n",
    "\n",
    "Add your TAVILY_SEARCH_API_KEY=\"tvly-dev-your-key\" to the `env.example` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting Up this Notebook\n",
    "We will start with a few imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client import Agent\n",
    "from llama_stack_client.lib.agents.event_logger import EventLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will initialize our environment as described in detail in our [\"Getting Started\" notebook](./Level0_getting_started_with_Llama_Stack.ipynb). Please refer to it for additional explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Llama Stack server\n",
      "Inference Parameters:\n",
      "\tModel: llama3-2-3b\n",
      "\tSampling Parameters: {'strategy': {'type': 'greedy'}, 'max_tokens': 6000}\n",
      "\tstream: True\n"
     ]
    }
   ],
   "source": [
    "# for accessing the environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# for communication with Llama Stack\n",
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "# pretty print of the results returned from the model/agent\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "from src.utils import step_printer\n",
    "from termcolor import cprint\n",
    "\n",
    "base_url = os.getenv(\"REMOTE_BASE_URL\")\n",
    "\n",
    "# Tavily search API key is required for some of our demos and must be provided to the client upon initialization.\n",
    "# We will cover it in the agentic demos that use the respective tool. Please ignore this parameter for all other demos.\n",
    "tavily_search_api_key = os.getenv(\"TAVILY_SEARCH_API_KEY\")\n",
    "if len(tavily_search_api_key) != 41:\n",
    "    raise ValueError(\"Sorry your Tavily Search key seems invalid?\")\n",
    "else:\n",
    "    provider_data = {\"tavily_search_api_key\": tavily_search_api_key}\n",
    "\n",
    "client = LlamaStackClient(\n",
    "    base_url=base_url,\n",
    "    provider_data=provider_data\n",
    ")\n",
    "\n",
    "print(f\"Connected to Llama Stack server\")\n",
    "\n",
    "# model_id for the model you wish to use that is configured with the Llama Stack server\n",
    "model_id = \"llama3-2-3b\" # \"deepseek-r1-0528-qwen3-8b-bnb-4bit\"\n",
    "\n",
    "temperature = float(os.getenv(\"TEMPERATURE\", 0.0))\n",
    "if temperature > 0.0:\n",
    "    top_p = float(os.getenv(\"TOP_P\", 0.95))\n",
    "    strategy = {\"type\": \"top_p\", \"temperature\": temperature, \"top_p\": top_p}\n",
    "else:\n",
    "    strategy = {\"type\": \"greedy\"}\n",
    "\n",
    "max_tokens = 6000\n",
    "\n",
    "# sampling_params will later be used to pass the parameters to Llama Stack Agents/Inference APIs\n",
    "sampling_params = {\n",
    "    \"strategy\": strategy,\n",
    "    \"max_tokens\": max_tokens,\n",
    "}\n",
    "\n",
    "stream = \"True\"\n",
    "\n",
    "print(f\"Inference Parameters:\\n\\tModel: {model_id}\\n\\tSampling Parameters: {sampling_params}\\n\\tstream: {stream}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure an agent for tool use.\n",
    "\n",
    "- **Agent Initialization**: First we create an `Agent` instance with the desired LLM model, agent instructions and tools.\n",
    "\n",
    "- **Instructions**: The `instructions` parameter, also referred to as the system prompt, specifies the agent's role and behavior. In this example, the agent is configured as a helpful web search assistant. It is instructed to use a tool whenever a web search is required and to respond in a friendly and helpful tone.\n",
    "\n",
    "- **Tools**: The `tools` parameter defines the tools available to the agent. In this case, the `builtin::websearch` tool is used, which enables the agent to perform web searches. This tool is essential for retrieving up-to-date information from the web.\n",
    "\n",
    "- **How It Works**: When a user query is provided, the agent processes the input and determines whether a tool is required to fulfill the request. If the query involves retrieving information from the web, the agent invokes the `builtin::websearch` tool. The tool interacts with Tavily Search to fetch real-time data, which is then processed and returned to the user in a friendly and helpful tone. This workflow ensures that the agent can handle a wide range of queries effectively.\n",
    "\n",
    "For more details on the `builtin::websearch` tool and its capabilities, refer to the [Llama-stack tools documentation](https://llama-stack.readthedocs.io/en/latest/building_applications/tools.html#web-search-providers). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    client, \n",
    "    model=model_id,\n",
    "    instructions=\"\"\"You are a helpful websearch assistant. When you are asked to search the latest you must use a tool. \n",
    "            Whenever a tool is called, be sure return the response in a friendly and helpful tone.\n",
    "            \"\"\" ,\n",
    "    tools=[\"builtin::websearch\"],\n",
    "    sampling_params=sampling_params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run the agent.\n",
    "- Populate `user_prompts` with questions that you would like to ask the agent.\n",
    "- Create a unique agent session for this conversation so that it can store metadata and context history in the Llama Stack server.\n",
    "- Finally, display the agent's responses for each query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "\u001b[34mProcessing user query: Whatâ€™s latest in OpenShift?\u001b[0m\n",
      "==================================================\n",
      "\u001b[33minference> \u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:brave_search Args:{'query': 'latest OpenShift news'}\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:brave_search Response:{\"query\": \"latest OpenShift news\", \"top_k\": [{\"title\": \"What's new with Azure Red Hat OpenShift? - Azure Red Hat OpenShift\", \"url\": \"https://learn.microsoft.com/en-us/azure/openshift/azure-redhat-openshift-release-notes\", \"content\": \"Azure Red Hat OpenShift receives improvements on an ongoing basis. To stay up to date with the most recent developments, this article provides you with information about the latest releases. Version 4.17 - June 2025. OpenShift version 4.17 is now available as an Azure Red Hat OpenShift install option.\", \"score\": 0.79522943, \"raw_content\": null}, {\"title\": \"Red Hat's new OpenShift delivers AI, edge and security enhancements\", \"url\": \"https://www.zdnet.com/article/red-hats-new-openshift-delivers-ai-edge-and-security-enhancements/\", \"content\": \"First up, Red Hat introduced OpenShift 4.17. This latest version of Red Hat's Kubernetes distribution is designed to accelerate innovation across hybrid cloud environments without compromising\", \"score\": 0.7892503, \"raw_content\": null}, {\"title\": \"What's new for developers in Red Hat OpenShift 4.19\", \"url\": \"https://developers.redhat.com/articles/2025/06/17/whats-new-developers-red-hat-openshift-419\", \"content\": \"Figure 2: Migrating multiple virtual machines in OpenShift 4.19. For a more detailed description of the latest release, see the release notes.. For VMware administrators interested in migrating your VMs to OpenShift, download the OpenShift Virtualization for VMware administrators cheat sheet.. OpenShift Pipelines 1.18 (Tekton 0.68)\", \"score\": 0.7180613, \"raw_content\": null}, {\"title\": \"Release notes | OpenShift Container Platform | 4.19 | Red Hat Documentation\", \"url\": \"https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html-single/release_notes/index\", \"content\": \"The IBM Power\\u00ae release on OpenShift Container Platform 4.19 adds improvements and new capabilities to OpenShift Container Platform components. This release introduces support for the following features on IBM Power: Expand Compliance Operator support with profiles for Defense Information Systems Agency Security Technical Implementation Guide\", \"score\": 0.70750624, \"raw_content\": null}, {\"title\": \"What's new in Red Hat OpenShift\", \"url\": \"https://www.redhat.com/en/whats-new-red-hat-openshift\", \"content\": \"News and insights; Technical blog; Research; Live AI events; Explore AI at Red Hat; Our portfolio. Red Hat AI; Red Hat Enterprise Linux AI; ... What's New in OpenShift 4.12. SLIDES. What's Next in Red Hat OpenShift Q4 CY 2022 (Briefing date: November 16, 2022) SLIDES. What's new in OpenShift 4.11\", \"score\": 0.65146625, \"raw_content\": null}]}\u001b[0m\n",
      "\u001b[33minference> \u001b[0m\u001b[33mHere\u001b[0m\u001b[33m are\u001b[0m\u001b[33m the\u001b[0m\u001b[33m latest\u001b[0m\u001b[33m news\u001b[0m\u001b[33m and\u001b[0m\u001b[33m updates\u001b[0m\u001b[33m in\u001b[0m\u001b[33m Open\u001b[0m\u001b[33mShift\u001b[0m\u001b[33m:\n",
      "\n",
      "\u001b[0m\u001b[33m*\u001b[0m\u001b[33m Azure\u001b[0m\u001b[33m Red\u001b[0m\u001b[33m Hat\u001b[0m\u001b[33m Open\u001b[0m\u001b[33mShift\u001b[0m\u001b[33m version\u001b[0m\u001b[33m \u001b[0m\u001b[33m4\u001b[0m\u001b[33m.\u001b[0m\u001b[33m17\u001b[0m\u001b[33m is\u001b[0m\u001b[33m now\u001b[0m\u001b[33m available\u001b[0m\u001b[33m,\u001b[0m\u001b[33m providing\u001b[0m\u001b[33m improvements\u001b[0m\u001b[33m and\u001b[0m\u001b[33m new\u001b[0m\u001b[33m capabilities\u001b[0m\u001b[33m for\u001b[0m\u001b[33m hybrid\u001b[0m\u001b[33m cloud\u001b[0m\u001b[33m environments\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m*\u001b[0m\u001b[33m Red\u001b[0m\u001b[33m Hat\u001b[0m\u001b[33m's\u001b[0m\u001b[33m new\u001b[0m\u001b[33m Open\u001b[0m\u001b[33mShift\u001b[0m\u001b[33m delivers\u001b[0m\u001b[33m AI\u001b[0m\u001b[33m,\u001b[0m\u001b[33m edge\u001b[0m\u001b[33m,\u001b[0m\u001b[33m and\u001b[0m\u001b[33m security\u001b[0m\u001b[33m enhancements\u001b[0m\u001b[33m,\u001b[0m\u001b[33m accelerating\u001b[0m\u001b[33m innovation\u001b[0m\u001b[33m across\u001b[0m\u001b[33m hybrid\u001b[0m\u001b[33m cloud\u001b[0m\u001b[33m environments\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m*\u001b[0m\u001b[33m Open\u001b[0m\u001b[33mShift\u001b[0m\u001b[33m \u001b[0m\u001b[33m4\u001b[0m\u001b[33m.\u001b[0m\u001b[33m19\u001b[0m\u001b[33m introduces\u001b[0m\u001b[33m support\u001b[0m\u001b[33m for\u001b[0m\u001b[33m the\u001b[0m\u001b[33m IBM\u001b[0m\u001b[33m Power\u001b[0m\u001b[33m release\u001b[0m\u001b[33m,\u001b[0m\u001b[33m adding\u001b[0m\u001b[33m improvements\u001b[0m\u001b[33m and\u001b[0m\u001b[33m new\u001b[0m\u001b[33m capabilities\u001b[0m\u001b[33m to\u001b[0m\u001b[33m Open\u001b[0m\u001b[33mShift\u001b[0m\u001b[33m Container\u001b[0m\u001b[33m Platform\u001b[0m\u001b[33m components\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m*\u001b[0m\u001b[33m Open\u001b[0m\u001b[33mShift\u001b[0m\u001b[33m Pip\u001b[0m\u001b[33melines\u001b[0m\u001b[33m \u001b[0m\u001b[33m1\u001b[0m\u001b[33m.\u001b[0m\u001b[33m18\u001b[0m\u001b[33m (\u001b[0m\u001b[33mTek\u001b[0m\u001b[33mton\u001b[0m\u001b[33m \u001b[0m\u001b[33m0\u001b[0m\u001b[33m.\u001b[0m\u001b[33m68\u001b[0m\u001b[33m)\u001b[0m\u001b[33m is\u001b[0m\u001b[33m available\u001b[0m\u001b[33m,\u001b[0m\u001b[33m providing\u001b[0m\u001b[33m enhanced\u001b[0m\u001b[33m pipeline\u001b[0m\u001b[33m features\u001b[0m\u001b[33m and\u001b[0m\u001b[33m capabilities\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33mYou\u001b[0m\u001b[33m can\u001b[0m\u001b[33m learn\u001b[0m\u001b[33m more\u001b[0m\u001b[33m about\u001b[0m\u001b[33m these\u001b[0m\u001b[33m updates\u001b[0m\u001b[33m and\u001b[0m\u001b[33m others\u001b[0m\u001b[33m by\u001b[0m\u001b[33m visiting\u001b[0m\u001b[33m the\u001b[0m\u001b[33m official\u001b[0m\u001b[33m Red\u001b[0m\u001b[33m Hat\u001b[0m\u001b[33m Open\u001b[0m\u001b[33mShift\u001b[0m\u001b[33m website\u001b[0m\u001b[33m or\u001b[0m\u001b[33m checking\u001b[0m\u001b[33m out\u001b[0m\u001b[33m the\u001b[0m\u001b[33m release\u001b[0m\u001b[33m notes\u001b[0m\u001b[33m for\u001b[0m\u001b[33m Open\u001b[0m\u001b[33mShift\u001b[0m\u001b[33m \u001b[0m\u001b[33m4\u001b[0m\u001b[33m.\u001b[0m\u001b[33m19\u001b[0m\u001b[33m.\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[30m\u001b[0m"
     ]
    }
   ],
   "source": [
    "user_prompts = [\n",
    "    \"Whatâ€™s latest in OpenShift?\",\n",
    "]\n",
    "for prompt in user_prompts:\n",
    "    print(\"\\n\"+\"=\"*50)\n",
    "    cprint(f\"Processing user query: {prompt}\", \"blue\")\n",
    "    print(\"=\"*50)\n",
    "    session_id = agent.create_session(\"web-session\")\n",
    "    response = agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        session_id=session_id,\n",
    "        stream=stream\n",
    "    )\n",
    "    if stream:\n",
    "        for log in EventLogger().log(response):\n",
    "            log.print()\n",
    "    else:\n",
    "        step_printer(response.steps) # print the steps of an agent's response in a formatted way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Analysis\n",
    "Here, we can observe that the `builtin::websearch` tool is used to perform a web search. Note that the \"type\" of the tool is still reported \"brave_search\" since Llama models have been trained with brave search as a builtin tool. Tavily is just being used in lieu of Brave search.\n",
    "\n",
    "Great! \n",
    "\n",
    "We can see that the model returned relevant and up-to-date information about OpenShift. This is particularly impressive given that the model that we are using here was released in September 2024 and has a knowledge cutoff of December 2023. These results were only possible due to its ability to call tools like web search, demonstrating the agent's capacity to retrieve real-time data effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- We've demonstrated how to set up Llama Stack agents and extended them with builtin tools (like web search) that come prepackaged with Llama Stack.\n",
    "- We've shown that this simple approach can provide significantly increased functionality of existing open source LLM's. \n",
    "- This will serves as a foundational example for the more advanced examples to come involving Agentic RAG, External Tools, and complex agentic patterns.\n",
    "\n",
    "Continue to the [next notebook](./Level3_advanced_agent_with_Prompt_Chaining_and_ReAct.ipynb) to learn how we can upgrade our agents to solve even more complex and multi-step tasks using advanced agentic patterns. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
