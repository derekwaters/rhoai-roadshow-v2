{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6126afe6-2eaf-4830-8946-db33701f0021",
   "metadata": {},
   "source": [
    "# Install requried Python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c05621ab-73fd-4249-9c1d-41bdabe98f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting uv\n",
      "  Downloading uv-0.7.21-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Downloading uv-0.7.21-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.6/18.6 MB\u001b[0m \u001b[31m307.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: uv\n",
      "Successfully installed uv-0.7.21\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Install necessary Python libraries\n",
    "\n",
    "#!pip install -q -r requirements.txt\n",
    "!pip install uv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ae9b1db-5d43-4d2f-a4c3-cb11879d2d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.11.11 environment at: /opt/app-root\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m190 packages\u001b[0m \u001b[2min 2.44s\u001b[0m\u001b[0m                                       \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m153 packages\u001b[0m \u001b[2min 1m 05s\u001b[0m\u001b[0m                                          \n",
      "\u001b[2K░░░░░░░░░░░░░░░░░░░░ [0/153] \u001b[2mInstalling wheels...                               \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m153 packages\u001b[0m \u001b[2min 1m 28s\u001b[0m\u001b[0m                            \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mabsl-py\u001b[0m\u001b[2m==2.3.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maccelerate\u001b[0m\u001b[2m==1.8.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mairportsdata\u001b[0m\u001b[2m==20250706\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mannotated-types\u001b[0m\u001b[2m==0.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mastor\u001b[0m\u001b[2m==0.8.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mblake3\u001b[0m\u001b[2m==1.0.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcachetools\u001b[0m\u001b[2m==6.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mchardet\u001b[0m\u001b[2m==5.2.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mclick\u001b[0m\u001b[2m==8.2.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcloudpickle\u001b[0m\u001b[2m==3.1.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcompressed-tensors\u001b[0m\u001b[2m==0.10.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcupy-cuda12x\u001b[0m\u001b[2m==13.5.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdataproperty\u001b[0m\u001b[2m==1.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdatasets\u001b[0m\u001b[2m==3.6.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdepyf\u001b[0m\u001b[2m==0.18.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdill\u001b[0m\u001b[2m==0.3.8\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdiskcache\u001b[0m\u001b[2m==5.6.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdistro\u001b[0m\u001b[2m==1.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdnspython\u001b[0m\u001b[2m==2.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1meinops\u001b[0m\u001b[2m==0.8.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1memail-validator\u001b[0m\u001b[2m==2.2.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mevaluate\u001b[0m\u001b[2m==0.4.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfastapi\u001b[0m\u001b[2m==0.116.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfastapi-cli\u001b[0m\u001b[2m==0.0.8\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfastapi-cloud-cli\u001b[0m\u001b[2m==0.1.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfastrlock\u001b[0m\u001b[2m==0.8.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfilelock\u001b[0m\u001b[2m==3.18.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2025.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgguf\u001b[0m\u001b[2m==0.17.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgoogleapis-common-protos\u001b[0m\u001b[2m==1.70.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgrpcio\u001b[0m\u001b[2m==1.73.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhf-xet\u001b[0m\u001b[2m==1.1.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttptools\u001b[0m\u001b[2m==0.6.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==0.33.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mimportlib-metadata\u001b[0m\u001b[2m==8.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1minteregular\u001b[0m\u001b[2m==0.3.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjiter\u001b[0m\u001b[2m==0.10.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjoblib\u001b[0m\u001b[2m==1.5.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjsonlines\u001b[0m\u001b[2m==4.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlark\u001b[0m\u001b[2m==1.2.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mllguidance\u001b[0m\u001b[2m==0.7.30\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mllmcompressor\u001b[0m\u001b[2m==0.5.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mllvmlite\u001b[0m\u001b[2m==0.44.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlm-eval\u001b[0m\u001b[2m==0.4.7\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlm-format-enforcer\u001b[0m\u001b[2m==0.10.11\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mloguru\u001b[0m\u001b[2m==0.7.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlxml\u001b[0m\u001b[2m==6.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmarkdown-it-py\u001b[0m\u001b[2m==3.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmbstrdecoder\u001b[0m\u001b[2m==1.1.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmdurl\u001b[0m\u001b[2m==0.1.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmistral-common\u001b[0m\u001b[2m==1.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmore-itertools\u001b[0m\u001b[2m==10.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmpmath\u001b[0m\u001b[2m==1.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmsgpack\u001b[0m\u001b[2m==1.1.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmsgspec\u001b[0m\u001b[2m==0.19.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmultiprocess\u001b[0m\u001b[2m==0.70.16\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnetworkx\u001b[0m\u001b[2m==3.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mninja\u001b[0m\u001b[2m==1.11.1.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnltk\u001b[0m\u001b[2m==3.9.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnumba\u001b[0m\u001b[2m==0.61.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnumexpr\u001b[0m\u001b[2m==2.11.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==1.26.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.6.4.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.6.80\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.6.77\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.6.77\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.5.1.17\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.3.0.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufile-cu12\u001b[0m\u001b[2m==1.11.1.6\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.7.77\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.7.1.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.5.4.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparselt-cu12\u001b[0m\u001b[2m==0.6.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-ml-py\u001b[0m\u001b[2m==12.575.51\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nccl-cu12\u001b[0m\u001b[2m==2.26.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.6.85\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvtx-cu12\u001b[0m\u001b[2m==12.6.77\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopenai\u001b[0m\u001b[2m==1.95.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopencv-python-headless\u001b[0m\u001b[2m==4.11.0.86\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-api\u001b[0m\u001b[2m==1.35.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-exporter-otlp\u001b[0m\u001b[2m==1.35.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-exporter-otlp-proto-common\u001b[0m\u001b[2m==1.35.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-exporter-otlp-proto-grpc\u001b[0m\u001b[2m==1.35.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-exporter-otlp-proto-http\u001b[0m\u001b[2m==1.35.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-proto\u001b[0m\u001b[2m==1.35.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-sdk\u001b[0m\u001b[2m==1.35.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-semantic-conventions\u001b[0m\u001b[2m==0.56b0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-semantic-conventions-ai\u001b[0m\u001b[2m==0.4.11\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1moutlines\u001b[0m\u001b[2m==0.1.11\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1moutlines-core\u001b[0m\u001b[2m==0.1.26\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpandas\u001b[0m\u001b[2m==2.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpartial-json-parser\u001b[0m\u001b[2m==0.2.1.1.post6\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpathvalidate\u001b[0m\u001b[2m==3.3.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpeft\u001b[0m\u001b[2m==0.16.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpillow\u001b[0m\u001b[2m==11.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mportalocker\u001b[0m\u001b[2m==3.2.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mprometheus-fastapi-instrumentator\u001b[0m\u001b[2m==7.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mprotobuf\u001b[0m\u001b[2m==6.31.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpy-cpuinfo\u001b[0m\u001b[2m==9.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyarrow\u001b[0m\u001b[2m==20.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpybind11\u001b[0m\u001b[2m==3.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpycountry\u001b[0m\u001b[2m==24.6.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic\u001b[0m\u001b[2m==2.11.7\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic-core\u001b[0m\u001b[2m==2.33.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpynvml\u001b[0m\u001b[2m==12.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpytablewriter\u001b[0m\u001b[2m==1.2.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpython-dotenv\u001b[0m\u001b[2m==1.1.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpython-multipart\u001b[0m\u001b[2m==0.0.20\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpytz\u001b[0m\u001b[2m==2025.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mray\u001b[0m\u001b[2m==2.47.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mregex\u001b[0m\u001b[2m==2024.11.6\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrich\u001b[0m\u001b[2m==14.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrich-toolkit\u001b[0m\u001b[2m==0.14.8\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrignore\u001b[0m\u001b[2m==0.6.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrouge-score\u001b[0m\u001b[2m==0.1.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msacrebleu\u001b[0m\u001b[2m==2.5.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msafetensors\u001b[0m\u001b[2m==0.5.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mscikit-learn\u001b[0m\u001b[2m==1.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mscipy\u001b[0m\u001b[2m==1.16.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msentencepiece\u001b[0m\u001b[2m==0.2.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msentry-sdk\u001b[0m\u001b[2m==2.32.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mshellingham\u001b[0m\u001b[2m==1.5.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msqlitedict\u001b[0m\u001b[2m==2.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mstarlette\u001b[0m\u001b[2m==0.47.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msympy\u001b[0m\u001b[2m==1.14.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtabledata\u001b[0m\u001b[2m==1.3.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtabulate\u001b[0m\u001b[2m==0.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtcolorpy\u001b[0m\u001b[2m==0.1.7\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mthreadpoolctl\u001b[0m\u001b[2m==3.6.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtiktoken\u001b[0m\u001b[2m==0.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.21.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorchaudio\u001b[0m\u001b[2m==2.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.22.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtqdm\u001b[0m\u001b[2m==4.67.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtqdm-multiprocess\u001b[0m\u001b[2m==0.0.11\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.53.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtypepy\u001b[0m\u001b[2m==1.3.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyper\u001b[0m\u001b[2m==0.16.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyping-inspection\u001b[0m\u001b[2m==0.4.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtzdata\u001b[0m\u001b[2m==2025.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1muvicorn\u001b[0m\u001b[2m==0.35.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1muvloop\u001b[0m\u001b[2m==0.21.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mvllm\u001b[0m\u001b[2m==0.9.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwatchfiles\u001b[0m\u001b[2m==1.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwebsockets\u001b[0m\u001b[2m==15.0.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mword2number\u001b[0m\u001b[2m==1.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mxformers\u001b[0m\u001b[2m==0.0.30\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mxgrammar\u001b[0m\u001b[2m==0.1.19\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mxxhash\u001b[0m\u001b[2m==3.5.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mzipp\u001b[0m\u001b[2m==3.23.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mzstandard\u001b[0m\u001b[2m==0.23.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Use uv to install the dependencies\n",
    "!uv pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9c9fc6-cc09-44a4-b7b4-2086952f04c6",
   "metadata": {},
   "source": [
    "# 💡 Disable the default vLLM inference serving\n",
    "Our `Lab 1 - inference with vllm` is quite computationally intensive. This lab is only using an L4 GPU so in order ensure the success of jupyterlab notebooks, we recommend disabling the default vLLM inference serving that is running in the namespace `llama-serving`.\n",
    "\n",
    "To do that, first login into the OpenShift CLI using `--token` or username/password given by the instructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a396768-eb7e-41c1-8c92-10db8bae7552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login successful.\n",
      "\n",
      "You have access to 108 projects, the list has been suppressed. You can list all projects with 'oc projects'\n",
      "\n",
      "Using project \"llama-serving\".\n"
     ]
    }
   ],
   "source": [
    "!oc login -u admin -p ${ADMIN_PASSWORD} --server=https://api.sno.${BASE_DOMAIN}:6443"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b423058-f36e-4f1e-9ea6-c3fd5da8f004",
   "metadata": {},
   "source": [
    "Once you login the `oc` CLI, run the following respectively to enable/disable the defautl model serving.\n",
    "\n",
    "```bash\n",
    "# undeploy deepseek-qwen3\n",
    "oc create configmap undeploy-sno-deepseek-qwen3-vllm -n llama-serving\n",
    "# redeploy deepseek-qwen3\n",
    "oc delete configmap undeploy-sno-deepseek-qwen3-vllm -n llama-serving\n",
    "\n",
    "# undeploy llama3-2-3b\n",
    "oc create configmap undeploy-llama3-2-3b -n llama-serving\n",
    "# redeploy llama3-2-3b\n",
    "oc delete configmap undeploy-llama3-2-3b -n llama-serving\n",
    "```\n",
    "\n",
    "Wait until the Pods are terminated before you continue with Lab 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04e78c0e-5c1b-43b5-a78a-af41a2a55168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configmap/undeploy-llama3-2-3b created\n"
     ]
    }
   ],
   "source": [
    "# undeploy llama3-2-3b\n",
    "!oc create configmap undeploy-llama3-2-3b -n llama-serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2905c5ac-4b7e-42ad-baeb-a11c0dec8824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configmap/undeploy-sno-deepseek-qwen3-vllm created\n"
     ]
    }
   ],
   "source": [
    "# undeploy deepseek-qwen3\n",
    "!oc create configmap undeploy-sno-deepseek-qwen3-vllm -n llama-serving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64c70b1-9238-4bf2-9837-5e0601a3ca77",
   "metadata": {},
   "source": [
    "# 🔍 Environment Validation Script\n",
    "\n",
    "This script validates the environment by checking:\n",
    "- vllm version (expected: 0.9.1)\n",
    "- llmcompressor version (expected: 0.5.2)\n",
    "- llama-serving namespace status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b867e95a-a7cc-4a52-9090-a540c9d2d7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_130/152499475.py:3: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Environment Validation Report\n",
      "==================================================\n",
      "✅ vllm: 0.9.1 (>= 0.9.1)\n",
      "✅ llmcompressor: 0.5.2 (>= 0.5.2)\n",
      "\n",
      "🔍 llama-serving Status:\n",
      "------------------------------\n",
      "✅ llama-serving namespace is accessible\n",
      "  ℹ️  No pods found - default vLLM inference serving is disabled (optimal for labs)\n",
      "\n",
      "==================================================\n",
      "🎉 All validation checks passed! Environment is ready.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import pkg_resources\n",
    "from packaging import version\n",
    "import os\n",
    "\n",
    "def check_package_version(package_name, expected_version):\n",
    "    \"\"\"Check if a package is installed and has the expected version\"\"\"\n",
    "    try:\n",
    "        installed_version = pkg_resources.get_distribution(package_name).version\n",
    "        if version.parse(installed_version) >= version.parse(expected_version):\n",
    "            return True, installed_version\n",
    "        else:\n",
    "            return False, installed_version\n",
    "    except pkg_resources.DistributionNotFound:\n",
    "        return False, \"Not installed\"\n",
    "\n",
    "def check_llama_serving_status():\n",
    "    \"\"\"Check the status of llama-serving namespace\"\"\"\n",
    "    try:\n",
    "        # Check if we can access the namespace\n",
    "        result = subprocess.run( \n",
    "            [\"oc\", \"get\", \"pods\", \"-n\", \"llama-serving\", \"--no-headers\"],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=30\n",
    "        )\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            lines = result.stdout.strip().split('\\n')\n",
    "            if not lines or lines == ['']:\n",
    "                return True, \"No pods found - default vLLM inference serving is disabled (optimal for labs)\"\n",
    "            \n",
    "            # Check pod status\n",
    "            pod_status = []\n",
    "            for line in lines:\n",
    "                if line.strip():\n",
    "                    parts = line.split()\n",
    "                    if len(parts) >= 3:\n",
    "                        pod_name = parts[0]\n",
    "                        ready = parts[1]\n",
    "                        status = parts[2]\n",
    "                        pod_status.append(f\"{pod_name}: {ready} {status}\")\n",
    "            \n",
    "            return True, pod_status\n",
    "        else:\n",
    "            return False, f\"Error accessing namespace: {result.stderr}\"\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return False, \"Timeout accessing llama-serving namespace\"\n",
    "    except Exception as e:\n",
    "        return False, f\"Error: {str(e)}\"\n",
    "\n",
    "def validate_environment():\n",
    "    \"\"\"Main validation function\"\"\"\n",
    "    print(\"🔍 Environment Validation Report\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Check required packages\n",
    "    required_packages = {\n",
    "        \"vllm\": \"0.9.1\",\n",
    "        \"llmcompressor\": \"0.5.2\"\n",
    "    }\n",
    "    \n",
    "    all_checks_passed = True\n",
    "    \n",
    "    for package, expected_ver in required_packages.items():\n",
    "        is_valid, installed_ver = check_package_version(package, expected_ver)\n",
    "        \n",
    "        if is_valid:\n",
    "            print(f\"✅ {package}: {installed_ver} (>= {expected_ver})\")\n",
    "        else:\n",
    "            print(f\"❌ {package}: {installed_ver} (expected >= {expected_ver})\")\n",
    "            all_checks_passed = False\n",
    "    \n",
    "    # Check llama-serving status\n",
    "    print(\"\\n🔍 llama-serving Status:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    is_accessible, status_info = check_llama_serving_status()\n",
    "    \n",
    "    if is_accessible:\n",
    "        print(\"✅ llama-serving namespace is accessible\")\n",
    "        if isinstance(status_info, list):\n",
    "            for pod_info in status_info:\n",
    "                print(f\"  📦 {pod_info}\")\n",
    "        else:\n",
    "            print(f\"  ℹ️  {status_info}\")\n",
    "    else:\n",
    "        print(f\"❌ llama-serving namespace: {status_info}\")\n",
    "        all_checks_passed = False\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    if all_checks_passed:\n",
    "        print(\"🎉 All validation checks passed! Environment is ready.\")\n",
    "    else:\n",
    "        print(\"⚠️  Some validation checks failed. Please review the issues above.\")\n",
    "    \n",
    "    return all_checks_passed\n",
    "\n",
    "# Run the validation\n",
    "validate_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846879af-205a-41d0-b3db-25285fa69e8a",
   "metadata": {},
   "source": [
    "## Quick Status Check\n",
    "\n",
    "Run the cell below for a quick status summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d98b3031-c505-4fa8-b010-7ab98d06d021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Quick Environment Status Check\n",
      "===================================\n",
      "✅ vllm: 0.9.1\n",
      "✅ llmcompressor: 0.5.2\n",
      "✅ llama-serving: accessible, default vLLM inference serving is disabled (optimal for labs)\n",
      "\n",
      "🎉 Overall: All systems ready!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def quick_status_check():\n",
    "    \"\"\"Quick validation check with minimal output\"\"\"\n",
    "    print(\"🚀 Quick Environment Status Check\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # Check vllm\n",
    "    vllm_ok, vllm_ver = check_package_version(\"vllm\", \"0.9.1\")\n",
    "    print(f\"{'✅' if vllm_ok else '❌'} vllm: {vllm_ver}\")\n",
    "    \n",
    "    # Check llmcompressor\n",
    "    llmc_ok, llmc_ver = check_package_version(\"llmcompressor\", \"0.5.2\")\n",
    "    print(f\"{'✅' if llmc_ok else '❌'} llmcompressor: {llmc_ver}\")\n",
    "    \n",
    "    # Check llama-serving\n",
    "    llama_ok, _ = check_llama_serving_status()\n",
    "    print(f\"{'✅' if llama_ok else '❌'} llama-serving: {'accessible, default vLLM inference serving is disabled (optimal for labs)' if llama_ok else 'issue detected'}\")\n",
    "    \n",
    "    # Overall status\n",
    "    all_good = vllm_ok and llmc_ok and llama_ok\n",
    "    print(f\"\\n{'🎉' if all_good else '⚠️'} Overall: {'All systems ready!' if all_good else 'Issues found - run full validation above'}\")\n",
    "    \n",
    "    return all_good\n",
    "\n",
    "# Run quick check\n",
    "quick_status_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116af22c-11fd-43d2-9caa-bbf0a283ec88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
