# Lab 1: Serving a Model with vLLM

This lab will walk you through how to serve an open-source model using vLLM (the upstream project of Red Hat AI Inference Server) and test its performance through benchmarking.

Please go to the notebook `1-inference-with-vllm.ipynb` to complete this lab.


## Summary

You've successfully:

✅ Installed and configured vLLM  
✅ Served a model using vLLM 
✅ Tested inference via HTTP API  
✅ Benchmarked performance with real metrics  
✅ Understood key performance indicators  

You now have hands-on experience with enterprise-grade LLM inference serving!
