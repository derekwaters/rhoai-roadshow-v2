# Lab 1: Serving a Model with vLLM

This lab will walk you through how to serve an open-source model using vLLM (the upstream project of Red Hat AI Inference Server) and test its performance through benchmarking.

Please go to the notebook In JupyterLab, open the notebook: <a href="https://github.com/odh-labs/rhoai-roadshow-v2/blob/main/docs/4-rhaiis/notebooks/1-inference-with-vllm.ipynb" target="_blank">1-inference-with-vllm.ipynb</a> to complete this lab.

## Summary

You've successfully:

- ✅ Installed and configured vLLM
- ✅ Served a model using vLLM
- ✅ Tested inference via HTTP API
- ✅ Benchmarked performance with real metrics
- ✅ Understood key performance indicators

You now have hands-on experience with enterprise-grade LLM inference serving!
