{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac9b9a5e-7bb4-4334-a2f1-df28fbf031ba",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd443db6-4eca-42c3-b63a-be3286249a67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07017b3f-0d40-4122-bbdd-242f57475de3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c2ba0ab-7fd1-4a33-bea0-49eb2f846931",
   "metadata": {},
   "source": [
    "# Document Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ade526e-cdf9-412f-8f94-76c633f506b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install boto3==1.34.103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b8599ad-7f3c-4dbd-b0d6-64f73535ab03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from rag-docs::2502.07835v1.pdf to: downloads/2502.07835v1.pdf\n",
      "✅ Downloaded '2502.07835v1.pdf' to 'downloads/2502.07835v1.pdf'\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from botocore.config import Config\n",
    "import os\n",
    "\n",
    "\"\"\"\n",
    "Environment variables:\n",
    "  AWS_S3_ENDPOINT        – MinIO service DNS name (e.g. minio.minio.svc.cluster.local)\n",
    "  AWS_ACCESS_KEY_ID      – MinIO access key\n",
    "  AWS_SECRET_ACCESS_KEY  – MinIO secret key\n",
    "  AWS_DEFAULT_REGION     – Dummy value; boto3 still expects one\n",
    "  AWS_S3_BUCKET          – Default bucket to use for the Workspace data connection \n",
    "\"\"\"\n",
    "\n",
    "# === Configuration ===\n",
    "endpoint = os.getenv(\"AWS_S3_ENDPOINT\")\n",
    "access_key = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "region = os.getenv(\"AWS_DEFAULT_REGION\")\n",
    "bucket_name = os.getenv(\"AWS_S3_BUCKET\")\n",
    "object_key = \"2502.07835v1.pdf\"  # The name of the PDF in the S3 bucket\n",
    "download_dir = \"downloads\"\n",
    "\n",
    "# === Initialise S3 client ===\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=f\"http://{endpoint}\",\n",
    "    aws_access_key_id=access_key,\n",
    "    aws_secret_access_key=secret_key,\n",
    "    region_name=region,\n",
    "    config=Config(signature_version=\"s3v4\"),\n",
    ")\n",
    "\n",
    "# === Ensure download directory exists ===\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "local_path = os.path.join(download_dir, object_key)\n",
    "print(f\"Downloading from {bucket_name}::{object_key} to: {local_path}\")\n",
    "\n",
    "# === Download the file ===\n",
    "try:\n",
    "    s3.download_file(bucket_name, object_key, local_path)\n",
    "    print(f\"✅ Downloaded '{object_key}' to '{local_path}'\")\n",
    "except s3.exceptions.NoSuchKey:\n",
    "    print(f\"❌ File '{object_key}' not found in bucket '{bucket_name}'\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error downloading file: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf2a9c2-784c-46aa-b7c4-b5a37dab7353",
   "metadata": {},
   "source": [
    "# Embedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d99c74a-14d3-45fe-9902-2afcffaef447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting docling==2.39.0\n",
      "  Downloading docling-2.39.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pydantic<3.0.0,>=2.0.0 (from docling==2.39.0)\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Collecting docling-core<3.0.0,>=2.39.0 (from docling-core[chunking]<3.0.0,>=2.39.0->docling==2.39.0)\n",
      "  Downloading docling_core-2.39.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting docling-ibm-models<4.0.0,>=3.4.4 (from docling==2.39.0)\n",
      "  Downloading docling_ibm_models-3.6.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting docling-parse<5.0.0,>=4.0.0 (from docling==2.39.0)\n",
      "  Downloading docling_parse-4.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Collecting filetype<2.0.0,>=1.2.0 (from docling==2.39.0)\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting pypdfium2<5.0.0,>=4.30.0 (from docling==2.39.0)\n",
      "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.3.0 (from docling==2.39.0)\n",
      "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting huggingface_hub<1,>=0.23 (from docling==2.39.0)\n",
      "  Downloading huggingface_hub-0.33.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.2 in /opt/app-root/lib64/python3.11/site-packages (from docling==2.39.0) (2.32.3)\n",
      "Collecting easyocr<2.0,>=1.7 (from docling==2.39.0)\n",
      "  Downloading easyocr-1.7.2-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: certifi>=2024.7.4 in /opt/app-root/lib64/python3.11/site-packages (from docling==2.39.0) (2025.4.26)\n",
      "Collecting rtree<2.0.0,>=1.3.0 (from docling==2.39.0)\n",
      "  Downloading rtree-1.4.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting typer<0.17.0,>=0.12.5 (from docling==2.39.0)\n",
      "  Downloading typer-0.16.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting python-docx<2.0.0,>=1.1.2 (from docling==2.39.0)\n",
      "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting python-pptx<2.0.0,>=1.0.2 (from docling==2.39.0)\n",
      "  Downloading python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /opt/app-root/lib64/python3.11/site-packages (from docling==2.39.0) (4.13.4)\n",
      "Requirement already satisfied: pandas<3.0.0,>=2.1.4 in /opt/app-root/lib64/python3.11/site-packages (from docling==2.39.0) (2.2.3)\n",
      "Collecting marko<3.0.0,>=2.1.2 (from docling==2.39.0)\n",
      "  Downloading marko-2.1.4-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting openpyxl<4.0.0,>=3.1.5 (from docling==2.39.0)\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting lxml<6.0.0,>=4.0.0 (from docling==2.39.0)\n",
      "  Downloading lxml-5.4.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: pillow<12.0.0,>=10.0.0 in /opt/app-root/lib64/python3.11/site-packages (from docling==2.39.0) (11.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.65.0 in /opt/app-root/lib64/python3.11/site-packages (from docling==2.39.0) (4.67.1)\n",
      "Requirement already satisfied: pluggy<2.0.0,>=1.0.0 in /opt/app-root/lib64/python3.11/site-packages (from docling==2.39.0) (1.6.0)\n",
      "Collecting pylatexenc<3.0,>=2.10 (from docling==2.39.0)\n",
      "  Downloading pylatexenc-2.10.tar.gz (162 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: scipy<2.0.0,>=1.6.0 in /opt/app-root/lib64/python3.11/site-packages (from docling==2.39.0) (1.15.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/app-root/lib64/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->docling==2.39.0) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/app-root/lib64/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->docling==2.39.0) (4.13.2)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.16.0 in /opt/app-root/lib64/python3.11/site-packages (from docling-core<3.0.0,>=2.39.0->docling-core[chunking]<3.0.0,>=2.39.0->docling==2.39.0) (4.23.0)\n",
      "Collecting jsonref<2.0.0,>=1.1.0 (from docling-core<3.0.0,>=2.39.0->docling-core[chunking]<3.0.0,>=2.39.0->docling==2.39.0)\n",
      "  Downloading jsonref-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /opt/app-root/lib64/python3.11/site-packages (from docling-core<3.0.0,>=2.39.0->docling-core[chunking]<3.0.0,>=2.39.0->docling==2.39.0) (0.9.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.1 in /opt/app-root/lib64/python3.11/site-packages (from docling-core<3.0.0,>=2.39.0->docling-core[chunking]<3.0.0,>=2.39.0->docling==2.39.0) (6.0.2)\n",
      "Collecting latex2mathml<4.0.0,>=3.77.0 (from docling-core<3.0.0,>=2.39.0->docling-core[chunking]<3.0.0,>=2.39.0->docling==2.39.0)\n",
      "  Downloading latex2mathml-3.78.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting semchunk<3.0.0,>=2.2.0 (from docling-core[chunking]<3.0.0,>=2.39.0->docling==2.39.0)\n",
      "  Downloading semchunk-2.2.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting transformers<5.0.0,>=4.34.0 (from docling-core[chunking]<3.0.0,>=2.39.0->docling==2.39.0)\n",
      "  Downloading transformers-4.53.0-py3-none-any.whl.metadata (39 kB)\n",
      "Collecting torch<3.0.0,>=2.2.2 (from docling-ibm-models<4.0.0,>=3.4.4->docling==2.39.0)\n",
      "  Downloading torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Collecting torchvision<1,>=0 (from docling-ibm-models<4.0.0,>=3.4.4->docling==2.39.0)\n",
      "  Downloading torchvision-0.22.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting jsonlines<4.0.0,>=3.1.0 (from docling-ibm-models<4.0.0,>=3.4.4->docling==2.39.0)\n",
      "  Downloading jsonlines-3.1.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting opencv-python-headless<5.0.0.0,>=4.6.0.66 (from docling-ibm-models<4.0.0,>=3.4.4->docling==2.39.0)\n",
      "  Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting safetensors<1,>=0.4.3 (from safetensors[torch]<1,>=0.4.3->docling-ibm-models<4.0.0,>=3.4.4->docling==2.39.0)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.24.4 in /opt/app-root/lib64/python3.11/site-packages (from docling-ibm-models<4.0.0,>=3.4.4->docling==2.39.0) (2.2.6)\n",
      "Collecting scikit-image (from easyocr<2.0,>=1.7->docling==2.39.0)\n",
      "  Downloading scikit_image-0.25.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Collecting python-bidi (from easyocr<2.0,>=1.7->docling==2.39.0)\n",
      "  Downloading python_bidi-0.6.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting Shapely (from easyocr<2.0,>=1.7->docling==2.39.0)\n",
      "  Downloading shapely-2.1.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting pyclipper (from easyocr<2.0,>=1.7->docling==2.39.0)\n",
      "  Downloading pyclipper-1.3.0.post6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting ninja (from easyocr<2.0,>=1.7->docling==2.39.0)\n",
      "  Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: filelock in /opt/app-root/lib64/python3.11/site-packages (from huggingface_hub<1,>=0.23->docling==2.39.0) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/app-root/lib64/python3.11/site-packages (from huggingface_hub<1,>=0.23->docling==2.39.0) (2025.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/app-root/lib64/python3.11/site-packages (from huggingface_hub<1,>=0.23->docling==2.39.0) (25.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface_hub<1,>=0.23->docling==2.39.0)\n",
      "  Downloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/app-root/lib64/python3.11/site-packages (from jsonlines<4.0.0,>=3.1.0->docling-ibm-models<4.0.0,>=3.4.4->docling==2.39.0) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/app-root/lib64/python3.11/site-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.39.0->docling-core[chunking]<3.0.0,>=2.39.0->docling==2.39.0) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/app-root/lib64/python3.11/site-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.39.0->docling-core[chunking]<3.0.0,>=2.39.0->docling==2.39.0) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/app-root/lib64/python3.11/site-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.39.0->docling-core[chunking]<3.0.0,>=2.39.0->docling==2.39.0) (0.25.0)\n",
      "Collecting et-xmlfile (from openpyxl<4.0.0,>=3.1.5->docling==2.39.0)\n",
      "  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/app-root/lib64/python3.11/site-packages (from pandas<3.0.0,>=2.1.4->docling==2.39.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/app-root/lib64/python3.11/site-packages (from pandas<3.0.0,>=2.1.4->docling==2.39.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/app-root/lib64/python3.11/site-packages (from pandas<3.0.0,>=2.1.4->docling==2.39.0) (2025.2)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.0.0->docling==2.39.0)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3.0.0,>=2.0.0->docling==2.39.0)\n",
      "  Downloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3.0.0,>=2.0.0->docling==2.39.0)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.3.0->docling==2.39.0)\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting XlsxWriter>=0.5.7 (from python-pptx<2.0.0,>=1.0.2->docling==2.39.0)\n",
      "  Downloading xlsxwriter-3.2.5-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/lib64/python3.11/site-packages (from requests<3.0.0,>=2.32.2->docling==2.39.0) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/app-root/lib64/python3.11/site-packages (from requests<3.0.0,>=2.32.2->docling==2.39.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/lib64/python3.11/site-packages (from requests<3.0.0,>=2.32.2->docling==2.39.0) (1.26.20)\n",
      "Collecting mpire[dill] (from semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.39.0->docling==2.39.0)\n",
      "  Downloading mpire-2.10.2-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting sympy>=1.13.3 (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.4->docling==2.39.0)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in /opt/app-root/lib64/python3.11/site-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.4->docling==2.39.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/app-root/lib64/python3.11/site-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.4->docling==2.39.0) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.4->docling==2.39.0)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.4->docling==2.39.0)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.4->docling==2.39.0)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.4->docling==2.39.0)\n",
      "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.4->docling==2.39.0)\n",
      "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.4->docling==2.39.0)\n",
      "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.7.77 (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.4->docling==2.39.0)\n",
      "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.4->docling==2.39.0)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.4->docling==2.39.0)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.4->docling==2.39.0)\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.26.2 (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.4->docling==2.39.0)\n",
      "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.6.77 (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.4->docling==2.39.0)\n",
      "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.4->docling==2.39.0)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.4->docling==2.39.0)\n",
      "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.3.1 (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.4->docling==2.39.0)\n",
      "  Downloading triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /opt/app-root/lib64/python3.11/site-packages (from triton==3.3.1->torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.4->docling==2.39.0) (75.8.2)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.34.0->docling-core[chunking]<3.0.0,>=2.39.0->docling==2.39.0)\n",
      "  Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.34.0->docling-core[chunking]<3.0.0,>=2.39.0->docling==2.39.0)\n",
      "  Downloading tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/app-root/lib64/python3.11/site-packages (from typer<0.17.0,>=0.12.5->docling==2.39.0) (8.2.1)\n",
      "Collecting shellingham>=1.3.0 (from typer<0.17.0,>=0.12.5->docling==2.39.0)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/app-root/lib64/python3.11/site-packages (from typer<0.17.0,>=0.12.5->docling==2.39.0) (13.9.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/app-root/lib64/python3.11/site-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=2.1.4->docling==2.39.0) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/app-root/lib64/python3.11/site-packages (from rich>=10.11.0->typer<0.17.0,>=0.12.5->docling==2.39.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/app-root/lib64/python3.11/site-packages (from rich>=10.11.0->typer<0.17.0,>=0.12.5->docling==2.39.0) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/app-root/lib64/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<0.17.0,>=0.12.5->docling==2.39.0) (0.1.2)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.4->docling==2.39.0)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/app-root/lib64/python3.11/site-packages (from jinja2->torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.4->docling==2.39.0) (3.0.2)\n",
      "Collecting multiprocess>=0.70.15 (from mpire[dill]->semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.39.0->docling==2.39.0)\n",
      "  Downloading multiprocess-0.70.18-py311-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: dill>=0.4.0 in /opt/app-root/lib64/python3.11/site-packages (from multiprocess>=0.70.15->mpire[dill]->semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.39.0->docling==2.39.0) (0.4.0)\n",
      "Collecting imageio!=2.35.0,>=2.33 (from scikit-image->easyocr<2.0,>=1.7->docling==2.39.0)\n",
      "  Downloading imageio-2.37.0-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting tifffile>=2022.8.12 (from scikit-image->easyocr<2.0,>=1.7->docling==2.39.0)\n",
      "  Downloading tifffile-2025.6.11-py3-none-any.whl.metadata (32 kB)\n",
      "Collecting lazy-loader>=0.4 (from scikit-image->easyocr<2.0,>=1.7->docling==2.39.0)\n",
      "  Downloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Downloading docling-2.39.0-py3-none-any.whl (185 kB)\n",
      "Downloading docling_core-2.39.0-py3-none-any.whl (152 kB)\n",
      "Downloading docling_ibm_models-3.6.0-py3-none-any.whl (84 kB)\n",
      "Downloading docling_parse-4.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.1/15.1 MB\u001b[0m \u001b[31m99.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading easyocr-1.7.2-py3-none-any.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m515.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading huggingface_hub-0.33.1-py3-none-any.whl (515 kB)\n",
      "Downloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m531.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jsonlines-3.1.0-py3-none-any.whl (8.6 kB)\n",
      "Downloading jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
      "Downloading latex2mathml-3.78.0-py3-none-any.whl (73 kB)\n",
      "Downloading lxml-5.4.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m450.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading marko-2.1.4-py3-none-any.whl (42 kB)\n",
      "Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 MB\u001b[0m \u001b[31m257.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Downloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m451.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
      "Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
      "Downloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
      "Downloading rtree-1.4.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (541 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m541.1/541.1 kB\u001b[0m \u001b[31m538.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Downloading semchunk-2.2.2-py3-none-any.whl (10 kB)\n",
      "Downloading torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl (821.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.2/821.2 MB\u001b[0m \u001b[31m133.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Downloading triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.22.1-cp311-cp311-manylinux_2_28_x86_64.whl (7.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.53.0-py3-none-any.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typer-0.16.0-py3-none-any.whl (46 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m792.7/792.7 kB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m112.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Downloading xlsxwriter-3.2.5-py3-none-any.whl (172 kB)\n",
      "Downloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Downloading mpire-2.10.2-py3-none-any.whl (272 kB)\n",
      "Downloading multiprocess-0.70.18-py311-none-any.whl (144 kB)\n",
      "Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
      "Downloading pyclipper-1.3.0.post6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (969 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m969.6/969.6 kB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_bidi-0.6.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (292 kB)\n",
      "Downloading scikit_image-0.25.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading imageio-2.37.0-py3-none-any.whl (315 kB)\n",
      "Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Downloading tifffile-2025.6.11-py3-none-any.whl (230 kB)\n",
      "Downloading shapely-2.1.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pylatexenc\n",
      "\u001b[33m  DEPRECATION: Building 'pylatexenc' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'pylatexenc'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for pylatexenc (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pylatexenc: filename=pylatexenc-2.10-py3-none-any.whl size=136863 sha256=b7c9f246052c2356e4831022ecdf5f7cd91228b96f957c61d7d87e4159f4d50e\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-atpcgchq/wheels/b1/7a/33/9fdd892f784ed4afda62b685ae3703adf4c91aa0f524c28f03\n",
      "Successfully built pylatexenc\n",
      "Installing collected packages: python-bidi, pylatexenc, pyclipper, nvidia-cusparselt-cu12, mpmath, filetype, XlsxWriter, typing-inspection, triton, tifffile, sympy, shellingham, Shapely, safetensors, rtree, regex, python-dotenv, pypdfium2, pydantic-core, opencv-python-headless, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ninja, multiprocess, mpire, marko, lxml, lazy-loader, latex2mathml, jsonref, jsonlines, imageio, hf-xet, et-xmlfile, annotated-types, scikit-image, python-pptx, python-docx, pydantic, openpyxl, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, huggingface_hub, typer, tokenizers, semchunk, pydantic-settings, nvidia-cusolver-cu12, transformers, torch, docling-core, torchvision, docling-parse, easyocr, docling-ibm-models, docling\n",
      "\u001b[2K  Attempting uninstall: pydantic━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m43/64\u001b[0m [python-pptx]]-cu12]u12]\n",
      "\u001b[2K    Found existing installation: pydantic 1.10.22━━━━━━━━━━━━━\u001b[0m \u001b[32m43/64\u001b[0m [python-pptx]\n",
      "\u001b[2K    Uninstalling pydantic-1.10.22:━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m45/64\u001b[0m [pydantic]\n",
      "\u001b[2K      Successfully uninstalled pydantic-1.10.22[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m45/64\u001b[0m [pydantic]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64/64\u001b[0m [docling][docling][easyocr]parse]r-cu12]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "codeflare-sdk 0.28.1 requires pydantic<2, but you have pydantic 2.11.7 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed Shapely-2.1.1 XlsxWriter-3.2.5 annotated-types-0.7.0 docling-2.39.0 docling-core-2.39.0 docling-ibm-models-3.6.0 docling-parse-4.1.0 easyocr-1.7.2 et-xmlfile-2.0.0 filetype-1.2.0 hf-xet-1.1.5 huggingface_hub-0.33.1 imageio-2.37.0 jsonlines-3.1.0 jsonref-1.1.0 latex2mathml-3.78.0 lazy-loader-0.4 lxml-5.4.0 marko-2.1.4 mpire-2.10.2 mpmath-1.3.0 multiprocess-0.70.18 ninja-1.11.1.4 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 opencv-python-headless-4.11.0.86 openpyxl-3.1.5 pyclipper-1.3.0.post6 pydantic-2.11.7 pydantic-core-2.33.2 pydantic-settings-2.10.1 pylatexenc-2.10 pypdfium2-4.30.1 python-bidi-0.6.6 python-docx-1.2.0 python-dotenv-1.1.1 python-pptx-1.0.2 regex-2024.11.6 rtree-1.4.0 safetensors-0.5.3 scikit-image-0.25.2 semchunk-2.2.2 shellingham-1.5.4 sympy-1.14.0 tifffile-2025.6.11 tokenizers-0.21.2 torch-2.7.1 torchvision-0.22.1 transformers-4.53.0 triton-3.3.1 typer-0.16.0 typing-inspection-0.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install docling==2.39.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4b07c68-85d1-46be-a14e-088ac5eab68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found /opt/app-root/src/rhoai-roadshow-v2/docs/2-rag/notebook/downloads/2502.07835v1.pdf\n"
     ]
    }
   ],
   "source": [
    "from utils import project_root\n",
    "\n",
    "# Assemble a complete path to the file so the document import can properly and reliably always find the document.\n",
    "DOC_SOURCE = project_root() / local_path\n",
    "\n",
    "if not DOC_SOURCE.is_file():\n",
    "    raise FileNotFoundError(f\"{DOC_SOURCE} does not exist.\")\n",
    "\n",
    "print(f\"Found {DOC_SOURCE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2cf3f59c-1c94-4511-b86d-1f2ae5f1705c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parse and chunk a PDF using Docling v2.x\n",
    "\"\"\"\n",
    "from docling.document_converter import DocumentConverter\n",
    "from pathlib import Path\n",
    "\n",
    "#base_dir = Path(__file__).resolve().parent\n",
    "base_dir = Path().resolve()\n",
    "doc_source = base_dir / local_path\n",
    "if not doc_source.is_file():\n",
    "    raise FileNotFoundError(f\"{doc_source} does not exist.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1eb104d7-caf6-45d7-9b22-cfeb6a483d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "doc = DocumentConverter().convert(source=doc_source).document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "51b3591c-65eb-4556-a71b-0476c9b516b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=1), 2: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=2), 3: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=3), 4: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=4), 5: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=5), 6: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=6), 7: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=7), 8: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=8), 9: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=9), 10: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=10), 11: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=11), 12: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=12), 13: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=13)}\n"
     ]
    }
   ],
   "source": [
    "print(doc.pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dadf3619-9526-4710-9428-3889d3c7dcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.chunking import HybridChunker\n",
    "\n",
    "chunker = HybridChunker()\n",
    "chunk_iter = chunker.chunk(dl_doc=doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7b3b5991-27da-449f-a7ba-a115a630112c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 0 ===\n",
      "chunk.text:\n",
      "'ahilanp@gmail.com\\nFebruary 13, 2025…'\n",
      "chunker.contextualize(chunk):\n",
      "'Ahilan Ayyachamy Nadar Ponnusamy\\nahilanp@gmail.com\\nFebruary 13, 2025…'\n",
      "\n",
      "=== 1 ===\n",
      "chunk.text:\n",
      "'The rise of Large Language Models (LLMs) in software engineering, particularly in code generation, has garnered significant attention. However, assessing the quality of AI-generated code remains a challenge due to the inherent complexity of programming tasks and the lack of robust evaluation metrics…'\n",
      "chunker.contextualize(chunk):\n",
      "'Abstract\\nThe rise of Large Language Models (LLMs) in software engineering, particularly in code generation, has garnered significant attention. However, assessing the quality of AI-generated code remains a challenge due to the inherent complexity of programming tasks and the lack of robust evaluatio…'\n",
      "\n",
      "=== 2 ===\n",
      "chunk.text:\n",
      "'AI-assisted coding has been shown to be more beneficial for senior developers, as they possess the expertise to critically evaluate the generated code for correctness, completeness, and compliance. In contrast, junior developers may struggle to identify hallucinations, missing functionality, or inco…'\n",
      "chunker.contextualize(chunk):\n",
      "'Abstract\\nAI-assisted coding has been shown to be more beneficial for senior developers, as they possess the expertise to critically evaluate the generated code for correctness, completeness, and compliance. In contrast, junior developers may struggle to identify hallucinations, missing functionality…'\n",
      "\n",
      "=== 3 ===\n",
      "chunk.text:\n",
      "'AI-powered code assistants, leveraging the power of Large Language Models (LLMs), are becoming a focal point for enterprises, offering promising capabilities in automating code generation. However, evaluating the quality of LLM-generated code remains a complex challenge due to the intricacies of pro…'\n",
      "chunker.contextualize(chunk):\n",
      "'1 Introduction\\nAI-powered code assistants, leveraging the power of Large Language Models (LLMs), are becoming a focal point for enterprises, offering promising capabilities in automating code generation. However, evaluating the quality of LLM-generated code remains a complex challenge due to the int…'\n",
      "\n",
      "=== 4 ===\n",
      "chunk.text:\n",
      "'Traditional evaluation techniques rely on test-based methods such as pass@k, which assess code correctness by executing manually written test cases [3, 4]. While effective in certain contexts, these methods are limited by the need for extensive test case coverage, which is labor-intensive and may no…'\n",
      "chunker.contextualize(chunk):\n",
      "'1 Introduction\\nTraditional evaluation techniques rely on test-based methods such as pass@k, which assess code correctness by executing manually written test cases [3, 4]. While effective in certain contexts, these methods are limited by the need for extensive test case coverage, which is labor-inten…'\n",
      "\n",
      "=== 5 ===\n",
      "chunk.text:\n",
      "'To address these limitations, we introduce a novel reverse generation technique combined with an SBC score to evaluate the accuracy and completeness of AI-generated code. Our approach does not rely on reference code but instead assesses how well the generated code aligns with the original requiremen…'\n",
      "chunker.contextualize(chunk):\n",
      "'1 Introduction\\nTo address these limitations, we introduce a novel reverse generation technique combined with an SBC score to evaluate the accuracy and completeness of AI-generated code. Our approach does not rely on reference code but instead assesses how well the generated code aligns with the orig…'\n",
      "\n",
      "=== 6 ===\n",
      "chunk.text:\n",
      "'There has been no prior work on applying a reverse generation technique with LLMs in conjunction with the SBC scoring mechanism . However, existing research has explored LLM-based evaluation methods for code analysis and natural language generation (NLG) , leveraging large models for assessing outpu…'\n",
      "chunker.contextualize(chunk):\n",
      "'2 Related Work\\nThere has been no prior work on applying a reverse generation technique with LLMs in conjunction with the SBC scoring mechanism . However, existing research has explored LLM-based evaluation methods for code analysis and natural language generation (NLG) , leveraging large models for …'\n",
      "\n",
      "=== 7 ===\n",
      "chunk.text:\n",
      "'This paper introduces an LLM-driven evaluation framework that employs a chain-of-thought (CoT) approach to assess the quality of generated text. The study demonstrates that LLM-based evaluators can outperform traditional NLG metrics in text summarization and dialogue generation . However, the author…'\n",
      "chunker.contextualize(chunk):\n",
      "'2.1 G-EVAL: NLG Evaluation using GPT-4 with Better Human Alignment (Yang et al., [13])\\nThis paper introduces an LLM-driven evaluation framework that employs a chain-of-thought (CoT) approach to assess the quality of generated text. The study demonstrates that LLM-based evaluators can outperform trad…'\n",
      "\n",
      "=== 8 ===\n",
      "chunk.text:\n",
      "'Inspired by G-EVAL , this paper proposes ICE-Score , an evaluation metric that leverages LLMs for code assessment across multiple programming languages, including Java, Python, C, C++, and JavaScript . The approach incorporates both human-centered usefulness and execution-based functional correctnes…'\n",
      "chunker.contextualize(chunk):\n",
      "'2.2 ICE-Score: Instructing Large Language Models to Evaluate Code (Terry, [14])\\nInspired by G-EVAL , this paper proposes ICE-Score , an evaluation metric that leverages LLMs for code assessment across multiple programming languages, including Java, Python, C, C++, and JavaScript . The approach incor…'\n",
      "\n",
      "=== 9 ===\n",
      "chunk.text:\n",
      "'As Large Language Models (LLMs) become increasingly integrated into the software development lifecycle, concerns regarding the quality, correctness, and reliability of generated code have grown. Xiaoyin et al. [15] highlight these challenges and propose metamorphic prompt testing as a validation app…'\n",
      "chunker.contextualize(chunk):\n",
      "'2.3 Metamorphic Prompt Testing for LLM Code Validation (Xiaoyin et al., [15] )\\nAs Large Language Models (LLMs) become increasingly integrated into the software development lifecycle, concerns regarding the quality, correctness, and reliability of generated code have grown. Xiaoyin et al. [15] highli…'\n",
      "\n",
      "=== 10 ===\n",
      "chunk.text:\n",
      "'This paper builds upon these advancements by extending the LLM-driven evaluation paradigm but introduces a fundamentally different approach . Rather than directly evaluating code output, we propose a reverse generation framework, where requirements are inferred from code and then compared to the ori…'\n",
      "chunker.contextualize(chunk):\n",
      "'2.3 Metamorphic Prompt Testing for LLM Code Validation (Xiaoyin et al., [15] )\\nThis paper builds upon these advancements by extending the LLM-driven evaluation paradigm but introduces a fundamentally different approach . Rather than directly evaluating code output, we propose a reverse generation fr…'\n",
      "\n",
      "=== 11 ===\n",
      "chunk.text:\n",
      "'The following methodology was adopted in this study to evaluate the effectiveness of reverse generation and SBC scoring in AI-assisted development.…'\n",
      "chunker.contextualize(chunk):\n",
      "'3 Methodology\\nThe following methodology was adopted in this study to evaluate the effectiveness of reverse generation and SBC scoring in AI-assisted development.…'\n",
      "\n",
      "=== 12 ===\n",
      "chunk.text:\n",
      "'To construct a dataset that closely mimics the software development lifecycle, requirements were distributed across multiple layers of application development, such as:\\n- · User Interface (UI) : React, Angular\\n- •\\n- Data Layer : SQL for data modeling\\n- · Data Objects and Business Logic : .NET, Node.…'\n",
      "chunker.contextualize(chunk):\n",
      "'3.1 Dataset Creation\\nTo construct a dataset that closely mimics the software development lifecycle, requirements were distributed across multiple layers of application development, such as:\\n- · User Interface (UI) : React, Angular\\n- •\\n- Data Layer : SQL for data modeling\\n- · Data Objects and Busines…'\n",
      "\n",
      "=== 13 ===\n",
      "chunk.text:\n",
      "'A reference implementation was developed in Python and is available in the associated GitHub repository. The implementation follows these key steps:\\n- 1. Iterate through the requirements and target technologies in the dataset.\\n- 2. Invoke the LLM to generate code for the given requirement.\\n- 3. Perf…'\n",
      "chunker.contextualize(chunk):\n",
      "'3.2 Reference Implementation\\nA reference implementation was developed in Python and is available in the associated GitHub repository. The implementation follows these key steps:\\n- 1. Iterate through the requirements and target technologies in the dataset.\\n- 2. Invoke the LLM to generate code for the…'\n",
      "\n",
      "=== 14 ===\n",
      "chunk.text:\n",
      "'The existing studies highlighted in the Related Work section [13, 14] use closedsource hosted models, such as GPT-3.5 and GPT-4, for their analysis. In contrast, this study focuses on open models, as discussed in [17].\\nWe selected four quantized models (Q4 or Q5) for this study:\\n- · Model 1: Codella…'\n",
      "chunker.contextualize(chunk):\n",
      "'3.3 Choice of Open Models\\nThe existing studies highlighted in the Related Work section [13, 14] use closedsource hosted models, such as GPT-3.5 and GPT-4, for their analysis. In contrast, this study focuses on open models, as discussed in [17].\\nWe selected four quantized models (Q4 or Q5) for this s…'\n",
      "\n",
      "=== 15 ===\n",
      "chunk.text:\n",
      "'These mid-level models were chosen for their balance between size, efficiency, and capability in handling both code and natural language tasks effectively.…'\n",
      "chunker.contextualize(chunk):\n",
      "'· Model 4: Codestral 22B\\nThese mid-level models were chosen for their balance between size, efficiency, and capability in handling both code and natural language tasks effectively.…'\n",
      "\n",
      "=== 16 ===\n",
      "chunk.text:\n",
      "'The SBC score is computed using the formula:\\n<!-- formula-not-decoded -->\\nwhere:\\n- · Semantic Similarity is computed using the PyTorch cos sim function to measure the cosine similarity between input and generated requirement encodings using the all-MiniLM-L6-v2 model from the Sentence Transformers l…'\n",
      "chunker.contextualize(chunk):\n",
      "'3.4 SBC Score Calculation\\nThe SBC score is computed using the formula:\\n<!-- formula-not-decoded -->\\nwhere:\\n- · Semantic Similarity is computed using the PyTorch cos sim function to measure the cosine similarity between input and generated requirement encodings using the all-MiniLM-L6-v2 model from t…'\n",
      "\n",
      "=== 17 ===\n",
      "chunk.text:\n",
      "'The weight distribution in the SBC score reflects the relative importance of each metric in capturing requirement-code alignment. Semantic similarity is given the highest weight (0.7) as it directly measures how closely the reversegenerated requirement aligns with the original intent, making it the …'\n",
      "chunker.contextualize(chunk):\n",
      "'3.4 SBC Score Calculation\\nThe weight distribution in the SBC score reflects the relative importance of each metric in capturing requirement-code alignment. Semantic similarity is given the highest weight (0.7) as it directly measures how closely the reversegenerated requirement aligns with the origi…'\n",
      "\n",
      "=== 18 ===\n",
      "chunk.text:\n",
      "'While correlation coefficients such as Kendall-Tau ( τ ), Pearson ( r p ), and Spearman ( r s ) are commonly used to measure relationships between variables, they were not included in the SBC score computation. These metrics primarily assess agreement and ranking consistency, whereas SBC is designed…'\n",
      "chunker.contextualize(chunk):\n",
      "'3.5 A Note on Correlation Metrics\\nWhile correlation coefficients such as Kendall-Tau ( τ ), Pearson ( r p ), and Spearman ( r s ) are commonly used to measure relationships between variables, they were not included in the SBC score computation. These metrics primarily assess agreement and ranking co…'\n",
      "\n",
      "=== 19 ===\n",
      "chunk.text:\n",
      "'For visualization and analysis, this study utilized Google Sheets to generate graphs and insights from the SBC score data. The process involved:\\n- • Exporting the SBC score output into a CSV format.\\n- • Creating a Pivot Table to aggregate and structure the data.\\n- • Generating graphs and charts to v…'\n",
      "chunker.contextualize(chunk):\n",
      "'3.6 Visualization and Analysis\\nFor visualization and analysis, this study utilized Google Sheets to generate graphs and insights from the SBC score data. The process involved:\\n- • Exporting the SBC score output into a CSV format.\\n- • Creating a Pivot Table to aggregate and structure the data.\\n- • Ge…'\n",
      "\n",
      "=== 20 ===\n",
      "chunk.text:\n",
      "'To ensure consistency in responses, we conducted all tests with the temperature set to zero. With the reference implementation, we conducted the evaluation in two cycles:…'\n",
      "chunker.contextualize(chunk):\n",
      "'4 Experiments and Results\\nTo ensure consistency in responses, we conducted all tests with the temperature set to zero. With the reference implementation, we conducted the evaluation in two cycles:…'\n",
      "\n",
      "=== 21 ===\n",
      "chunk.text:\n",
      "'Each LLM was run for three iterations across all 90 questions in the dataset. The SBC score details were recorded in JSON format, capturing key elements such as input requirements , reverse generated requirements , final accuracy score , semantic similarity , BLEU score , completeness score , missin…'\n",
      "chunker.contextualize(chunk):\n",
      "'4.1 Per-LLM Evaluation\\nEach LLM was run for three iterations across all 90 questions in the dataset. The SBC score details were recorded in JSON format, capturing key elements such as input requirements , reverse generated requirements , final accuracy score , semantic similarity , BLEU score , comp…'\n",
      "\n",
      "=== 22 ===\n",
      "chunk.text:\n",
      "'After evaluating all four LLMs, resulting in a total of 12 iterations, the results were consolidated into a final comparative analysis. For this, the maximum SBC score for each question within each LLM was extracted to form a representative dataset. A final consolidated chart was then created to com…'\n",
      "chunker.contextualize(chunk):\n",
      "'4.2 Cross-LLM Comparison\\nAfter evaluating all four LLMs, resulting in a total of 12 iterations, the results were consolidated into a final comparative analysis. For this, the maximum SBC score for each question within each LLM was extracted to form a representative dataset. A final consolidated char…'\n",
      "\n",
      "=== 23 ===\n",
      "chunk.text:\n",
      "'Figure 4: Consolidated score with max scores for all LLMs.\\nWe observed that reverse-generated requirements are of high quality and easy to interpret when SBC scores exceed 0.55 as shown in the following figure.\\nFigure 5: Sample output for SBC score above 0.55.\\nFor SBC scores above 0.65, the generate…'\n",
      "chunker.contextualize(chunk):\n",
      "'Code Specialized Models Consolidated\\nFigure 4: Consolidated score with max scores for all LLMs.\\nWe observed that reverse-generated requirements are of high quality and easy to interpret when SBC scores exceed 0.55 as shown in the following figure.\\nFigure 5: Sample output for SBC score above 0.55.\\nFo…'\n",
      "\n",
      "=== 24 ===\n",
      "chunk.text:\n",
      "'Given the significant benefits that the SBC score, along with missing and extra keyword analysis, can provide, it is highly beneficial for enterprises to integrate this process into the AI code assistant workflow. Instead of merely returning the generated code, AI code assistant responses can be mod…'\n",
      "chunker.contextualize(chunk):\n",
      "'4.3 Integrating SBC Score and Artifacts with the Application Development Life Cycle\\nGiven the significant benefits that the SBC score, along with missing and extra keyword analysis, can provide, it is highly beneficial for enterprises to integrate this process into the AI code assistant workflow. In…'\n",
      "\n",
      "=== 25 ===\n",
      "chunk.text:\n",
      "'In this study, we introduced a novel approach to evaluating LLM-generated code by leveraging reverse generation and the Semantic-BLEU-Completeness (SBC) hybrid metric. Our methodology provides a tool to validate if the code generation aligns closely with initial requirements by measuring semantic si…'\n",
      "chunker.contextualize(chunk):\n",
      "'5 Conclusion and Next Steps\\nIn this study, we introduced a novel approach to evaluating LLM-generated code by leveraging reverse generation and the Semantic-BLEU-Completeness (SBC) hybrid metric. Our methodology provides a tool to validate if the code generation aligns closely with initial requireme…'\n",
      "\n",
      "=== 26 ===\n",
      "chunk.text:\n",
      "'A key objective of this work is to provide a solution that serves both senior and junior developers by enabling them to assess the validity and completeness of LLM-generated code without the need for manual inspection of the code itself. Our approach allows developers to quickly evaluate whether the…'\n",
      "chunker.contextualize(chunk):\n",
      "'5 Conclusion and Next Steps\\nA key objective of this work is to provide a solution that serves both senior and junior developers by enabling them to assess the validity and completeness of LLM-generated code without the need for manual inspection of the code itself. Our approach allows developers to …'\n",
      "\n",
      "=== 27 ===\n",
      "chunk.text:\n",
      "'Despite the promising results, several areas warrant further exploration. While we relied on SBC scores to compare alignment between input requirements and generated code, human feedback remains an essential validation mechanism. Future work should incorporate qualitative assessments from software e…'\n",
      "chunker.contextualize(chunk):\n",
      "'5 Conclusion and Next Steps\\nDespite the promising results, several areas warrant further exploration. While we relied on SBC scores to compare alignment between input requirements and generated code, human feedback remains an essential validation mechanism. Future work should incorporate qualitative…'\n",
      "\n",
      "=== 28 ===\n",
      "chunk.text:\n",
      "'By bridging the gap between LLM-generated code and human defined requirements, our approach provides a structured and interpretable framework for evaluating AI-assisted development. As LLMs continue to evolve, refining hybrid metrics such as SBC and integrating human feedback will be crucial to ensu…'\n",
      "chunker.contextualize(chunk):\n",
      "'5 Conclusion and Next Steps\\nBy bridging the gap between LLM-generated code and human defined requirements, our approach provides a structured and interpretable framework for evaluating AI-assisted development. As LLMs continue to evolve, refining hybrid metrics such as SBC and integrating human feed…'\n",
      "\n",
      "=== 29 ===\n",
      "chunk.text:\n",
      "\"- [1] M. Evtikhiev, A. Pankevich, V. Zakharov, D. Chernobrov, and D. Ustalov, 'Out of the bleu: How should we assess quality of the code generation models?,' in Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering (ASE 2023) , 2023.\\n- [2] A. Hindle, E. T. Barr,…\"\n",
      "chunker.contextualize(chunk):\n",
      "\"References\\n- [1] M. Evtikhiev, A. Pankevich, V. Zakharov, D. Chernobrov, and D. Ustalov, 'Out of the bleu: How should we assess quality of the code generation models?,' in Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering (ASE 2023) , 2023.\\n- [2] A. Hindle, …\"\n",
      "\n",
      "=== 30 ===\n",
      "chunk.text:\n",
      "\"- [5] K. Papineni et al. , 'Bleu: a method for automatic evaluation of machine translation,' in Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics , pp. 311-318, 2002.\\n- [6] C.-Y. Lin, 'Rouge: A package for automatic evaluation of summaries,' in Text Summarizatio…\"\n",
      "chunker.contextualize(chunk):\n",
      "\"References\\n- [5] K. Papineni et al. , 'Bleu: a method for automatic evaluation of machine translation,' in Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics , pp. 311-318, 2002.\\n- [6] C.-Y. Lin, 'Rouge: A package for automatic evaluation of summaries,' in Text S…\"\n",
      "\n",
      "=== 31 ===\n",
      "chunk.text:\n",
      "\"- [10] S. Barke, M. B. James, and N. Polikarpova, 'Grounded copilot: How programmers interact with code-generating models,' arXiv preprint arXiv:2206.15000 , 2022.\\n- [11] V. Arghavan, F. Amin, and Z. Michel, 'Github copilot ai pair programmer: Asset or liability?,' arXiv preprint arXiv:2206.15331 , …\"\n",
      "chunker.contextualize(chunk):\n",
      "\"References\\n- [10] S. Barke, M. B. James, and N. Polikarpova, 'Grounded copilot: How programmers interact with code-generating models,' arXiv preprint arXiv:2206.15000 , 2022.\\n- [11] V. Arghavan, F. Amin, and Z. Michel, 'Github copilot ai pair programmer: Asset or liability?,' arXiv preprint arXiv:22…\"\n",
      "\n",
      "=== 32 ===\n",
      "chunk.text:\n",
      "\"- [15] X. Wang and D. Zhu, 'Validating llm-generated programs with metamorphic prompt testing,' arXiv preprint arXiv:2406.06864 , 2024.\\n- [16] K. Aiyyappa, S. Kumar, and V. Ramesh, 'A comprehensive survey on code generation techniques,' International Journal of Advanced Computer Science and Applicat…\"\n",
      "chunker.contextualize(chunk):\n",
      "\"References\\n- [15] X. Wang and D. Zhu, 'Validating llm-generated programs with metamorphic prompt testing,' arXiv preprint arXiv:2406.06864 , 2024.\\n- [16] K. Aiyyappa, S. Kumar, and V. Ramesh, 'A comprehensive survey on code generation techniques,' International Journal of Advanced Computer Science a…\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, chunk in enumerate(chunk_iter):\n",
    "    print(f\"=== {i} ===\")\n",
    "    print(f\"chunk.text:\\n{f'{chunk.text[:300]}…'!r}\")\n",
    "\n",
    "    enriched_text = chunker.contextualize(chunk=chunk)\n",
    "    print(f\"chunker.contextualize(chunk):\\n{f'{enriched_text[:300]}…'!r}\")\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bab9aa5-b709-4ff6-98e5-8aebf7759585",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93dfd8b8-34b9-4377-98aa-fe0b7fb8011a",
   "metadata": {},
   "source": [
    "# Vector Storage and Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da3aeba-3526-48e6-a2ff-c4d00aa2c72d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "054fbcd4-be4b-4a35-b8e3-9f33ea5d0d3d",
   "metadata": {},
   "source": [
    "# Query-Time Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1fbadd-29f1-4192-95f4-8b81232fa0c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8613c6a7-b781-4e48-a8af-41bb97b9f03c",
   "metadata": {},
   "source": [
    "# Augmented Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45528a5-4cbf-4f88-a720-aeb614923d4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
