{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac9b9a5e-7bb4-4334-a2f1-df28fbf031ba",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd443db6-4eca-42c3-b63a-be3286249a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -r requirements.txt\n",
    "\n",
    "# !pip install openai==1.93.0      # Only for testing\n",
    "# ! pip install --upgrade docling openai torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08964f15-4c5b-48da-aaed-8d0bb817fc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "from docling.document_converter import DocumentConverter\n",
    "from pathlib import Path\n",
    "from pymilvus import MilvusClient\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts.chat import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "import httpx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f4d9cd0-05a5-4073-b76a-7ab829679400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Configuration ===\n",
    "endpoint = os.getenv(\"AWS_S3_ENDPOINT\")           # MinIO service DNS name (e.g. minio.minio.svc.cluster.local)\n",
    "access_key = os.getenv(\"AWS_ACCESS_KEY_ID\")       # MinIO access key\n",
    "secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")   # MinIO secret key\n",
    "region = os.getenv(\"AWS_DEFAULT_REGION\")          # Dummy value; boto3 still expects one\n",
    "bucket_name = os.getenv(\"AWS_S3_BUCKET\")          # Default bucket to use for the Workspace data connection \n",
    "object_key = \"2502.07835v1.pdf\"                   # The name of the PDF in the S3 bucket\n",
    "download_dir = \"downloads\"                        # Location to download the dowuments to\n",
    "\n",
    "# RAG demo server URL\n",
    "inference_server_url = \"https://llama-32-3b-instruct-quantizedw8a8.rag-demo.svc.cluster.local/v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2ba0ab-7fd1-4a33-bea0-49eb2f846931",
   "metadata": {},
   "source": [
    "# Document Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b8599ad-7f3c-4dbd-b0d6-64f73535ab03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from rag-docs::2502.07835v1.pdf to: downloads/2502.07835v1.pdf\n",
      "🟢 INFO: Downloading documents for chunking and ingestion...\n",
      "✅ Downloaded '2502.07835v1.pdf' to 'downloads/2502.07835v1.pdf'\n"
     ]
    }
   ],
   "source": [
    "# === Initialise S3 client ===\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=f\"http://{endpoint}\",\n",
    "    aws_access_key_id=access_key,\n",
    "    aws_secret_access_key=secret_key,\n",
    "    region_name=region,\n",
    "    config=Config(signature_version=\"s3v4\"),\n",
    ")\n",
    "\n",
    "# === Ensure download directory exists ===\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "local_path = os.path.join(download_dir, object_key)\n",
    "print(f\"Downloading from {bucket_name}::{object_key} to: {local_path}\")\n",
    "\n",
    "# === Download the file ===\n",
    "try:\n",
    "    print(f\"🟢 INFO: Downloading documents for chunking and ingestion...\")\n",
    "    s3.download_file(bucket_name, object_key, local_path)\n",
    "    print(f\"✅ Downloaded '{object_key}' to '{local_path}'\")\n",
    "except s3.exceptions.NoSuchKey:\n",
    "    print(f\"❌ File '{object_key}' not found in bucket '{bucket_name}'\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error downloading file: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf2a9c2-784c-46aa-b7c4-b5a37dab7353",
   "metadata": {},
   "source": [
    "# Embedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cefa6e0-e615-429b-a25c-3a0e882a457c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SentenceTransformer for generating text embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\"\"\"\n",
    "Text Embedding Module  \n",
    "This module initialises a SentenceTransformer model using the ‘all-MiniLM-L6-v2’ embedding model and provides a function to generate text embeddings. (M.S. 0.98)\n",
    "\n",
    "Global Variables:\n",
    "    embedding_model (str): Name of the Hugging Face embedding model to load. (M.S. 0.98)\n",
    "    model (SentenceTransformer): Instance of SentenceTransformer initialised with the specified embedding model. (M.S. 0.98)\n",
    "\n",
    "Functions:\n",
    "    emb_text(text: str) -> list[float]:\n",
    "        Encode the input text and return its embedding vector as a list of floats. (M.S. 0.98)\n",
    "\"\"\"\n",
    "embedding_model=\"all-MiniLM-L6-v2\"\n",
    "model = SentenceTransformer(embedding_model)\n",
    "\n",
    "def emb_text(text: str) -> list[float]:\n",
    "    return model.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b5f32f6-8970-48e7-9d5e-a4d82f057ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🟢 INFO: Embedding dimensions for the all-MiniLM-L6-v2 model are: 384\n",
      "👀 DEBUG: Embedding snippet: [ 0.0306124   0.01383137 -0.02084381  0.01632793 -0.01023149 -0.04798423\n",
      " -0.01731336  0.03728744  0.04588732  0.034405  ]\n"
     ]
    }
   ],
   "source": [
    "# Use this to find the default number of dimensions this embedding model generates. We will use that later when we create the Milvus database schema.\n",
    "test_embedding = emb_text(\"This is a test\")\n",
    "embedding_dim = len(test_embedding)\n",
    "\n",
    "print(f\"🟢 INFO: Embedding dimensions for the {embedding_model} model are: {embedding_dim}\")\n",
    "print(f\"👀 DEBUG: Embedding snippet: {test_embedding[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4b07c68-85d1-46be-a14e-088ac5eab68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🟢 INFO: Found document at: /opt/app-root/src/rhoai-roadshow-v2/docs/2-rag/notebook/downloads/2502.07835v1.pdf\n"
     ]
    }
   ],
   "source": [
    "from utils import project_root\n",
    "\n",
    "# Assemble a complete path to the file so the document import can properly and reliably always find the document.\n",
    "doc_source = project_root() / local_path\n",
    "\n",
    "if not doc_source.is_file():\n",
    "    raise FileNotFoundError(f\"{DOC_SOURCE} does not exist.\")\n",
    "\n",
    "print(f\"🟢 INFO: Found document at: {doc_source}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1eb104d7-caf6-45d7-9b22-cfeb6a483d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parse and chunk a PDF using Docling v2.x\n",
    "\"\"\"\n",
    "doc = DocumentConverter().convert(source=doc_source).document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51b3591c-65eb-4556-a71b-0476c9b516b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🟢 INFO: {1: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=1), 2: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=2), 3: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=3), 4: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=4), 5: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=5), 6: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=6), 7: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=7), 8: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=8), 9: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=9), 10: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=10), 11: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=11), 12: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=12), 13: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=13)}\n"
     ]
    }
   ],
   "source": [
    "print(f\"🟢 INFO: {doc.pages}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7656184-67dd-41ce-83a9-3c3560337049",
   "metadata": {},
   "source": [
    "# Connect to Milvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bab9aa5-b709-4ff6-98e5-8aebf7759585",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"my_rag_collection\"\n",
    "\n",
    "milvus_client = MilvusClient(\n",
    "    uri=\"http://milvus-service.milvus.svc.cluster.local:19530\",\n",
    "    db_name=\"default\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07c126b9-5d76-46ee-8a11-5e16336fdf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "if milvus_client.has_collection(collection_name):\n",
    "    milvus_client.drop_collection(collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c527d363-59fb-4e09-a7c1-7960d2b276b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "milvus_client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    dimension=embedding_dim,\n",
    "    metric_type=\"IP\",  # Inner product distance\n",
    "    consistency_level=\"Strong\",  # Supported values are (`\"Strong\"`, `\"Session\"`, `\"Bounded\"`, `\"Eventually\"`). See https://milvus.io/docs/consistency.md#Consistency-Level for more details.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "663ab01b-5591-413c-9b58-c756bc412602",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling_core.transforms.chunker import HierarchicalChunker\n",
    "\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "converter = DocumentConverter()\n",
    "chunker = HierarchicalChunker()\n",
    "\n",
    "# Convert the input file to Docling Document\n",
    "source = doc_source\n",
    "doc = converter.convert(source).document\n",
    "\n",
    "# Perform hierarchical chunking. This is faster than Hybrid chunking, but not as good.\n",
    "texts = [chunk.text for chunk in chunker.chunk(doc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dfd8b8-34b9-4377-98aa-fe0b7fb8011a",
   "metadata": {},
   "source": [
    "# Vector Storage and Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7da3aeba-3526-48e6-a2ff-c4d00aa2c72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 70/70 [00:00<00:00, 197.46it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'insert_count': 70, 'ids': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69], 'cost': 0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "data = []\n",
    "\n",
    "for i, chunk in enumerate(tqdm(texts, desc=\"Processing chunks\")):\n",
    "    embedding = emb_text(chunk)\n",
    "    data.append({\"id\": i, \"vector\": embedding, \"text\": chunk})\n",
    "\n",
    "milvus_client.insert(collection_name=collection_name, data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24873e3-fe35-4f22-80a6-426ffc890765",
   "metadata": {},
   "source": [
    "# Visualising how embeddings are stored in a vector database\n",
    "\n",
    "<Describe how this visualises how the text is stored in the vector database.\n",
    "\n",
    "https://projector.tensorflow.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054fbcd4-be4b-4a35-b8e3-9f33ea5d0d3d",
   "metadata": {},
   "source": [
    "# Query-Time Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d1fbadd-29f1-4192-95f4-8b81232fa0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = (\n",
    "    \"What are the challenges of assessing assessing the quality of AI-generated code? What are some strategies for doing this?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd66032c-3d2b-4c87-936c-31031f4d84a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_res = milvus_client.search(\n",
    "    collection_name=collection_name,\n",
    "    data=[emb_text(question)],\n",
    "    limit=3,\n",
    "    search_params={\"metric_type\": \"IP\", \"params\": {}},\n",
    "    output_fields=[\"text\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e78eac4-fd36-4321-a629-f81f4120e4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🟢 INFO: Raw format of document chunks retrieved from the database: \n",
      "[\n",
      "    [\n",
      "        \"The rise of Large Language Models (LLMs) in software engineering, particularly in code generation, has garnered significant attention. However, assessing the quality of AI-generated code remains a challenge due to the inherent complexity of programming tasks and the lack of robust evaluation metrics that align well with human judgment. Traditional token-based metrics such as BLEU and ROUGE, while commonly used in natural language processing, exhibit weak correlations with human assessments in code intelligence and verification tasks. Furthermore, these metrics are primarily research focused and are not designed for seamless integration into the software development lifecycle, limiting their practical utility for developers seeking to improve code quality and security.\",\n",
      "        0.7107064723968506\n",
      "    ],\n",
      "    [\n",
      "        \"Inspired by G-EVAL , this paper proposes ICE-Score , an evaluation metric that leverages LLMs for code assessment across multiple programming languages, including Java, Python, C, C++, and JavaScript . The approach incorporates both human-centered usefulness and execution-based functional correctness , aiming to provide a holistic evaluation of generated code. ICE-Score refines the instruction-based evaluation paradigm for assessing AI-generated code in a structured manner.\",\n",
      "        0.6452593803405762\n",
      "    ],\n",
      "    [\n",
      "        \"The SBC score, along with the reverse-generated requirements, provides actionable insights for developers, helping them assess AI-generated code without requiring extensive reference implementations. Unlike prior evaluation methods, this approach inherently addresses the challenges of syntactic variations and alternative solutions in generated code, as highlighted in recent studies [12]. By\",\n",
      "        0.6442171335220337\n",
      "    ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "retrieved_lines_with_distances = [\n",
    "    (res[\"entity\"][\"text\"], res[\"distance\"]) for res in search_res[0]\n",
    "]\n",
    "print(f\"🟢 INFO: Raw format of document chunks retrieved from the database: \\n{json.dumps(retrieved_lines_with_distances, indent=4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8613c6a7-b781-4e48-a8af-41bb97b9f03c",
   "metadata": {},
   "source": [
    "# Augmented Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c45528a5-4cbf-4f88-a720-aeb614923d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🟢 INFO: Documents assembled into a single context for the LLM: : \"The rise of Large Language Models (LLMs) in software engineering, particularly in code generation, has garnered significant attention. However, assessing the quality of AI-generated code remains a challenge due to the inherent complexity of programming tasks and the lack of robust evaluation metrics that align well with human judgment. Traditional token-based metrics such as BLEU and ROUGE, while commonly used in natural language processing, exhibit weak correlations with human assessments in code intelligence and verification tasks. Furthermore, these metrics are primarily research focused and are not designed for seamless integration into the software development lifecycle, limiting their practical utility for developers seeking to improve code quality and security.\n",
      "Inspired by G-EVAL , this paper proposes ICE-Score , an evaluation metric that leverages LLMs for code assessment across multiple programming languages, including Java, Python, C, C++, and JavaScript . The approach incorporates both human-centered usefulness and execution-based functional correctness , aiming to provide a holistic evaluation of generated code. ICE-Score refines the instruction-based evaluation paradigm for assessing AI-generated code in a structured manner.\n",
      "The SBC score, along with the reverse-generated requirements, provides actionable insights for developers, helping them assess AI-generated code without requiring extensive reference implementations. Unlike prior evaluation methods, this approach inherently addresses the challenges of syntactic variations and alternative solutions in generated code, as highlighted in recent studies [12]. By\"\n"
     ]
    }
   ],
   "source": [
    "# Create the context that we will pass to the LLM along with the question so that it can generate a response.\n",
    "context = \"\\n\".join(\n",
    "    [line_with_distance[0] for line_with_distance in retrieved_lines_with_distances]\n",
    ")\n",
    "print(f\"🟢 INFO: Documents assembled into a single context for the LLM: : \\\"{context}\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48a52e88-eeb0-4766-bfd2-c1c718f008ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "  \"You are an AI assistant that answers questions based solely on the provided context. \"\n",
    "  \"If the answer cannot be found in context, reply truthfully that you don’t know.\"\n",
    ")\n",
    "\n",
    "USER_PROMPT = (\n",
    "  \"Context:\\n\"\n",
    "  \"{context}\\n\"\n",
    "  \"Question:\\n\"\n",
    "  \"{question}\\n\"\n",
    "  \"Answer concisely:\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3976092f-fd56-44f0-9822-d073acd5a245",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"llama-32-3b-instruct-quantizedw8a8\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    api_key=\"EMPTY\",  # if you prefer to pass api key in directly instaed of using env vars\n",
    "    base_url=inference_server_url,\n",
    "    http_client=httpx.Client(verify=False)    # Because we are using an internal API endpoint (service) we need to disable SSL certificate checking.\n",
    ")\n",
    "\n",
    "# Define system and human templates\n",
    "SYSTEM_PROMPT = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are an AI assistant that answers questions based solely on the provided context. \"\n",
    "    \"If the answer cannot be found in context, reply truthfully that you don’t know.\"\n",
    ")\n",
    "\n",
    "HumanMessagePromptTemplate = HumanMessagePromptTemplate.from_template(\n",
    "    \"Context:\\n\"\n",
    "    \"{context}\\n\"\n",
    "    \"Question:\\n\"\n",
    "    \"{question}\\n\"\n",
    "    \"Answer concisely:\"\n",
    ")\n",
    "\n",
    "# Combine into a chat prompt\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [SYSTEM_PROMPT, HumanMessagePromptTemplate]\n",
    ")\n",
    "\n",
    "prompt = chat_prompt.format_prompt(context=context, question=question)\n",
    "\n",
    "ai_msg = llm.invoke(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93c9a430-0118-4faa-a657-672927bcdda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original queston: What are the challenges of assessing assessing the quality of AI-generated code? What are some strategies for doing this?\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original queston: {question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6d6e266-fb32-4094-bd65-b9ea5ed05e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original context: \"The rise of Large Language Models (LLMs) in software engineering, particularly in code generation, has garnered significant attention. However, assessing the quality of AI-generated code remains a challenge due to the inherent complexity of programming tasks and the lack of robust evaluation metrics that align well with human judgment. Traditional token-based metrics such as BLEU and ROUGE, while commonly used in natural language processing, exhibit weak correlations with human assessments in code intelligence and verification tasks. Furthermore, these metrics are primarily research focused and are not designed for seamless integration into the software development lifecycle, limiting their practical utility for developers seeking to improve code quality and security.\n",
      "Inspired by G-EVAL , this paper proposes ICE-Score , an evaluation metric that leverages LLMs for code assessment across multiple programming languages, including Java, Python, C, C++, and JavaScript . The approach incorporates both human-centered usefulness and execution-based functional correctness , aiming to provide a holistic evaluation of generated code. ICE-Score refines the instruction-based evaluation paradigm for assessing AI-generated code in a structured manner.\n",
      "The SBC score, along with the reverse-generated requirements, provides actionable insights for developers, helping them assess AI-generated code without requiring extensive reference implementations. Unlike prior evaluation methods, this approach inherently addresses the challenges of syntactic variations and alternative solutions in generated code, as highlighted in recent studies [12]. By\"\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original context: \\\"{context}\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1bba1be1-409b-48e4-b84d-b5c87face1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from LLM: The challenges of assessing the quality of AI-generated code include:\n",
      "\n",
      "1. Complexity of programming tasks\n",
      "2. Lack of robust evaluation metrics that align with human judgment\n",
      "3. Weak correlations between traditional token-based metrics (e.g., BLEU, ROUGE) and human assessments\n",
      "\n",
      "Strategies for assessing AI-generated code include:\n",
      "\n",
      "1. Using human-centered evaluation metrics (e.g., ICE-Score) that incorporate both usefulness and functional correctness.\n",
      "2. Leveraging execution-based evaluation to assess the code's functionality.\n",
      "3. Providing actionable insights through reverse-generated requirements and SBC scores.\n",
      "4. Addressing syntactic variations and alternative solutions in generated code.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Response from LLM: {ai_msg.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b51ff2c-c438-4340-919a-de1c0651827d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
