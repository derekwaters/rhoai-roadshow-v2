{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72373e99-c9a9-44be-b0a8-d572666c4a0b",
   "metadata": {},
   "source": [
    "# üîß Environment Setup and Dependencies\n",
    "\n",
    "Before we begin, let's install the required packages. This RAG implementation uses several key libraries:\n",
    "\n",
    "- **`docling`**: Advanced PDF parsing and document structure extraction\n",
    "- **`sentence-transformers`**: Pre-trained models for text embeddings\n",
    "- **`pymilvus`**: Python client for Milvus vector database\n",
    "- **`langchain-openai`**: LLM integration and prompt templating\n",
    "- **`boto3`**: AWS/MinIO S3 client for object storage\n",
    "- **`httpx`**: HTTP client for API calls\n",
    "- **`tqdm`**: Progress bars for better user experience\n",
    "\n",
    "> **Note**: The installation may take a few minutes as it downloads pre-trained models and dependencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd443db6-4eca-42c3-b63a-be3286249a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.11.11 environment at: /opt/app-root\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m10 packages\u001b[0m \u001b[2min 21ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08964f15-4c5b-48da-aaed-8d0bb817fc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Core Python libraries\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Object storage and cloud services\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "# Document processing and parsing\n",
    "from docling.document_converter import DocumentConverter\n",
    "from docling_core.transforms.chunker.hierarchical_chunker import HierarchicalChunker\n",
    "\n",
    "# Vector database\n",
    "from pymilvus import connections, utility, Collection, CollectionSchema, FieldSchema, DataType\n",
    "\n",
    "# LLM integration and prompt management\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts.chat import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "# HTTP client for API calls\n",
    "import httpx\n",
    "\n",
    "# Matplotlib for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "# PCA for dimensionality reduction\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Progress tracking for user experience\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb50b9a4-9eae-4a1c-b8ef-d397ef7c0483",
   "metadata": {},
   "source": [
    "# ‚öôÔ∏è Configuration and Connection Setup\n",
    "\n",
    "This section configures the various services our RAG system depends on:\n",
    "\n",
    "## Object Storage Configuration\n",
    "- **MinIO/S3**: For storing and retrieving source documents\n",
    "- **Bucket and Object**: Specifies which document to process\n",
    "\n",
    "## Model and Database Configuration\n",
    "- **Embedding Model**: The SentenceTransformer model for text embeddings\n",
    "- **Vector Database**: Milvus connection details\n",
    "- **LLM Endpoint**: The inference server for generating responses\n",
    "\n",
    "> **Environment Variables**: These configurations are typically stored as environment variables for security and flexibility across different deployment environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f4d9cd0-05a5-4073-b76a-7ab829679400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Configuration loaded successfully!\n",
      "ü™£ Bucket containing files: rag-docs\n",
      "üìÅ Document to process: 2502.07835v1.pdf\n",
      "üîó Inference server: http://llama3-2-3b-predictor.llama-serving.svc.cluster.local:8080/v1\n",
      "üîó Inference model name: llama3-2-3b\n",
      "üóÉÔ∏è Vector database: http://milvus-service.milvus.svc.cluster.local:19530\n",
      "üß† Embedding model: all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "# ===== STORAGE CONFIGURATION =====\n",
    "# MinIO/S3 object storage settings - used for retrieving source documents\n",
    "endpoint = os.getenv(\"AWS_S3_ENDPOINT\")           # MinIO service DNS name (e.g. minio.minio.svc.cluster.local)\n",
    "access_key = os.getenv(\"AWS_ACCESS_KEY_ID\")       # MinIO access key credentials\n",
    "secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")   # MinIO secret key credentials\n",
    "region = os.getenv(\"AWS_DEFAULT_REGION\")          # AWS region (dummy value for MinIO)\n",
    "bucket_name = os.getenv(\"AWS_S3_BUCKET\")          # Bucket containing our source documents\n",
    "object_key = \"2502.07835v1.pdf\"                   # Specific PDF document to process (research paper on AI code assessment)\n",
    "download_dir = \"downloads\"                        # Local directory for downloaded documents\n",
    "\n",
    "# ===== MODEL AND INFERENCE CONFIGURATION =====\n",
    "# LLM inference server - provides the generation capabilities for RAG responses\n",
    "# Default to a known default if the environment variable is not defined.\n",
    "inference_server_url = os.getenv(\"INFERENCE_SERVER_URL\", \"http://llama3-2-3b-predictor.llama-serving.svc.cluster.local:8080/v1\")\n",
    "inference_server_model_name = os.getenv(\"INFERENCE_SERVER_MODEL_NAME\", \"llama3-2-3b\")\n",
    "\n",
    "# Vector database configuration\n",
    "milvus_url = \"http://milvus-service.milvus.svc.cluster.local:19530\"\n",
    "collection_name = \"my_rag_collection\"\n",
    "\n",
    "# Embedding model configuration\n",
    "embedding_model_name = \"all-MiniLM-L6-v2\"  # Lightweight, effective model for semantic similarity\n",
    "\n",
    "print(\"üîß Configuration loaded successfully!\")\n",
    "print(f\"ü™£ Bucket containing files: {bucket_name}\")\n",
    "print(f\"üìÅ Document to process: {object_key}\")\n",
    "print(f\"üîó Inference server: {inference_server_url}\")\n",
    "print(f\"üîó Inference model name: {inference_server_model_name}\")\n",
    "print(f\"üóÉÔ∏è Vector database: {milvus_url}\")\n",
    "print(f\"üß† Embedding model: {embedding_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed065a2f-157f-491b-b2d2-923a21566cfb",
   "metadata": {},
   "source": [
    "## ü™£ Connect to the s3 storage to retrieve the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23873c03-a3a7-48ff-a66d-f96b8de90e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Preparing to download document...\n",
      "   Source: rag-docs/2502.07835v1.pdf\n",
      "   Destination: downloads/2502.07835v1.pdf\n",
      "üîÑ Downloading document for processing...\n",
      "‚úÖ Successfully downloaded '2502.07835v1.pdf' to 'downloads/2502.07835v1.pdf'\n",
      "üìä File size: 1,870,265 bytes (1.8 MB)\n"
     ]
    }
   ],
   "source": [
    "# ===== INITIALIZE S3/MINIO CLIENT =====\n",
    "# Create boto3 client configured for MinIO (S3-compatible object storage)\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=f\"http://{endpoint}\",        # MinIO endpoint URL\n",
    "    aws_access_key_id=access_key,             # Authentication credentials\n",
    "    aws_secret_access_key=secret_key,\n",
    "    region_name=region,                       # Required by boto3, but not used by MinIO\n",
    "    config=Config(signature_version=\"s3v4\"),  # S3 signature version for authentication\n",
    ")\n",
    "\n",
    "# ===== PREPARE LOCAL STORAGE =====\n",
    "# Create local directory to store downloaded documents\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "local_path = os.path.join(download_dir, object_key)\n",
    "\n",
    "print(f\"üì• Preparing to download document...\")\n",
    "print(f\"   Source: {bucket_name}/{object_key}\")\n",
    "print(f\"   Destination: {local_path}\")\n",
    "\n",
    "# ===== DOWNLOAD DOCUMENT =====\n",
    "# Download the PDF document from object storage with proper error handling\n",
    "try:\n",
    "    print(f\"üîÑ Downloading document for processing...\")\n",
    "    s3.download_file(bucket_name, object_key, local_path)\n",
    "    print(f\"‚úÖ Successfully downloaded '{object_key}' to '{local_path}'\")\n",
    "    \n",
    "    # Verify the file was downloaded and get its size\n",
    "    file_size = os.path.getsize(local_path)\n",
    "    print(f\"üìä File size: {file_size:,} bytes ({file_size/1024/1024:.1f} MB)\")\n",
    "    \n",
    "except s3.exceptions.NoSuchKey:\n",
    "    print(f\"‚ùå ERROR: File '{object_key}' not found in bucket '{bucket_name}'\")\n",
    "    print(\"   Please check the bucket name and object key are correct.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR: Failed to download file: {e}\")\n",
    "    print(\"   Please check your MinIO/S3 configuration and network connectivity.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32670b30-629f-4563-b44f-e71d8b04aaff",
   "metadata": {},
   "source": [
    "---\n",
    "# üìÑ Docling: Preparing Text for RAG Systems\n",
    "\n",
    "**Docling** is used to prepare documents for Retrieval-Augmented Generation (RAG) systems.\n",
    "\n",
    "Text documents such as PDFs, HTML pages, or plain text need to be **converted into chunks** that can be embedded and stored in a vector database. Docling simplifies this process by extracting, cleaning, and chunking content into well-structured semantic units.\n",
    "\n",
    "For this lab, Docling will be used to prepare documents before they are embedded and indexed in Milvus.\n",
    "\n",
    "---\n",
    "\n",
    "## What is Docling?\n",
    "\n",
    "**Docling** is a Python library designed for **document preparation** in GenAI pipelines. Its role is to transform unstructured content into structured, embeddable units, typically for use in:\n",
    "\n",
    "- **RAG applications**: Extracting retrievable units from long documents\n",
    "- **Semantic search pipelines**: Chunking content into searchable segments\n",
    "- **LLM input pipelines**: Creating well-scoped context windows\n",
    "\n",
    "Docling is particularly useful for converting complex documents like PDFs into consistent formats that can be embedded by LLMs.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Use Docling?\n",
    "\n",
    "**Docling** provides:\n",
    "\n",
    "- **Multi-format support**: PDF, DOCX, TXT, Markdown, HTML\n",
    "- **Intelligent chunking**: Preserves context while segmenting content\n",
    "- **Metadata extraction**: Title, headers, sections, and more\n",
    "- **Customisable workflows**: Fine-tune chunk size, overlap, and cleaning\n",
    "- **Streamlined RAG integration**: Designed to fit directly into vector pipelines\n",
    "\n",
    "---\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### Chunking\n",
    "- Breaks large documents into smaller parts for efficient retrieval.\n",
    "- Can be based on token count, sentence boundaries, or structure (e.g. headings).\n",
    "- Prevents LLMs from exceeding context window limits.\n",
    "\n",
    "### Overlap\n",
    "- Ensures context is preserved across chunks.\n",
    "- Helpful in maintaining continuity of thought or narrative.\n",
    "\n",
    "### Metadata\n",
    "- Docling can attach metadata (e.g. file name, page number, section header) to each chunk.\n",
    "- Useful for traceability and debugging in RAG outputs.\n",
    "\n",
    "---\n",
    "\n",
    "## Typical Docling Workflow\n",
    "\n",
    "1. **Load the document**\n",
    "   - From local files or URLs\n",
    "\n",
    "2. **Parse and clean**\n",
    "   - Normalise spacing, remove boilerplate, handle special characters\n",
    "\n",
    "3. **Chunk**\n",
    "   - Create segments that fit within model context limits\n",
    "\n",
    "4. **Enrich**\n",
    "   - Add metadata such as section titles or page numbers\n",
    "\n",
    "5. **Output**\n",
    "   - Return a list of chunk objects ready for embedding\n",
    "\n",
    "---\n",
    "\n",
    "Let‚Äôs use Docling to transform raw documents into structured, retrievable content!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a8e142-c422-4a84-bea7-887aa9862ba8",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Load the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8144c5f0-b14e-480f-8d69-adbc793c336e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üü¢ INFO: Found document at: /opt/app-root/src/rhoai-roadshow-v2/docs/2-rag/notebook/downloads/2502.07835v1.pdf\n"
     ]
    }
   ],
   "source": [
    "from utils import project_root\n",
    "\n",
    "# Assemble a complete path to the file so the document import can properly and reliably always find the document.\n",
    "doc_source = project_root() / local_path\n",
    "\n",
    "if not doc_source.is_file():\n",
    "    raise FileNotFoundError(f\"{doc_source} does not exist.\")\n",
    "\n",
    "print(f\"üü¢ INFO: Found document at: {doc_source}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7e4aa2-2651-480d-b92b-bbd54463b3d6",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Parse and clean\n",
    "First we parse the document and store it in an internal Docling format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c36081b3-4b28-40f7-9ef4-1c26ad36a50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üü¢ INFO: {1: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=1), 2: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=2), 3: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=3), 4: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=4), 5: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=5), 6: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=6), 7: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=7), 8: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=8), 9: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=9), 10: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=10), 11: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=11), 12: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=12), 13: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=13)}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Parse and chunk a PDF using Docling v2.x\n",
    "\"\"\"\n",
    "doc = DocumentConverter().convert(source=doc_source).document\n",
    "\n",
    "print(f\"üü¢ INFO: {doc.pages}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723a7e45-a680-413c-9d18-600afdb90570",
   "metadata": {},
   "source": [
    "--- \n",
    "## 3. Chunk the document\n",
    "This code loads the document and converts it to chunks that can be invididually embedded.\n",
    "Once the document is chunked it can be stored in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd0e5e2c-f062-433d-a8a1-d6ce7664392a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Processing document with Docling...\n",
      "   Converting PDF: /opt/app-root/src/rhoai-roadshow-v2/docs/2-rag/notebook/downloads/2502.07835v1.pdf\n",
      "üìä DOCUMENT ANALYSIS:\n",
      "   ‚Ä¢ Total pages: 13\n",
      "   ‚Ä¢ Document type: <class 'docling_core.types.doc.document.DoclingDocument'>\n",
      "   ‚Ä¢ Processing strategy: Structure-aware parsing\n",
      "\n",
      "üî™ Chunking document into smaller pieces...\n",
      "   ‚Ä¢ Strategy: Hierarchical chunking\n",
      "   ‚Ä¢ Benefits: Preserves document structure and context\n",
      "üìà CHUNKING RESULTS:\n",
      "   ‚Ä¢ Total chunks created: 70\n",
      "   ‚Ä¢ Average chunk length: 326 characters\n",
      "   ‚Ä¢ Shortest chunk: 6 characters\n",
      "   ‚Ä¢ Longest chunk: 1545 characters\n",
      "\n",
      "üìù SAMPLE CHUNK (first 200 characters):\n",
      "   \"ahilanp@gmail.com...\"\n",
      "\n",
      "‚úÖ Document processing complete! Ready for embedding generation.\n"
     ]
    }
   ],
   "source": [
    "# ===== DOCUMENT PROCESSING AND CHUNKING =====\n",
    "# Now let's process our PDF document and break it into searchable chunks\n",
    "print(\"üìÑ Processing document with Docling...\")\n",
    "\n",
    "# Initialize document converter and chunker\n",
    "converter = DocumentConverter()\n",
    "chunker = HierarchicalChunker()\n",
    "\n",
    "# Convert the PDF to a structured document object\n",
    "print(f\"   Converting PDF: {doc_source}\")\n",
    "doc = converter.convert(source=doc_source).document\n",
    "\n",
    "# Analyze document structure\n",
    "print(f\"üìä DOCUMENT ANALYSIS:\")\n",
    "print(f\"   ‚Ä¢ Total pages: {len(doc.pages)}\")\n",
    "print(f\"   ‚Ä¢ Document type: {type(doc)}\")\n",
    "\n",
    "# Document metadata would be available here if the document contained it\n",
    "print(f\"   ‚Ä¢ Processing strategy: Structure-aware parsing\")\n",
    "\n",
    "# Perform hierarchical chunking\n",
    "print(f\"\\nüî™ Chunking document into smaller pieces...\")\n",
    "print(f\"   ‚Ä¢ Strategy: Hierarchical chunking\")\n",
    "print(f\"   ‚Ä¢ Benefits: Preserves document structure and context\")\n",
    "\n",
    "# Extract text chunks from the document\n",
    "texts = [chunk.text for chunk in chunker.chunk(doc)]\n",
    "\n",
    "print(f\"üìà CHUNKING RESULTS:\")\n",
    "print(f\"   ‚Ä¢ Total chunks created: {len(texts)}\")\n",
    "print(f\"   ‚Ä¢ Average chunk length: {sum(len(text) for text in texts) / len(texts):.0f} characters\")\n",
    "print(f\"   ‚Ä¢ Shortest chunk: {min(len(text) for text in texts)} characters\")\n",
    "max_chunk_size = max(len(text) for text in texts)             # This is needed for defining the Milvus schema.\n",
    "print(f\"   ‚Ä¢ Longest chunk: {max_chunk_size} characters\")\n",
    "\n",
    "# Show a sample chunk\n",
    "print(f\"\\nüìù SAMPLE CHUNK (first 200 characters):\")\n",
    "print(f\"   \\\"{texts[0][:200]}...\\\"\")\n",
    "\n",
    "print(f\"\\n‚úÖ Document processing complete! Ready for embedding generation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f25d9d7-a5af-4cb2-94a1-1235bb2c1c40",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Enrich\n",
    "We won't be doing any enrichment in this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8864db5-ee23-4dea-8139-cd58a3d52f80",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Output\n",
    "Because we are not enriching the documents we already have the output from step 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3b8066-e98f-4fa3-888c-5626d95cb483",
   "metadata": {},
   "source": [
    "---\n",
    "# Embedding the the document chunks\n",
    "As we saw in the previous activity, whenever we store our objects in a vector database we need to convert them to a vector. To do that we need an embedding model.\n",
    "\n",
    "## Configure the transformer model for embedding the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4ac0725-49c2-4e4c-86d0-a89e344f0aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Select the model we will use for embedding chunks\n",
    "embedding_model=\"all-MiniLM-L6-v2\"\n",
    "model = SentenceTransformer(embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cefa6e0-e615-429b-a25c-3a0e882a457c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we have created a simple helper function `emb_text()` that we will use when we embed the chunks.\n",
    "\"\"\"\n",
    "Text Embedding Module  \n",
    "This module initialises a SentenceTransformer model using the ‚Äòall-MiniLM-L6-v2‚Äô embedding model and provides a function to generate text embeddings.\n",
    "\n",
    "Global Variables:\n",
    "    embedding_model (str): Name of the Hugging Face embedding model to load.\n",
    "    model (SentenceTransformer): Instance of SentenceTransformer initialised with the specified embedding model.\n",
    "\n",
    "Functions:\n",
    "    emb_text(text: str) -> list[float]:\n",
    "        Encode the input text and return its embedding vector as a list of floats.\n",
    "\"\"\"\n",
    "def emb_text(text: str) -> list[float]:\n",
    "    return model.encode(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25fce4f-8811-41c3-a93f-bc5d963708cc",
   "metadata": {},
   "source": [
    "## Test the embedding is working and also extract the dimensions\n",
    "This next code not only tests the embedding is working, but also determines the dimensions that the embedding model generates. We need that number for when we define the vector database schema later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c942a6e-18e7-4a1b-983d-c9bfbd32eb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing embedding function with sample text...\n",
      "üìä EMBEDDING ANALYSIS:\n",
      "   ‚Ä¢ Input text: 'This is a test sentence to demonstrate text embeddings.'\n",
      "   ‚Ä¢ Embedding dimensions: 384\n",
      "   ‚Ä¢ Data type: <class 'numpy.ndarray'>\n",
      "   ‚Ä¢ Sample values: [-0.00573698  0.00202521  0.07564172  0.0383721   0.02895643  0.0611613\n",
      " -0.01208316 -0.01273861  0.02499382 -0.04949658]\n",
      "   ‚Ä¢ Value range: [-0.1730, 0.1419]\n"
     ]
    }
   ],
   "source": [
    "# ===== EMBEDDING DIMENSION EXPLORATION =====\n",
    "# Test our embedding function to understand the output format and dimensions\n",
    "# This information is crucial for configuring the vector database schema\n",
    "\n",
    "print(\"üß™ Testing embedding function with sample text...\")\n",
    "test_text = \"This is a test sentence to demonstrate text embeddings.\"\n",
    "test_embedding = emb_text(test_text)\n",
    "embedding_dim = len(test_embedding)\n",
    "\n",
    "print(f\"üìä EMBEDDING ANALYSIS:\")\n",
    "print(f\"   ‚Ä¢ Input text: '{test_text}'\")\n",
    "print(f\"   ‚Ä¢ Embedding dimensions: {embedding_dim}\")    # We need this value when we define the Milvus schema\n",
    "print(f\"   ‚Ä¢ Data type: {type(test_embedding)}\")\n",
    "print(f\"   ‚Ä¢ Sample values: {test_embedding[:10]}\")\n",
    "print(f\"   ‚Ä¢ Value range: [{min(test_embedding):.4f}, {max(test_embedding):.4f}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cd9b17-08d6-4c43-868e-3b7d3bc9f64e",
   "metadata": {},
   "source": [
    "## Create the embeddings (vectors) for all of the document chunks \n",
    "Iterate through all of the chunks and creayte an embedding vecotr for them. Later we will store these in the vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43c3ea5d-a0ca-4eb3-b732-0fa654803b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Generating embeddings for all document chunks...\n",
      "   Processing 70 chunks with all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üßÆ Embedding chunks: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 70/70 [00:00<00:00, 185.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä EMBEDDING STATISTICS:\n",
      "   ‚Ä¢ Total chunks embedded: 70\n",
      "   ‚Ä¢ Total words processed: 3,313\n",
      "   ‚Ä¢ Average words per chunk: 47.3\n",
      "   ‚Ä¢ Embedding dimensions: 384\n",
      "   ‚Ä¢ Memory usage: ~0.1 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ===== EMBEDDING GENERATION AND STORAGE =====\n",
    "print(\"üîÑ Generating embeddings for all document chunks...\")\n",
    "print(f\"   Processing {len(texts)} chunks with {embedding_model_name}\")\n",
    "\n",
    "# Prepare data structure for batch insertion (Milvus expects a list of dicts)\n",
    "data = []\n",
    "embeddings_list = []  # üîπ Collect embeddings for plotting PCA below\n",
    "total_tokens = 0\n",
    "doc_name = object_key  # This is the file name that we are embedding\n",
    "\n",
    "# Process each chunk: generate embedding and prepare for storage\n",
    "for i, chunk in enumerate(tqdm(texts, desc=\"üßÆ Embedding chunks\")):\n",
    "    # Generate embedding for this chunk\n",
    "    embedding = emb_text(chunk)\n",
    "    \n",
    "    # Add embedding to list for PCA plotting below\n",
    "    embeddings_list.append(embedding)\n",
    "\n",
    "    # Prepare record for Milvus\n",
    "    data.append({\n",
    "        \"id\": i,                             # Unique identifier for this chunk\n",
    "        \"embedding\": embedding.tolist(),     # Convert the vectyor to a list for Milvus\n",
    "        \"original_text\" : chunk,             # The original text in the chunk\n",
    "        \"metadata\": doc_name                 # Constant metadata\n",
    "    })\n",
    "    \n",
    "    # Track statistics\n",
    "    total_tokens += len(chunk.split())\n",
    "\n",
    "# Display embedding statistics\n",
    "print(f\"\\nüìä EMBEDDING STATISTICS:\")\n",
    "print(f\"   ‚Ä¢ Total chunks embedded: {len(data)}\")\n",
    "print(f\"   ‚Ä¢ Total words processed: {total_tokens:,}\")\n",
    "print(f\"   ‚Ä¢ Average words per chunk: {total_tokens/len(data):.1f}\")\n",
    "print(f\"   ‚Ä¢ Embedding dimensions: {len(data[0]['embedding'])}\")\n",
    "print(f\"   ‚Ä¢ Memory usage: ~{len(data) * len(data[0]['embedding']) * 4 / 1024 / 1024:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879a3f89-8b21-4770-bb2d-96412d41613a",
   "metadata": {},
   "source": [
    "# üìä Visualizing Vector Embeddings (Optional Educational Step)\n",
    "\n",
    "## Understanding Vector Spaces\n",
    "\n",
    "Embeddings exist in high-dimensional space (384 dimensions in our case), which is difficult to visualize directly. However, we can use dimensionality reduction techniques to project these vectors into 2D or 3D space for visualization.\n",
    "\n",
    "## Tools for Visualization\n",
    "\n",
    "### TensorFlow Projector\n",
    "- **URL**: https://projector.tensorflow.org/\n",
    "- **Purpose**: Interactive visualization of high-dimensional embeddings\n",
    "- **Features**: \n",
    "  - PCA and t-SNE dimensionality reduction\n",
    "  - Color-coding and clustering\n",
    "  - Interactive exploration of vector neighborhoods\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. **Dimensionality Reduction**: Algorithms like PCA or t-SNE compress 384D vectors to 2D/3D\n",
    "2. **Semantic Clustering**: Similar concepts appear close together in the visualization\n",
    "3. **Interactive Exploration**: Click on points to see the original text and find similar chunks\n",
    "\n",
    "## What You Would See\n",
    "\n",
    "- **Document Clusters**: Related sections of the paper grouped together\n",
    "- **Concept Boundaries**: Clear separation between different topics\n",
    "- **Similarity Relationships**: Semantic connections between text chunks\n",
    "\n",
    "## Educational Value\n",
    "\n",
    "Visualizing embeddings helps you understand:\n",
    "- How semantic similarity translates to geometric proximity\n",
    "- Why vector search is effective for finding related content\n",
    "- The relationship between embedding quality and retrieval performance\n",
    "\n",
    "> **Note**: While visualization is helpful for understanding, it's optional for the RAG system functionality. The vector database performs searches directly in the high-dimensional space without needing to reduce dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "503b7fee-46f8-4c85-a886-45b35ed6b576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualise our embeddings using dimensionality reduction to plot the embedded chunks in two dimensions.  \n",
    "# Unlike the previous exercise, we won't print the text with the plot because the text is too long.\n",
    "pca = PCA(n_components=2)\n",
    "reduced = pca.fit_transform(embeddings_list)\n",
    "plt.scatter(reduced[:, 0], reduced[:, 1])\n",
    "plt.title(\"PCA Projection of Embeddings\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7656184-67dd-41ce-83a9-3c3560337049",
   "metadata": {},
   "source": [
    "---\n",
    "# Connect to Milvus\n",
    "We want to store all of the embedded chunks in the vector database. Let's create a new database to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf4918fc-52dd-492d-9751-edafe7d5d5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully connected to Milvus!\n",
      "   Endpoint: http://milvus-service.milvus.svc.cluster.local:19530\n",
      "   Existing collections: ['vectordb_collection', 'rag_vector_db', 'my_rag_collection']\n",
      "   Target collection: my_rag_collection\n"
     ]
    }
   ],
   "source": [
    "# Connect to Milvus first\n",
    "connections.connect(alias=\"simple_rag\", uri=milvus_url)\n",
    "\n",
    "# Test the connection\n",
    "try:\n",
    "    # List existing collections to verify connectivity\n",
    "    existing_collections = utility.list_collections(using=\"simple_rag\")\n",
    "    \n",
    "    print(f\"‚úÖ Successfully connected to Milvus!\")\n",
    "    print(f\"   Endpoint: {milvus_url}\")\n",
    "    print(f\"   Existing collections: {existing_collections}\")\n",
    "    print(f\"   Target collection: {collection_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to connect to Milvus: {e}\")\n",
    "    print(\"   Please check that Milvus is running and accessible.\")\n",
    "    sys.exit(\"‚ùå Fatal error, stopping execution\")\n",
    "\n",
    "# Define the collection name\n",
    "collection_name = \"vectordb_collection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ab52897-6457-4167-b256-14026cb82dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Collection 'vectordb_collection' already exists. Dropping for clean run.\n",
      "üìö Collection list: ['rag_vector_db', 'my_rag_collection']\n"
     ]
    }
   ],
   "source": [
    "# If the collection already exists, drop it for a clean start (optional)\n",
    "if utility.has_collection(collection_name, using=\"simple_rag\"):\n",
    "    print(f\"‚ö†Ô∏è Collection '{collection_name}' already exists. Dropping for clean run.\")\n",
    "    utility.drop_collection(collection_name, using=\"simple_rag\")\n",
    "\n",
    "print(f\"üìö Collection list: {utility.list_collections(using='simple_rag')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4b6e52-b55d-43d5-9e55-1ce126387e4b",
   "metadata": {},
   "source": [
    "## Milvus Setup: Define the schema now we have created it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f0bb843-cbbf-4d2a-b199-a52db9428b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Collection list: ['rag_vector_db', 'my_rag_collection', 'vectordb_collection']\n"
     ]
    }
   ],
   "source": [
    "# Define schema after connecting\n",
    "id_field = FieldSchema(\n",
    "    name=\"id\",\n",
    "    dtype=DataType.INT64,\n",
    "    is_primary=True,\n",
    "    auto_id=False\n",
    ")\n",
    "\n",
    "# This will contain the embedding vectors\n",
    "embedding_field = FieldSchema(\n",
    "        name=\"embedding\",\n",
    "        dtype=DataType.FLOAT_VECTOR,\n",
    "        dim=embedding_dim\n",
    "    )\n",
    "\n",
    "# This will contain the original text so we can use it with the LLM.\n",
    "text_field = FieldSchema(\n",
    "    name=\"original_text\",\n",
    "    dtype=DataType.VARCHAR,\n",
    "    max_length=max_chunk_size    # This is calculated when we chunked the document.\n",
    ")\n",
    "\n",
    "# This will contain the embedding metadata (source document in our example)\n",
    "doc_field = FieldSchema(\n",
    "    name=\"metadata\",\n",
    "    dtype=DataType.VARCHAR,\n",
    "    max_length=256               # This is a safe size for a URL.\n",
    ")\n",
    "\n",
    "# Create the schema using the fields defined above\n",
    "schema = CollectionSchema(\n",
    "    fields=[id_field, embedding_field, text_field, doc_field],\n",
    "    description=\"Simple RAG Schema\",\n",
    "    enable_dynamic_field=False\n",
    ")\n",
    "\n",
    "# ‚úÖ Create the collection now\n",
    "collection = Collection(\n",
    "    name=collection_name,\n",
    "    schema=schema,\n",
    "    using=\"simple_rag\",\n",
    "    shards_num=2\n",
    ")\n",
    "\n",
    "# Create an index on the embedding attribute\n",
    "collection.create_index(\n",
    "    field_name=\"embedding\",\n",
    "    index_params={\n",
    "        \"metric_type\": \"COSINE\",\n",
    "        \"index_type\": \"IVF_FLAT\",\n",
    "        \"params\": {\"nlist\": 128}\n",
    "    },\n",
    "    index_name=\"idx\"\n",
    ")\n",
    "\n",
    "print(f\"üìö Collection list: {utility.list_collections(using='simple_rag')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9a0355-758e-4b7e-b6e9-6c5703391622",
   "metadata": {},
   "source": [
    "### Describe the collection we just created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c527d363-59fb-4e09-a7c1-7960d2b276b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Collection description: Simple RAG Schema\n",
      "üìÑ Collection schema: {'auto_id': False, 'description': 'Simple RAG Schema', 'fields': [{'name': 'id', 'description': '', 'type': <DataType.INT64: 5>, 'is_primary': True, 'auto_id': False}, {'name': 'embedding', 'description': '', 'type': <DataType.FLOAT_VECTOR: 101>, 'params': {'dim': 384}}, {'name': 'original_text', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 1545}}, {'name': 'metadata', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 256}}], 'enable_dynamic_field': False}\n",
      "üìÑ Number of entities: 0\n",
      "üìÑ Indexes: [<pymilvus.orm.index.Index object at 0x7f2cccbe48d0>]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print collection information\n",
    "print(\"üìÑ Collection description:\", collection.description)\n",
    "print(\"üìÑ Collection schema:\", collection.schema)\n",
    "print(\"üìÑ Number of entities:\", collection.num_entities)\n",
    "\n",
    "# List indexes\n",
    "print(\"üìÑ Indexes:\", collection.indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dfd8b8-34b9-4377-98aa-fe0b7fb8011a",
   "metadata": {},
   "source": [
    "# üíæ Vector Storage and Search\n",
    "\n",
    "## Understanding Vector Storage\n",
    "\n",
    "Now that we have processed our document into chunks, we need to:\n",
    "\n",
    "1. **Convert each chunk to embeddings**: Transform text into numerical vectors\n",
    "2. **Store in vector database**: Save embeddings with metadata for efficient retrieval\n",
    "3. **Index for search**: Prepare the database for fast similarity queries\n",
    "\n",
    "## The Embedding Process\n",
    "\n",
    "For each text chunk, we will:\n",
    "- **Generate embeddings** using our SentenceTransformer model\n",
    "- **Store the vector** along with the original text in Milvus\n",
    "- **Create an index** for fast similarity search\n",
    "\n",
    "## Why This Approach Works\n",
    "\n",
    "- **Semantic Understanding**: Vector representations capture meaning, not just keywords\n",
    "- **Scalability**: Vector databases handle millions of embeddings efficiently\n",
    "- **Fast Retrieval**: Approximate nearest neighbor search provides quick results\n",
    "- **Flexibility**: Easy to update, add, or remove documents\n",
    "\n",
    "Let's embed our document chunks and store them in the vector database!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7da3aeba-3526-48e6-a2ff-c4d00aa2c72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Storing embeddings in Milvus vector database...\n",
      "   Records to insert: 70\n",
      "‚úÖ STORAGE COMPLETE!\n",
      "   Total vectors stored: 70\n",
      "üìÑ Collection stats: {'name': 'vectordb_collection', 'description': 'Simple RAG Schema', 'num_entities': 70, 'fields': ['id', 'embedding', 'original_text', 'metadata']}\n",
      "\n",
      "üîç Vector database is ready for similarity search!\n"
     ]
    }
   ],
   "source": [
    "# ===== BATCH INSERT TO MILVUS =====\n",
    "print(f\"\\nüíæ Storing embeddings in Milvus vector database...\")\n",
    "print(f\"   Records to insert: {len(data)}\")\n",
    "\n",
    "# We created the embeddings earlier and stored them in the \"data\" array. Now we will do a batch insert into the vector database.\n",
    "insert_result = collection.insert(data=data)\n",
    "\n",
    "# Commit the update to the database\n",
    "collection.flush()\n",
    "collection.load()\n",
    "\n",
    "print(f\"‚úÖ STORAGE COMPLETE!\")\n",
    "print(f\"   Total vectors stored: {insert_result.insert_count}\")\n",
    "\n",
    "# Verify the data was inserted correctly\n",
    "collection_stats = {\n",
    "    \"name\": collection.name,\n",
    "    \"description\": collection.description,\n",
    "    \"num_entities\": collection.num_entities,\n",
    "    \"fields\": [field.name for field in collection.schema.fields]\n",
    "}\n",
    "print(\"üìÑ Collection stats:\", collection_stats)\n",
    "\n",
    "print(f\"\\nüîç Vector database is ready for similarity search!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4ff147-0a52-49e4-957a-f9c1a568c78f",
   "metadata": {},
   "source": [
    "---\n",
    "# ‚è∏Ô∏è Checkpoint\n",
    "Let's pause and recap what we have done so far...\n",
    "\n",
    "At this point we have loaded a PDF document and converted it to chunks using Docling. We then generated vectors (embeddings) for each chunk and stored these in our vector database, along with the original text and some metadata that identifies the source document for each chunk.  \n",
    "\n",
    "In our exercise we are only working with a single document. But in the real world you would have multiple documents and thus the metadata becomes very important so you can find what the LLM used to generate its response.\n",
    "\n",
    "In the next section you will see how we can answer questions of our documents using the vector database and the large-language capabilities of the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054fbcd4-be4b-4a35-b8e3-9f33ea5d0d3d",
   "metadata": {},
   "source": [
    "___\n",
    "# üîç Query-Time Retrieval\n",
    "\n",
    "## How RAG Retrieval Works\n",
    "\n",
    "When a user asks a question, the RAG system performs the following steps:\n",
    "\n",
    "1. **Query Embedding**: Convert the user's question into the same vector space as our documents\n",
    "2. **Similarity Search**: Find the most similar document chunks using vector distance\n",
    "3. **Ranking**: Sort results by relevance score (similarity distance)\n",
    "4. **Selection**: Choose the top-k most relevant chunks for context\n",
    "\n",
    "## Vector Search Process\n",
    "\n",
    "### Step 1: Query Embedding\n",
    "- Use the **same embedding model** that was used for document chunks\n",
    "- This ensures query and document vectors are in the same semantic space\n",
    "\n",
    "### Step 2: Distance Calculation\n",
    "- **Inner Product (IP)**: Our chosen metric - higher values mean more similar\n",
    "- **Cosine Similarity**: Measures angle between vectors (normalized IP)\n",
    "- **Euclidean Distance**: Straight-line distance in vector space\n",
    "\n",
    "### Step 3: Approximate Nearest Neighbor (ANN)\n",
    "- **Speed vs Accuracy**: ANN provides fast search with minimal accuracy loss\n",
    "- **Indexing**: Milvus creates indexes for efficient search across millions of vectors\n",
    "- **Scalability**: Can handle large document collections in real-time\n",
    "\n",
    "## Retrieval Parameters\n",
    "\n",
    "- **Limit**: Number of top results to return (we'll use 3)\n",
    "- **Search Params**: Configuration for the search algorithm\n",
    "- **Output Fields**: Which metadata to return with results (we want the original text)\n",
    "\n",
    "Let's test our retrieval system with a sample question!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d1fbadd-29f1-4192-95f4-8b81232fa0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùì USER QUESTION:\n",
      "   \"What are the challenges of assessing the quality of AI-generated code? What are some strategies for doing this?\"\n",
      "\n",
      "üéØ Why this question tests RAG effectively:\n",
      "   ‚Ä¢ Domain-specific: Related to AI code evaluation\n",
      "   ‚Ä¢ Multi-faceted: Asks for both challenges AND strategies\n",
      "   ‚Ä¢ Synthesis required: Needs information from multiple sources\n",
      "   ‚Ä¢ Context-dependent: Benefits from specific document knowledge\n"
     ]
    }
   ],
   "source": [
    "# ===== DEFINE USER QUERY =====\n",
    "# This is the question we want to answer using our RAG system. The question is specific to the document that we saved in our vector database. The LLM has no knowledge of this topic and with out the vector database it would hallucinate the answer.\n",
    "question = (\n",
    "    \"What are the challenges of assessing the quality of AI-generated code? What are some strategies for doing this?\"\n",
    ")\n",
    "\n",
    "print(\"‚ùì USER QUESTION:\")\n",
    "print(f\"   \\\"{question}\\\"\")\n",
    "\n",
    "# This question is perfect for testing our RAG system because:\n",
    "# 1. It's directly related to our document (AI code evaluation)\n",
    "# 2. It has two parts - challenges AND strategies\n",
    "# 3. It requires synthesis of information from multiple sections\n",
    "# 4. It's the kind of question that benefits from retrieved context\n",
    "\n",
    "print(f\"\\nüéØ Why this question tests RAG effectively:\")\n",
    "print(f\"   ‚Ä¢ Domain-specific: Related to AI code evaluation\")\n",
    "print(f\"   ‚Ä¢ Multi-faceted: Asks for both challenges AND strategies\")\n",
    "print(f\"   ‚Ä¢ Synthesis required: Needs information from multiple sources\")\n",
    "print(f\"   ‚Ä¢ Context-dependent: Benefits from specific document knowledge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8598f0c1-0631-437c-8a1c-b04c6af2d0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Converting question to embedding...\n",
      "   Question embedding shape: 384 dimensions\n",
      "   Embedding sample: [-0.08172768 -0.02636518 -0.04366045  0.04490916  0.0063496 ]\n",
      "\n",
      "üîç Searching for similar document chunks...\n",
      "   Collection: vectordb_collection\n",
      "   Search method: Vector similarity using Inner Product\n",
      "   Top results to return: 3\n"
     ]
    }
   ],
   "source": [
    "# ===== PERFORM VECTOR SEARCH =====\n",
    "# Step 1: Convert the question to an embedding vector\n",
    "print(\"üîÑ Converting question to embedding...\")\n",
    "question_embedding = emb_text(question)\n",
    "\n",
    "print(f\"   Question embedding shape: {len(question_embedding)} dimensions\")\n",
    "print(f\"   Embedding sample: {question_embedding[:5]}\")\n",
    "\n",
    "# Step 2: Search for similar vectors in the database\n",
    "print(f\"\\nüîç Searching for similar document chunks...\")\n",
    "print(f\"   Collection: {collection_name}\")\n",
    "print(f\"   Search method: Vector similarity using Inner Product\")\n",
    "print(f\"   Top results to return: 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd66032c-3d2b-4c87-936c-31031f4d84a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä SEARCH RESULTS ANALYSIS:\n",
      "   ‚Ä¢ Total matches found: 3\n",
      "   ‚Ä¢ Search completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Search DB using the embedded query\n",
    "results = collection.search(\n",
    "    data=[question_embedding.tolist()],   # Convert the embedding to a list for Milvus\n",
    "    anns_field=\"embedding\",\n",
    "    param={\"metric_type\": \"COSINE\"},\n",
    "    limit=3,                           # Return at most three results\n",
    "    output_fields=[\"embedding\",\"original_text\",\"metadata\"]\n",
    ")\n",
    "\n",
    "# Analyze search results\n",
    "print(f\"\\nüìä SEARCH RESULTS ANALYSIS:\")\n",
    "print(f\"   ‚Ä¢ Total matches found: {len(results[0])}\")\n",
    "print(f\"   ‚Ä¢ Search completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e78eac4-fd36-4321-a629-f81f4120e4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Processing retrieved chunks for LLM context...\n",
      "\n",
      "üìã RETRIEVED CHUNKS (Raw Format):\n",
      "\n",
      "--- CHUNK 1 ---\n",
      "üìèSimilarity Score: 0.7006\n",
      "üëæ Metadata: 2502.07835v1.pdf\n",
      "üß† Text: The rise of Large Language Models (LLMs) in software engineering, particularly in code generation, has garnered significant attention. However, assessing the quality of AI-generated code remains a challenge due to the inherent complexity of programming tasks and the lack of robust evaluation metrics that align well with human judgment. Traditional token-based metrics such as BLEU and ROUGE, while commonly used in natural language processing, exhibit weak correlations with human assessments in code intelligence and verification tasks. Furthermore, these metrics are primarily research focused and are not designed for seamless integration into the software development lifecycle, limiting their practical utility for developers seeking to improve code quality and security.\n",
      "--- END CHUNK 1 ---\n",
      "\n",
      "--- CHUNK 2 ---\n",
      "üìèSimilarity Score: 0.6455\n",
      "üëæ Metadata: 2502.07835v1.pdf\n",
      "üß† Text: The SBC score, along with the reverse-generated requirements, provides actionable insights for developers, helping them assess AI-generated code without requiring extensive reference implementations. Unlike prior evaluation methods, this approach inherently addresses the challenges of syntactic variations and alternative solutions in generated code, as highlighted in recent studies [12]. By\n",
      "--- END CHUNK 2 ---\n",
      "\n",
      "--- CHUNK 3 ---\n",
      "üìèSimilarity Score: 0.6278\n",
      "üëæ Metadata: 2502.07835v1.pdf\n",
      "üß† Text: AI-powered code assistants, leveraging the power of Large Language Models (LLMs), are becoming a focal point for enterprises, offering promising capabilities in automating code generation. However, evaluating the quality of LLM-generated code remains a complex challenge due to the intricacies of programming concepts and syntax, which differ significantly from natural language generation [1, 2].\n",
      "--- END CHUNK 3 ---\n",
      "\n",
      "üìä RETRIEVED CONTENT STATISTICS:\n",
      "   ‚Ä¢ Total chunks: 3\n",
      "   ‚Ä¢ Total characters: 1,568\n",
      "   ‚Ä¢ Total words: 211\n",
      "   ‚Ä¢ Average words per chunk: 70\n",
      "   ‚Ä¢ Similarity score range: 0.6278 - 0.7006\n"
     ]
    }
   ],
   "source": [
    "# ===== PROCESS SEARCH RESULTS =====\n",
    "print(\"üìù Processing retrieved chunks for LLM context...\")\n",
    "\n",
    "# Extract text, metadata, and similarity scores\n",
    "retrieved_lines_with_distances = [\n",
    "    (\n",
    "        res.entity.get(\"original_text\"),   # The chunk of text\n",
    "        res.entity.get(\"metadata\"),        # The metadata (e.g., source document)\n",
    "        res.score                          # Similarity score\n",
    "    )\n",
    "    for res in results[0]\n",
    "]\n",
    "\n",
    "# Display the raw results in a structured format\n",
    "print(f\"\\nüìã RETRIEVED CHUNKS (Raw Format):\")\n",
    "for i, (text, metadata, distance) in enumerate(retrieved_lines_with_distances):\n",
    "    print(f\"\\n--- CHUNK {i+1} ---\")\n",
    "    print(f\"üìèSimilarity Score: {distance:.4f}\")\n",
    "    print(f\"üëæ Metadata: {metadata}\")\n",
    "    print(f\"üß† Text: {text}\")\n",
    "    print(f\"--- END CHUNK {i+1} ---\")\n",
    "\n",
    "# Statistics about retrieved content\n",
    "total_chars = sum(len(text) for text, _, _ in retrieved_lines_with_distances)\n",
    "total_words = sum(len(text.split()) for text, _, _ in retrieved_lines_with_distances)\n",
    "\n",
    "print(f\"\\nüìä RETRIEVED CONTENT STATISTICS:\")\n",
    "print(f\"   ‚Ä¢ Total chunks: {len(retrieved_lines_with_distances)}\")\n",
    "print(f\"   ‚Ä¢ Total characters: {total_chars:,}\")\n",
    "print(f\"   ‚Ä¢ Total words: {total_words:,}\")\n",
    "print(f\"   ‚Ä¢ Average words per chunk: {total_words / len(retrieved_lines_with_distances):.0f}\")\n",
    "print(f\"   ‚Ä¢ Similarity score range: \"\n",
    "      f\"{min(d for _, _, d in retrieved_lines_with_distances):.4f} - \"\n",
    "      f\"{max(d for _, _, d in retrieved_lines_with_distances):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8613c6a7-b781-4e48-a8af-41bb97b9f03c",
   "metadata": {},
   "source": [
    "# ü§ñ Augmented Generation\n",
    "\n",
    "## From Retrieval to Response\n",
    "\n",
    "Now comes the final step of RAG - using the retrieved context to generate a well-informed response. This process involves:\n",
    "\n",
    "1. **Context Preparation**: Combine retrieved chunks into a coherent context\n",
    "2. **Prompt Engineering**: Structure the prompt to include context and question\n",
    "3. **LLM Generation**: Use the language model to generate a response\n",
    "4. **Response Synthesis**: Produce a final answer based on the retrieved evidence\n",
    "\n",
    "## The Power of Context\n",
    "\n",
    "Without RAG, an LLM would answer based only on its training data, which might:\n",
    "- **Lack specific information** about our document\n",
    "- **Provide outdated information** if the model is older\n",
    "- **Generate hallucinations** without factual grounding\n",
    "\n",
    "With RAG, the LLM has access to:\n",
    "- **Relevant, specific content** from our document\n",
    "- **Current information** from the retrieved chunks\n",
    "- **Factual grounding** to reduce hallucinations\n",
    "\n",
    "## Prompt Engineering for RAG\n",
    "\n",
    "A well-designed RAG prompt includes:\n",
    "- **System instructions** that define the AI's role and constraints\n",
    "- **Retrieved context** that provides factual information\n",
    "- **User question** that specifies what to answer\n",
    "- **Response guidelines** that ensure appropriate formatting\n",
    "\n",
    "Let's see how this works in practice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c45528a5-4cbf-4f88-a720-aeb614923d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Preparing context for LLM generation...\n",
      "üìä CONTEXT STATISTICS:\n",
      "   ‚Ä¢ Context length: 1,572 characters\n",
      "   ‚Ä¢ Context words: 211 words\n",
      "   ‚Ä¢ Number of chunks: 3\n",
      "\n",
      "üìù PREPARED CONTEXT (first 300 characters):\n",
      "   \"The rise of Large Language Models (LLMs) in software engineering, particularly in code generation, has garnered significant attention. However, assessing the quality of AI-generated code remains a challenge due to the inherent complexity of programming tasks and the lack of robust evaluation metrics...\"\n",
      "\n",
      "‚úÖ Context prepared successfully!\n",
      "   The LLM will use this context to generate an informed response.\n"
     ]
    }
   ],
   "source": [
    "# ===== CONTEXT PREPARATION =====\n",
    "# Combine the retrieved chunks into a single context string for the LLM\n",
    "print(\"üìã Preparing context for LLM generation...\")\n",
    "\n",
    "# Extract just the text (first element of tuple: text, metadata, score)\n",
    "context_chunks = [text for text, _, _ in retrieved_lines_with_distances]\n",
    "context = \"\\n\\n\".join(context_chunks)  # Use double newlines for better separation\n",
    "\n",
    "print(f\"üìä CONTEXT STATISTICS:\")\n",
    "print(f\"   ‚Ä¢ Context length: {len(context):,} characters\")\n",
    "print(f\"   ‚Ä¢ Context words: {len(context.split()):,} words\")\n",
    "print(f\"   ‚Ä¢ Number of chunks: {len(context_chunks)}\")\n",
    "\n",
    "# Show the prepared context (truncated for readability)\n",
    "print(f\"\\nüìù PREPARED CONTEXT (first 300 characters):\")\n",
    "print(f\"   \\\"{context[:300]}...\\\"\")\n",
    "\n",
    "# This context will be included in the prompt to provide the LLM with\n",
    "# relevant information from our document to answer the user's question\n",
    "print(f\"\\n‚úÖ Context prepared successfully!\")\n",
    "print(f\"   The LLM will use this context to generate an informed response.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57421cbd-7cf8-42ad-a9c1-3b794d1e6134",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Prompt Engineering for RAG\n",
    "\n",
    "Effective prompt engineering is crucial for RAG success. Our prompts need to:\n",
    "\n",
    "### System Prompt Design\n",
    "- **Role Definition**: Clearly specify the AI's role and constraints\n",
    "- **Context Grounding**: Ensure responses are based only on provided context\n",
    "- **Honesty Enforcement**: Require admission when information is unavailable\n",
    "- **Quality Guidelines**: Set expectations for response structure and completeness\n",
    "\n",
    "### User Prompt Structure\n",
    "- **Context Section**: Present retrieved information clearly\n",
    "- **Question Section**: State the user's question explicitly\n",
    "- **Response Instructions**: Guide the format and style of the answer\n",
    "\n",
    "### Why This Matters\n",
    "- **Reduces Hallucinations**: Strict context adherence prevents made-up information\n",
    "- **Improves Relevance**: Clear instructions help focus on what's important\n",
    "- **Ensures Consistency**: Structured prompts lead to predictable response formats\n",
    "- **Enhances Quality**: Well-designed prompts improve response accuracy and usefulness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "48a52e88-eeb0-4766-bfd2-c1c718f008ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "  \"You are an AI assistant that answers questions based solely on the provided context. \"\n",
    "  \"If the answer cannot be found in context, reply truthfully that you don‚Äôt know.\"\n",
    ")\n",
    "\n",
    "USER_PROMPT = (\n",
    "  \"Context:\\n\"\n",
    "  \"{context}\\n\"\n",
    "  \"Question:\\n\"\n",
    "  \"{question}\\n\"\n",
    "  \"Answer concisely:\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ddcc05-f348-4d62-8466-a37059310575",
   "metadata": {},
   "source": [
    "# ‚öôÔ∏è LLM Integration and Response Generation\n",
    "\n",
    "## Language Model Setup\n",
    "\n",
    "Our RAG system uses a **Llama 3.2 3B model** that's been quantized for efficiency. Key configuration choices:\n",
    "\n",
    "### Model Configuration\n",
    "- **Temperature: 0**: Ensures deterministic, consistent responses\n",
    "- **Max Tokens: None**: Allows full-length responses without artificial cutoffs\n",
    "- **Retries: 2**: Handles temporary network or service issues\n",
    "- **SSL Verification: Disabled**: Required for internal service endpoints\n",
    "\n",
    "### Why Llama 3.2 3B?\n",
    "- **Efficiency**: Smaller model with good performance for focused tasks\n",
    "- **Quantization**: 8-bit quantization reduces memory usage while maintaining quality\n",
    "- **Instruction Following**: Fine-tuned to follow instructions and answer questions accurately\n",
    "- **Context Awareness**: Capable of understanding and using provided context effectively\n",
    "\n",
    "## The Generation Process\n",
    "\n",
    "1. **Prompt Construction**: Combine system instructions, context, and question\n",
    "2. **Model Invocation**: Send the complete prompt to the LLM\n",
    "3. **Response Generation**: Model generates answer based on context\n",
    "4. **Result Processing**: Extract and present the final response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3976092f-fd56-44f0-9822-d073acd5a245",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=inference_server_model_name,\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    api_key=\"EMPTY\",  # if you prefer to pass api key in directly instaed of using env vars\n",
    "    base_url=inference_server_url,\n",
    "    http_client=httpx.Client(verify=False)    # Because we are using an internal API endpoint (service) we need to disable SSL certificate checking.\n",
    ")\n",
    "\n",
    "# Define system and human templates\n",
    "SYSTEM_PROMPT = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are an AI assistant that answers questions based solely on the provided context. \"\n",
    "    \"If the answer cannot be found in context, reply truthfully that you don‚Äôt know.\"\n",
    ")\n",
    "\n",
    "HumanMessagePromptTemplate = HumanMessagePromptTemplate.from_template(\n",
    "    \"Context:\\n\"\n",
    "    \"{context}\\n\"\n",
    "    \"Question:\\n\"\n",
    "    \"{question}\\n\"\n",
    "    \"Answer concisely:\"\n",
    ")\n",
    "\n",
    "# Combine into a chat prompt\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [SYSTEM_PROMPT, HumanMessagePromptTemplate]\n",
    ")\n",
    "\n",
    "prompt = chat_prompt.format_prompt(context=context, question=question)\n",
    "\n",
    "ai_msg = llm.invoke(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93c9a430-0118-4faa-a657-672927bcdda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ RAG PIPELINE RESULTS\n",
      "==================================================\n",
      "\n",
      "‚ùì ORIGINAL QUESTION:\n",
      "   What are the challenges of assessing the quality of AI-generated code? What are some strategies for doing this?\n",
      "\n",
      "üìä PIPELINE STATISTICS:\n",
      "   ‚Ä¢ Document chunks retrieved: 3\n",
      "   ‚Ä¢ Context length: 1,572 characters\n",
      "   ‚Ä¢ Context words: 211\n",
      "   ‚Ä¢ Similarity scores: ['0.701', '0.645', '0.628']\n",
      "\n",
      "üìù RETRIEVED CONTEXT SUMMARY:\n",
      "   Chunk 1 (0.701) [Source: 2502.07835v1.pdf]: The rise of Large Language Models (LLMs) in software engineering, particularly i...\n",
      "   Chunk 2 (0.645) [Source: 2502.07835v1.pdf]: The SBC score, along with the reverse-generated requirements, provides actionabl...\n",
      "   Chunk 3 (0.628) [Source: 2502.07835v1.pdf]: AI-powered code assistants, leveraging the power of Large Language Models (LLMs)...\n"
     ]
    }
   ],
   "source": [
    "# ===== DISPLAY RESULTS =====\n",
    "# Show the complete RAG pipeline results for analysis and learning\n",
    "print(\"üéØ RAG PIPELINE RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\n‚ùì ORIGINAL QUESTION:\")\n",
    "print(f\"   {question}\")\n",
    "\n",
    "print(f\"\\nüìä PIPELINE STATISTICS:\")\n",
    "print(f\"   ‚Ä¢ Document chunks retrieved: {len(retrieved_lines_with_distances)}\")\n",
    "print(f\"   ‚Ä¢ Context length: {len(context):,} characters\")\n",
    "print(f\"   ‚Ä¢ Context words: {len(context.split()):,}\")\n",
    "print(f\"   ‚Ä¢ Similarity scores: {[f'{score:.3f}' for _, _, score in retrieved_lines_with_distances]}\")\n",
    "\n",
    "print(f\"\\nüìù RETRIEVED CONTEXT SUMMARY:\")\n",
    "for i, (text, metadata, score) in enumerate(retrieved_lines_with_distances):\n",
    "    print(f\"   Chunk {i+1} ({score:.3f}) [Source: {metadata}]: {text[:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6d6e266-fb32-4094-bd65-b9ea5ed05e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† CONTEXT PROVIDED TO LLM:\n",
      "   Length: 1,572 characters (211 words)\n",
      "   Number of chunks: 3\n",
      "\n",
      "   First 200 characters:\n",
      "   \"The rise of Large Language Models (LLMs) in software engineering, particularly in code generation, has garnered significant attention. However, assessing the quality of AI-generated code remains a cha...\"\n",
      "\n",
      "   Last 200 characters:\n",
      "   \"... evaluating the quality of LLM-generated code remains a complex challenge due to the intricacies of programming concepts and syntax, which differ significantly from natural language generation [1, 2].\"\n",
      "\n",
      "üìö CONTEXT SOURCES:\n",
      "   Source 1 (similarity: 0.701)\n",
      "      Metadata: 2502.07835v1.pdf\n",
      "      Preview: The rise of Large Language Models (LLMs) in software engineering, particularly i...\n",
      "   Source 2 (similarity: 0.645)\n",
      "      Metadata: 2502.07835v1.pdf\n",
      "      Preview: The SBC score, along with the reverse-generated requirements, provides actionabl...\n",
      "   Source 3 (similarity: 0.628)\n",
      "      Metadata: 2502.07835v1.pdf\n",
      "      Preview: AI-powered code assistants, leveraging the power of Large Language Models (LLMs)...\n"
     ]
    }
   ],
   "source": [
    "# ===== CONTEXT ANALYSIS =====\n",
    "print(f\"\\nüß† CONTEXT PROVIDED TO LLM:\")\n",
    "print(f\"   Length: {len(context):,} characters ({len(context.split())} words)\")\n",
    "print(f\"   Number of chunks: {len(retrieved_lines_with_distances)}\")\n",
    "print(f\"\\n   First 200 characters:\")\n",
    "print(f\"   \\\"{context[:200]}...\\\"\")\n",
    "\n",
    "print(f\"\\n   Last 200 characters:\")\n",
    "print(f\"   \\\"...{context[-200:]}\\\"\")\n",
    "\n",
    "# Show the context sources\n",
    "print(f\"\\nüìö CONTEXT SOURCES:\")\n",
    "for i, (text, metadata, score) in enumerate(retrieved_lines_with_distances):\n",
    "    print(f\"   Source {i+1} (similarity: {score:.3f})\")\n",
    "    print(f\"      Metadata: {metadata}\")\n",
    "    print(f\"      Preview: {text[:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1bba1be1-409b-48e4-b84d-b5c87face1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ RAG SYSTEM RESPONSE\n",
      "============================================================\n",
      "\n",
      "üí¨ GENERATED RESPONSE:\n",
      "\n",
      "======================================================\n",
      "The challenges of assessing the quality of AI-generated code include:\n",
      "\n",
      "1. Complexity of programming tasks\n",
      "2. Lack of robust evaluation metrics that align with human judgment\n",
      "3. Inherent differences between programming concepts and syntax and natural language generation\n",
      "\n",
      "Strategies for assessing AI-generated code quality include:\n",
      "\n",
      "1. Using the SBC score and reverse-generated requirements for actionable insights\n",
      "2. Addressing syntactic variations and alternative solutions in generated code\n",
      "3. Developing evaluation metrics that are specifically designed for code intelligence and verification tasks.\n",
      "\n",
      "======================================================\n",
      "\n",
      "üìä RESPONSE ANALYSIS:\n",
      "   ‚Ä¢ Response length: 602 characters\n",
      "   ‚Ä¢ Response words: 80\n",
      "   ‚Ä¢ Structure: Well-structured\n",
      "   ‚Ä¢ Addresses both challenges and strategies: Yes\n",
      "\n",
      "‚úÖ RAG PIPELINE COMPLETE!\n",
      "   The system successfully:\n",
      "   ‚Ä¢ Embedded the user's question\n",
      "   ‚Ä¢ Retrieved relevant document chunks\n",
      "   ‚Ä¢ Generated a contextually grounded response\n",
      "   ‚Ä¢ Provided specific, accurate information from the source document\n"
     ]
    }
   ],
   "source": [
    "# ===== FINAL RESPONSE ANALYSIS =====\n",
    "print(f\"\\nüéØ RAG SYSTEM RESPONSE\")\n",
    "print(f\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüí¨ GENERATED RESPONSE:\")\n",
    "print(f\"\\n======================================================\")\n",
    "print(f\"{ai_msg.content}\")\n",
    "print(f\"\\n======================================================\")\n",
    "\n",
    "print(f\"\\nüìä RESPONSE ANALYSIS:\")\n",
    "response_words = len(ai_msg.content.split())\n",
    "response_chars = len(ai_msg.content)\n",
    "print(f\"   ‚Ä¢ Response length: {response_chars} characters\")\n",
    "print(f\"   ‚Ä¢ Response words: {response_words}\")\n",
    "print(f\"   ‚Ä¢ Structure: {'Well-structured' if '1.' in ai_msg.content or '‚Ä¢' in ai_msg.content else 'Paragraph format'}\")\n",
    "print(f\"   ‚Ä¢ Addresses both challenges and strategies: {'Yes' if 'challenges' in ai_msg.content.lower() and 'strategies' in ai_msg.content.lower() else 'Partial'}\")\n",
    "\n",
    "print(f\"\\n‚úÖ RAG PIPELINE COMPLETE!\")\n",
    "print(f\"   The system successfully:\")\n",
    "print(f\"   ‚Ä¢ Embedded the user's question\")\n",
    "print(f\"   ‚Ä¢ Retrieved relevant document chunks\")\n",
    "print(f\"   ‚Ä¢ Generated a contextually grounded response\")\n",
    "print(f\"   ‚Ä¢ Provided specific, accurate information from the source document\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67ec792-221a-45a8-af48-51deb40734b3",
   "metadata": {},
   "source": [
    "# üéì Conclusion: RAG System Complete!\n",
    "\n",
    "## What We Accomplished\n",
    "\n",
    "You've successfully built and run a complete RAG (Retrieval-Augmented Generation) system! Here's what we covered:\n",
    "\n",
    "### üìÑ **Document Ingestion**\n",
    "- Downloaded documents from object storage (MinIO/S3)\n",
    "- Processed PDF documents using advanced parsing (Docling)\n",
    "- Performed intelligent document chunking for optimal retrieval\n",
    "\n",
    "### üß† **Text Embeddings**\n",
    "- Learned about semantic vector representations\n",
    "- Used SentenceTransformers to generate 384-dimensional embeddings\n",
    "- Understood how embeddings capture semantic similarity\n",
    "\n",
    "### üóÉÔ∏è **Vector Database**\n",
    "- Set up Milvus for high-performance vector storage\n",
    "- Stored embeddings with metadata for efficient retrieval\n",
    "- Configured search parameters for optimal performance\n",
    "\n",
    "### üîç **Semantic Search**\n",
    "- Converted queries to embeddings for similarity search\n",
    "- Retrieved the most relevant document chunks\n",
    "- Analyzed similarity scores and retrieval quality\n",
    "\n",
    "### ü§ñ **Response Generation**\n",
    "- Designed effective prompts for contextual responses\n",
    "- Integrated with Llama 3.2 3B model for generation\n",
    "- Generated accurate, grounded responses using retrieved context\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "### RAG Benefits\n",
    "- **Accuracy**: Responses grounded in specific document content\n",
    "- **Transparency**: See exactly which sources informed the answer\n",
    "- **Flexibility**: Easy to update knowledge by changing documents\n",
    "- **Efficiency**: No need to retrain models for new information\n",
    "\n",
    "### Technical Insights\n",
    "- **Embedding Quality**: Choice of embedding model impacts retrieval performance\n",
    "- **Chunking Strategy**: Proper document segmentation improves context relevance\n",
    "- **Prompt Engineering**: Well-designed prompts are crucial for quality responses\n",
    "- **Vector Search**: Semantic similarity enables meaning-based retrieval\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "To extend this RAG system, consider:\n",
    "\n",
    "1. **Multiple Documents**: Expand to handle document collections\n",
    "2. **Advanced Chunking**: Implement hybrid or semantic chunking strategies\n",
    "3. **Reranking**: Add reranking models to improve retrieval quality\n",
    "4. **Evaluation Metrics**: Implement retrieval and generation quality metrics\n",
    "5. **Production Deployment**: Scale for production with distributed systems\n",
    "6. **Multi-modal RAG**: Extend to handle images, tables, and other content types\n",
    "\n",
    "## Learning Resources\n",
    "\n",
    "- **Vector Databases**: Explore other options like Pinecone, Weaviate, Chroma\n",
    "- **Embedding Models**: Try domain-specific or larger embedding models\n",
    "- **LLM Options**: Experiment with different language models and sizes\n",
    "- **Advanced RAG**: Learn about query expansion, hypothesis verification, and multi-hop reasoning\n",
    "\n",
    "**Congratulations!** You now understand the fundamentals of building production-ready RAG systems. This knowledge forms the foundation for many modern AI applications that combine retrieval and generation capabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1032fc0-1bfb-4b2d-80f3-5a0d0e86e12e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
