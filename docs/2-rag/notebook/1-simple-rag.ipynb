{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac9b9a5e-7bb4-4334-a2f1-df28fbf031ba",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd443db6-4eca-42c3-b63a-be3286249a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai==1.93.0 in /opt/app-root/lib64/python3.11/site-packages (1.93.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/app-root/lib64/python3.11/site-packages (from openai==1.93.0) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/app-root/lib64/python3.11/site-packages (from openai==1.93.0) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/app-root/lib64/python3.11/site-packages (from openai==1.93.0) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/app-root/lib64/python3.11/site-packages (from openai==1.93.0) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/app-root/lib64/python3.11/site-packages (from openai==1.93.0) (2.11.7)\n",
      "Requirement already satisfied: sniffio in /opt/app-root/lib64/python3.11/site-packages (from openai==1.93.0) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/app-root/lib64/python3.11/site-packages (from openai==1.93.0) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/app-root/lib64/python3.11/site-packages (from openai==1.93.0) (4.13.2)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/app-root/lib64/python3.11/site-packages (from anyio<5,>=3.5.0->openai==1.93.0) (3.10)\n",
      "Requirement already satisfied: certifi in /opt/app-root/lib64/python3.11/site-packages (from httpx<1,>=0.23.0->openai==1.93.0) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/app-root/lib64/python3.11/site-packages (from httpx<1,>=0.23.0->openai==1.93.0) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/app-root/lib64/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.93.0) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/app-root/lib64/python3.11/site-packages (from pydantic<3,>=1.9.0->openai==1.93.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/app-root/lib64/python3.11/site-packages (from pydantic<3,>=1.9.0->openai==1.93.0) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/app-root/lib64/python3.11/site-packages (from pydantic<3,>=1.9.0->openai==1.93.0) (0.4.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -r requirements.txt\n",
    "\n",
    "!pip install openai==1.93.0      # Only for testing\n",
    "# ! pip install --upgrade docling openai torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2ba0ab-7fd1-4a33-bea0-49eb2f846931",
   "metadata": {},
   "source": [
    "# Document Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b8599ad-7f3c-4dbd-b0d6-64f73535ab03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from rag-docs::2502.07835v1.pdf to: downloads/2502.07835v1.pdf\n",
      "✅ Downloaded '2502.07835v1.pdf' to 'downloads/2502.07835v1.pdf'\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from botocore.config import Config\n",
    "import os\n",
    "\n",
    "\"\"\"\n",
    "Environment variables:\n",
    "  AWS_S3_ENDPOINT        – MinIO service DNS name (e.g. minio.minio.svc.cluster.local)\n",
    "  AWS_ACCESS_KEY_ID      – MinIO access key\n",
    "  AWS_SECRET_ACCESS_KEY  – MinIO secret key\n",
    "  AWS_DEFAULT_REGION     – Dummy value; boto3 still expects one\n",
    "  AWS_S3_BUCKET          – Default bucket to use for the Workspace data connection \n",
    "\"\"\"\n",
    "\n",
    "# === Configuration ===\n",
    "open_ai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "endpoint = os.getenv(\"AWS_S3_ENDPOINT\")\n",
    "access_key = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "region = os.getenv(\"AWS_DEFAULT_REGION\")\n",
    "bucket_name = os.getenv(\"AWS_S3_BUCKET\")\n",
    "object_key = \"2502.07835v1.pdf\"  # The name of the PDF in the S3 bucket\n",
    "download_dir = \"downloads\"\n",
    "\n",
    "# === Initialise S3 client ===\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=f\"http://{endpoint}\",\n",
    "    aws_access_key_id=access_key,\n",
    "    aws_secret_access_key=secret_key,\n",
    "    region_name=region,\n",
    "    config=Config(signature_version=\"s3v4\"),\n",
    ")\n",
    "\n",
    "# === Ensure download directory exists ===\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "local_path = os.path.join(download_dir, object_key)\n",
    "print(f\"Downloading from {bucket_name}::{object_key} to: {local_path}\")\n",
    "\n",
    "# === Download the file ===\n",
    "try:\n",
    "    s3.download_file(bucket_name, object_key, local_path)\n",
    "    print(f\"✅ Downloaded '{object_key}' to '{local_path}'\")\n",
    "except s3.exceptions.NoSuchKey:\n",
    "    print(f\"❌ File '{object_key}' not found in bucket '{bucket_name}'\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error downloading file: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf2a9c2-784c-46aa-b7c4-b5a37dab7353",
   "metadata": {},
   "source": [
    "# Embedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5cefa6e0-e615-429b-a25c-3a0e882a457c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate an embedding vector for a piece of text.\n",
    "This helper wraps the embedding function to reduce boiler-plate when you frequently need sentence- or paragraph-level embeddings.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "text : str\n",
    "    The input text to embed.\n",
    "\n",
    "Returns: A 1-D list of 1 536 floats representing the semantic embedding of *text*. \n",
    "The vector can be indexed, stored, or compared with other embeddings (e.g., via cosine similarity).\n",
    "\"\"\"\n",
    "def emb_text(text: str) -> list[float]:\n",
    "    return (\n",
    "        openai_client.embeddings.create(\n",
    "            input=text,\n",
    "            model=\"text-embedding-3-small\"\n",
    "        ).data[0].embedding\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a98ec3c6-04e9-4c79-a673-8145f52da819",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b5f32f6-8970-48e7-9d5e-a4d82f057ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimensions: 1536\n",
      "[0.009873751550912857, -0.005582896992564201, 0.0068350606597959995, -0.038091305643320084, -0.018248096108436584, -0.041217729449272156, -0.00763660529628396, 0.032221291214227676, 0.018918044865131378, 0.00010168847802560776]\n"
     ]
    }
   ],
   "source": [
    "# Use this to find the default number of dimensions this embedding model generates. We will use that later.\n",
    "test_embedding = emb_text(\"This is a test\")\n",
    "embedding_dim = len(test_embedding)\n",
    "print(f\"Embedding dimensions: {embedding_dim}\")\n",
    "print(test_embedding[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb9ba5b",
   "metadata": {},
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_transformer = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b4b07c68-85d1-46be-a14e-088ac5eab68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found /opt/app-root/src/rhoai-roadshow-v2/docs/2-rag/notebook/downloads/2502.07835v1.pdf\n"
     ]
    }
   ],
   "source": [
    "from utils import project_root\n",
    "\n",
    "# Assemble a complete path to the file so the document import can properly and reliably always find the document.\n",
    "doc_source = project_root() / local_path\n",
    "\n",
    "if not doc_source.is_file():\n",
    "    raise FileNotFoundError(f\"{DOC_SOURCE} does not exist.\")\n",
    "\n",
    "print(f\"Found {doc_source}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1eb104d7-caf6-45d7-9b22-cfeb6a483d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parse and chunk a PDF using Docling v2.x\n",
    "\"\"\"\n",
    "from docling.document_converter import DocumentConverter\n",
    "from pathlib import Path\n",
    "\n",
    "doc = DocumentConverter().convert(source=doc_source).document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "51b3591c-65eb-4556-a71b-0476c9b516b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=1), 2: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=2), 3: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=3), 4: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=4), 5: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=5), 6: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=6), 7: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=7), 8: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=8), 9: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=9), 10: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=10), 11: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=11), 12: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=12), 13: PageItem(size=Size(width=612.0, height=792.0), image=None, page_no=13)}\n"
     ]
    }
   ],
   "source": [
    "print(doc.pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7656184-67dd-41ce-83a9-3c3560337049",
   "metadata": {},
   "source": [
    "# Connect to Milvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0bab9aa5-b709-4ff6-98e5-8aebf7759585",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import MilvusClient\n",
    "\n",
    "collection_name = \"my_rag_collection\"\n",
    "\n",
    "milvus_client = MilvusClient(\n",
    "    uri=\"http://milvus-service.milvus.svc.cluster.local:19530\",\n",
    "    db_name=\"default\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "07c126b9-5d76-46ee-8a11-5e16336fdf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "if milvus_client.has_collection(collection_name):\n",
    "    milvus_client.drop_collection(collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c527d363-59fb-4e09-a7c1-7960d2b276b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection list: ['animal_test']\n",
      "Collection list: ['animal_test', 'my_rag_collection']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Collection list: {milvus_client.list_collections()}\") \n",
    "\n",
    "milvus_client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    dimension=embedding_dim,\n",
    "    metric_type=\"IP\",  # Inner product distance\n",
    "    consistency_level=\"Strong\",  # Supported values are (`\"Strong\"`, `\"Session\"`, `\"Bounded\"`, `\"Eventually\"`). See https://milvus.io/docs/consistency.md#Consistency-Level for more details.\n",
    ")\n",
    "\n",
    "print(f\"Collection list: {milvus_client.list_collections()}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "663ab01b-5591-413c-9b58-c756bc412602",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling_core.transforms.chunker import HierarchicalChunker\n",
    "\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "converter = DocumentConverter()\n",
    "chunker = HierarchicalChunker()\n",
    "\n",
    "# Convert the input file to Docling Document\n",
    "source = doc_source\n",
    "doc = converter.convert(source).document\n",
    "\n",
    "# Perform hierarchical chunking. This is faster than Hybrid chunking, but not as good.\n",
    "texts = [chunk.text for chunk in chunker.chunk(doc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dfd8b8-34b9-4377-98aa-fe0b7fb8011a",
   "metadata": {},
   "source": [
    "# Vector Storage and Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7da3aeba-3526-48e6-a2ff-c4d00aa2c72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 70/70 [00:20<00:00,  3.44it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'insert_count': 70, 'ids': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69], 'cost': 0}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "data = []\n",
    "\n",
    "for i, chunk in enumerate(tqdm(texts, desc=\"Processing chunks\")):\n",
    "    embedding = emb_text(chunk)\n",
    "    data.append({\"id\": i, \"vector\": embedding, \"text\": chunk})\n",
    "\n",
    "milvus_client.insert(collection_name=collection_name, data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24873e3-fe35-4f22-80a6-426ffc890765",
   "metadata": {},
   "source": [
    "# Visualising how embeddings are stored in a vector database\n",
    "\n",
    "<Describe how this visualises how the text is stored in the vector database.\n",
    "\n",
    "https://projector.tensorflow.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054fbcd4-be4b-4a35-b8e3-9f33ea5d0d3d",
   "metadata": {},
   "source": [
    "# Query-Time Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d1fbadd-29f1-4192-95f4-8b81232fa0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = (\n",
    "    \"What are the challenges of assessing assessing the quality of AI-generated code? What are some strategies for doing this\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fd66032c-3d2b-4c87-936c-31031f4d84a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_res = milvus_client.search(\n",
    "    collection_name=collection_name,\n",
    "    data=[emb_text(question)],\n",
    "    limit=3,\n",
    "    search_params={\"metric_type\": \"IP\", \"params\": {}},\n",
    "    output_fields=[\"text\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0e78eac4-fd36-4321-a629-f81f4120e4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    [\n",
      "        \"AI-assisted coding has been shown to be more beneficial for senior developers, as they possess the expertise to critically evaluate the generated code for correctness, completeness, and compliance. In contrast, junior developers may struggle to identify hallucinations, missing functionality, or incorrect logic in AI-generated code. To bridge this gap, This paper introduces a novel scoring mechanism called the SBC score , which is based on a reverse generation technique that leverages the natural language generation capabilities of LLMs. Unlike direct code analysis, our approach reconstructs system requirements from AI-generated code and compares them with the original specifications to quantify accuracy. The SBC score combines semantic similarity, BLEU, and completeness analysis , providing actionable insights to developers by highlighting missing features and hallucinations. This hybrid metric not only improves the evaluation of AI-generated code but also offers a real-time, interpretable scoring system that can be integrated into the software development process, benefiting developers of all experience levels. Our code and datasets are available on GitHub: GitHub Repository.\",\n",
      "        0.6798123121261597\n",
      "    ],\n",
      "    [\n",
      "        \"The rise of Large Language Models (LLMs) in software engineering, particularly in code generation, has garnered significant attention. However, assessing the quality of AI-generated code remains a challenge due to the inherent complexity of programming tasks and the lack of robust evaluation metrics that align well with human judgment. Traditional token-based metrics such as BLEU and ROUGE, while commonly used in natural language processing, exhibit weak correlations with human assessments in code intelligence and verification tasks. Furthermore, these metrics are primarily research focused and are not designed for seamless integration into the software development lifecycle, limiting their practical utility for developers seeking to improve code quality and security.\",\n",
      "        0.6651515960693359\n",
      "    ],\n",
      "    [\n",
      "        \"Another key challenge is that AI-generated code is often more beneficial to experienced developers who can assess correctness, completeness, and security, whereas junior developers may struggle to critically evaluate and refine the generated code. Studies suggest that GitHub Copilot, one of the most widely used AI-powered coding assistants, provides significant advantages for senior developers but offers only limited value for junior developers [11, 9].\",\n",
      "        0.6572281718254089\n",
      "    ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "retrieved_lines_with_distances = [\n",
    "    (res[\"entity\"][\"text\"], res[\"distance\"]) for res in search_res[0]\n",
    "]\n",
    "print(json.dumps(retrieved_lines_with_distances, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8613c6a7-b781-4e48-a8af-41bb97b9f03c",
   "metadata": {},
   "source": [
    "# Augmented Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c45528a5-4cbf-4f88-a720-aeb614923d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\\n\".join(\n",
    "    [line_with_distance[0] for line_with_distance in retrieved_lines_with_distances]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "48a52e88-eeb0-4766-bfd2-c1c718f008ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "Human: You are an AI assistant. You are able to find answers to the questions from the contextual passage snippets provided.\n",
    "\"\"\"\n",
    "USER_PROMPT = f\"\"\"\n",
    "Use the following pieces of information enclosed in <context> tags to provide an answer to the question enclosed in <question> tags.\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f2d84dc7-b839-4f02-b72c-16b3c82d76c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The challenges of assessing the quality of AI-generated code include:\n",
      "\n",
      "1. **Inherent Complexity of Programming Tasks**: Programming tasks are complex, making it difficult to assess the quality of AI-generated code accurately.\n",
      "\n",
      "2. **Lack of Robust Evaluation Metrics**: There is a lack of robust evaluation metrics that align well with human judgment. Traditional token-based metrics like BLEU and ROUGE show weak correlations with human assessments in code intelligence and verification tasks.\n",
      "\n",
      "3. **Difficulty for Junior Developers**: Junior developers may find it hard to identify issues such as hallucinations, missing functionality, or incorrect logic in AI-generated code, while senior developers are generally more equipped to conduct such evaluations.\n",
      "\n",
      "Strategies for assessing AI-generated code:\n",
      "\n",
      "1. **SBC Score**: The paper introduces a novel scoring mechanism called the SBC score, which leverages a reverse generation technique to assess AI-generated code. It reconstructs system requirements from the generated code and compares them with the original specifications to quantify accuracy.\n",
      "\n",
      "2. **Hybrid Metric Approach**: The SBC score combines semantic similarity, BLEU, and completeness analysis, offering actionable insights by highlighting missing features and hallucinations.\n",
      "\n",
      "3. **Integration into the Software Development Lifecycle**: Providing a real-time, interpretable scoring system that can be integrated into the software development process benefits developers of all experience levels.\n",
      "\n",
      "4. **GitHub Repository**: The paper mentions that their code and datasets are available on GitHub, offering resources for developers to apply these strategies.\n"
     ]
    }
   ],
   "source": [
    "response = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": USER_PROMPT},\n",
    "    ],\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3976092f-fd56-44f0-9822-d073acd5a245",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
