{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ad2cc4e-31ec-4648-b0fe-6632f2bdbc36",
   "metadata": {},
   "source": [
    "## Working with an LLM programmatically\n",
    "\n",
    "You have certainly interacted before with a Large Language Model (LLM) like ChatGPT. This is usually done through a UI or an application.\n",
    "\n",
    "In this Notebook, we are going to use Python to connect and query an LLM directly through its API. For this Lab we have selected the model **Granite-3.1-8B-Instruct**.(https://huggingface.co/RedHatAI/granite-3.1-8b-instruct). This is a fully Open Source model (Apache 2.0 license) developed by IBM Research.\n",
    "\n",
    "This model has already been deployed on the Lab cluster because even if it's a smaller model, it still needs a GPU with 24GB of RAM to run..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4e2b81-0e10-4390-a7b8-5ddfda53a3e3",
   "metadata": {},
   "source": [
    "### Requirements and Imports\n",
    "\n",
    "If you have selected the right workbench image to launch as per the Lab's instructions, you should already have all the needed libraries. If not uncomment the first line in the next cell to install all the right packages. We will then import the libraries we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6491bcf-62d3-42c0-bdaa-e93c3d3dff82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.11.11 environment at: /opt/app-root\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m3 packages\u001b[0m \u001b[2min 5ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Install necessary libraries (run in a cell if needed)\n",
    "!uv pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d61c595d-967e-47de-a598-02b5d1ccec85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uncomment the following line only if you have not selected the right workbench image, or are using this notebook outside of the workshop environment.\n",
    "# !pip install --no-cache-dir --no-dependencies --disable-pip-version-check -r requirements.txt\n",
    "import json\n",
    "\n",
    "# HTTP client for API calls\n",
    "import httpx\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts.chat import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c428fbad-2345-4536-b687-72416d6b9b15",
   "metadata": {},
   "source": [
    "### Langchain\n",
    "\n",
    "Langchain (https://www.langchain.com/) is a framework for developing applications powered by language models. It will take care for us of all the boilerplate code we would have to manually write to properly query an LLM API.\n",
    "\n",
    "We will start by creating an **llm** instance, defined by the location where the LLM API can be queried and some parameters that will be applied to the model. For example, `max_new_tokens` will instruct the model to answer with a maximum of 512 tokens (words or parts of words). `temperature`, set really low here, will instruct the model to stay truth-grounded, and not try to be too \"creative\". After all, we're not trying to write a fancy poem here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77f95a70-89fb-4e21-a51c-24e862b7953e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LLM Inference Server URL\n",
    "inference_server_url = \"http://llama3-2-3b-predictor.llama-serving.svc.cluster.local:8080/v1\"\n",
    "inference_server_model_name = \"llama3-2-3b\"\n",
    "\n",
    "\n",
    "# LLM definition\n",
    "llm = ChatOpenAI(\n",
    "    model=inference_server_model_name,\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    api_key=\"EMPTY\",  # if you prefer to pass api key in directly instaed of using env vars\n",
    "    base_url=inference_server_url,\n",
    "    http_client=httpx.Client(verify=False)    # Because we are using an internal API endpoint (service) we need to disable SSL certificate checking.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b950bc-4d73-49e5-a35b-083a784edd50",
   "metadata": {},
   "source": [
    "We also need a **template** to be applied to every request we are sending to the model (the \"Prompt\").\n",
    "\n",
    "When querying a model, you almost never want to send directly what the user has typed. On top of this entry, you need to give proper instructions to the model so that it knows how to handle it: what and how to answer, what NOT to answer, the tone it must use..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8bb7517-faa2-43ed-a95d-835de975f916",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prompt template\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\n",
    "        \"\"\"You are a helpful, respectful, and honest assistant.\n",
    "        Answer each question clearly and concisely in a single response only.\n",
    "        Do not continue the conversation or simulate dialogue unless explicitly asked.\n",
    "        Never include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\n",
    "        Ensure that your responses are socially unbiased and positive in nature.\n",
    "        If a question does not make sense or is not factually coherent, explain why instead of trying to answer.\n",
    "        If you don't know the answer to a question, say \"I don't know\".\"\"\"\n",
    "    ),\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\"),\n",
    "])\n",
    "\n",
    "query = \"What is Artificial Intelligence?\"\n",
    "\n",
    "# âœ… Generate the prompt first\n",
    "prompt = template.invoke({\"input\": query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849fbd67-220c-4a02-8e4e-7e0d1aa91588",
   "metadata": {},
   "source": [
    "We are now ready to query the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca714bca-7cec-4afc-b275-fa389c05a993",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial Intelligence (AI) refers to the development of computer systems that can perform tasks that typically require human intelligence, such as:\n",
      "\n",
      "1. Learning: AI systems can learn from data and improve their performance over time.\n",
      "2. Problem-solving: AI systems can analyze problems and find solutions.\n",
      "3. Reasoning: AI systems can draw conclusions based on data and rules.\n",
      "4. Perception: AI systems can interpret and understand data from sensors and other sources.\n",
      "\n",
      "AI systems can be categorized into two main types:\n",
      "\n",
      "1. Narrow or Weak AI: Designed to perform a specific task, such as image recognition, speech recognition, or playing chess.\n",
      "2. General or Strong AI: A hypothetical AI system that possesses human-like intelligence and can perform any intellectual task.\n",
      "\n",
      "AI is used in various applications, including virtual assistants, self-driving cars, healthcare, finance, and more.\n"
     ]
    }
   ],
   "source": [
    "# Convert ChatPromptValue -> list of messages\n",
    "response = llm.invoke(prompt.to_messages())\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c37573-7917-4b32-bf21-9a280b31f3d4",
   "metadata": {},
   "source": [
    "Some information, like the tokens that were consumed and generated are present in the metadata. That can be useful to measure the consumption of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe2fb653-ef68-4817-8360-13a6692e948e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"input_tokens\": 154,\n",
      "  \"output_tokens\": 172,\n",
      "  \"total_tokens\": 326,\n",
      "  \"input_token_details\": {},\n",
      "  \"output_token_details\": {}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(response.usage_metadata, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0089476-bba0-4093-8be8-1469780afaba",
   "metadata": {},
   "source": [
    "---\n",
    "# End of activity\n",
    "Return to the Roadshow instructions for next steps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
