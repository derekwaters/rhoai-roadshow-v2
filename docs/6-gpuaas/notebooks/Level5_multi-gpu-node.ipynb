{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e687dc7-84a4-4761-ac03-921be2dc5b98",
   "metadata": {},
   "source": [
    "## Running a larger LLM on multiple GPU and multiple Nodes\n",
    "\n",
    "üë∑‚Äç‚ôÇÔ∏è Work In Progress üë∑‚Äç‚ôÇÔ∏è\n",
    "\n",
    "Based partly on the product documentation with enhancements. Useful Links\n",
    "\n",
    "- https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/2.21/html/serving_models/serving-large-models_serving-large-models#deploying-models-using-multiple-gpu-nodes_serving-large-models\n",
    "- https://access.redhat.com/articles/6966373\n",
    "- https://github.com/rh-aiservices-bu/multi-node-multi-gpu-poc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e783d636",
   "metadata": {},
   "source": [
    "### Configure RWX Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4c5e9e",
   "metadata": {},
   "source": [
    "Shared storage RWX is needed for deploying LLM's across nodes.\n",
    "\n",
    "Your cluster should already be configured to provide EFS via a Kubernetes Storage Class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c38052",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!oc login -u admin -p ${ADMIN_PASSWORD} --server=https://api.sno.${BASE_DOMAIN}:6443 --insecure-skip-tls-verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6281b4c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "$ oc get sc efs-sc\n",
    "NAME                   PROVISIONER       RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE\n",
    "efs-sc                 efs.csi.aws.com   Delete          Immediate              false                  35h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b642e03",
   "metadata": {},
   "source": [
    "You can create a test PVC to make sure EFS is working, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7228e6ef",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "oc apply -f- << EOF\n",
    "apiVersion: v1\n",
    "kind: PersistentVolumeClaim\n",
    "metadata:\n",
    "  name: test\n",
    "spec:\n",
    "  storageClassName: efs-sc\n",
    "  accessModes:\n",
    "    - ReadWriteMany\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: 1Gi\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8083a173",
   "metadata": {},
   "source": [
    "### Download Larger Model for Inference to Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564887e6",
   "metadata": {},
   "source": [
    "We are going to serve a larger LLM model across multiple GPUs and multiple Nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bc0117",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "oc new-project kserve-demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227d247f",
   "metadata": {},
   "source": [
    "We can choose any model that will fit on the the 2 GPUs nodes we have in our cluster. As a rough guide each billion FP32 model parameters will take up approx 4Gi of VRAM. Let's try the quantized version of Mistral-Small-24B-Instruct-2501 model - which should fit OK onto our 2x GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687dde55",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "-- https://huggingface.co/RedHatAI/Mistral-Small-24B-Instruct-2501-FP8-dynamic/tree/main\n",
    "\n",
    "export MODEL_PATH=mistral-small"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a454be4f",
   "metadata": {},
   "source": [
    "We need to create a PVC on EFS RWX storage - looking at the model safetensor files - we will need approx 30Gi size PVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b03590",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "oc apply -f- << EOF\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: PersistentVolumeClaim\n",
    "metadata:\n",
    "  name: ${MODEL_PATH}-pvc\n",
    "spec:\n",
    "  accessModes:\n",
    "    - ReadWriteMany\n",
    "  volumeMode: Filesystem\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: 30Gi\n",
    "  storageClassName: efs-sc\n",
    "EOF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e6a3e4",
   "metadata": {},
   "source": [
    "Lets grab a YAML file that will help us download the Hugging Face model to a PVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8c85f2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "curl -o download-model-to-pvc.yaml https://raw.githubusercontent.com/eformat/rhoai-policy-collection/refs/heads/main/gitops/applications/model-download/download-model-to-pvc.yaml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680811a2",
   "metadata": {},
   "source": [
    "Export the following environment variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a6c192",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "export PVC_CLAIM_NAME=${MODEL_PATH}-pvc\n",
    "export HF_TOKEN=hf_...\n",
    "export MODEL=RedHatAI/Mistral-Small-24B-Instruct-2501-FP8-dynamic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7906d5c8",
   "metadata": {},
   "source": [
    "Now create the downloader pod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b60e4ff",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "envsubst < download-model-to-pvc.yaml | oc create -n kserve-demo -f-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d347ad",
   "metadata": {},
   "source": [
    "Follow the pod logs to ensure the model downloads as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c569fd",
   "metadata": {},
   "source": [
    "### Create the Inference Service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1542a39c",
   "metadata": {},
   "source": [
    "OpenShift comes with Templates to help run vLLM in Multi Node. Let's create the ServingRuntime using the Template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a486c199",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "oc process vllm-multinode-runtime-template -n redhat-ods-applications | oc apply -n kserve-demo -f-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de0f162",
   "metadata": {},
   "source": [
    "The ServingRuntime resources need to be pruned (as the over allocate resources)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921563a5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "-- Serving Runtime ... prune resources HARD!\n",
    "spec:\n",
    "  containers:\n",
    "    - resources:\n",
    "        requests:\n",
    "          cpu: '1'\n",
    "          memory: 2Gi\n",
    "      readinessProbe:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd5ac2c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "export INFERENCE_NAME=$MODEL_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc42451",
   "metadata": {},
   "source": [
    "Now create the InferenceService that points the `storageUri` to the PVC we downloaded the model to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f36842f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "oc apply -f- << EOF\n",
    "apiVersion: serving.kserve.io/v1beta1\n",
    "kind: InferenceService\n",
    "metadata:\n",
    "  annotations:\n",
    "    serving.kserve.io/deploymentMode: RawDeployment\n",
    "    serving.kserve.io/autoscalerClass: external\n",
    "  name: ${INFERENCE_NAME}\n",
    "spec:\n",
    "  predictor:\n",
    "    model:\n",
    "      modelFormat:\n",
    "        name: vLLM\n",
    "      runtime: vllm-multinode-runtime\n",
    "      storageUri: pvc://${PVC_CLAIM_NAME}/${MODEL_PATH}\n",
    "    workerSpec: {}\n",
    "    tolerations:\n",
    "      - effect: NoSchedule\n",
    "        key: nvidia.com/gpu\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599bdf1d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919e7a00",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:25:53 [ray_distributed_executor.py:357] If certain env vars should NOT be copied to workers, add them to /tmp/.config/vllm/ray_non_carry_over_env_vars.json file\n",
    "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]pid=912) \n",
    "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:10<00:50, 10.05s/it] \n",
    "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:10<00:16,  4.24s/it] \n",
    "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:13<00:10,  3.61s/it] \n",
    "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:20<00:10,  5.01s/it] \n",
    "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:22<00:04,  4.04s/it] \n",
    "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:32<00:00,  6.14s/it] \n",
    "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:32<00:00,  5.46s/it]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef65e23",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:28] Available routes are:\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /openapi.json, Methods: HEAD, GET\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /docs, Methods: HEAD, GET\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /redoc, Methods: HEAD, GET\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /health, Methods: GET\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /load, Methods: GET\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /ping, Methods: POST\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /ping, Methods: GET\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /tokenize, Methods: POST\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /detokenize, Methods: POST\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /v1/models, Methods: GET\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /version, Methods: GET\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /v1/chat/completions, Methods: POST\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /v1/completions, Methods: POST\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /v1/embeddings, Methods: POST\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /pooling, Methods: POST\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /classify, Methods: POST\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /score, Methods: POST\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /v1/score, Methods: POST\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /v1/audio/transcriptions, Methods: POST\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /rerank, Methods: POST\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /v1/rerank, Methods: POST\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /v2/rerank, Methods: POST\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /invocations, Methods: POST\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /metrics, Methods: GET\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO:     Started server process [1]\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO:     Waiting for application startup.\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO:     Application startup complete.\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO:     10.128.0.218:55526 - \"GET /metrics HTTP/1.1\" 200 OK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62317f19",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "$ oc get pods -o wide\n",
    "NAME                                              READY   STATUS      RESTARTS   AGE     IP             NODE                                       NOMINATED NODE   READINESS GATES\n",
    "download-model                                    0/1     Completed   0          6h9m    10.129.0.47    ip-10-0-37-35.us-east-2.compute.internal   <none>           <none>\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm          1/1     Running     0          6m10s   10.128.1.222   ip-10-0-40-85.us-east-2.compute.internal   <none>           <none>\n",
    "mistral-small-predictor-worker-7489767864-l6mfg   1/1     Running     0          6m10s   10.129.0.163   ip-10-0-37-35.us-east-2.compute.internal   <none>           <none>\n",
    "tools-56447bb8b-27wsl                             1/1     Running     0          6h9m    10.129.0.45    ip-10-0-37-35.us-east-2.compute.internal   <none>           <none>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbcc612",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "DEMO_NAMESPACE=kserve-demo\n",
    "export MODEL_NAME=mistral-small\n",
    "\n",
    "podName=$(oc get pod -n $DEMO_NAMESPACE -l app=isvc.$MODEL_NAME-predictor --no-headers|cut -d' ' -f1)\n",
    "workerPodName=$(kubectl get pod -n $DEMO_NAMESPACE -l app=isvc.$MODEL_NAME-predictor-worker --no-headers|cut -d' ' -f1)\n",
    "\n",
    "oc -n $DEMO_NAMESPACE wait --for=condition=ready pod/${podName} --timeout=300s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810b0c1f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "$ echo \"### HEAD NODE GPU Memory Size\"\n",
    "kubectl -n $DEMO_NAMESPACE exec $podName -- nvidia-smi\n",
    "echo \"### Worker NODE GPU Memory Size\"\n",
    "kubectl -n $DEMO_NAMESPACE exec $workerPodName -- nvidia-smi\n",
    "### HEAD NODE GPU Memory Size\n",
    "Defaulted container \"kserve-container\" out of: kserve-container, ray-tls-generator (init)\n",
    "Sun Jul  6 08:31:40 2025       \n",
    "+-----------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |\n",
    "|-----------------------------------------+------------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                        |               MIG M. |\n",
    "|=========================================+========================+======================|\n",
    "|   0  NVIDIA L4                      On  |   00000000:36:00.0 Off |                    0 |\n",
    "| N/A   54C    P0             35W /   72W |   20252MiB /  23034MiB |      0%      Default |\n",
    "|                                         |                        |                  N/A |\n",
    "+-----------------------------------------+------------------------+----------------------+\n",
    "                                                                                         \n",
    "+-----------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                              |\n",
    "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
    "|        ID   ID                                                               Usage      |\n",
    "|=========================================================================================|\n",
    "|    0   N/A  N/A             912      C   ray::RayWorkerWrapper                 20244MiB |\n",
    "+-----------------------------------------------------------------------------------------+\n",
    "### Worker NODE GPU Memory Size\n",
    "Defaulted container \"worker-container\" out of: worker-container, ray-tls-generator (init)\n",
    "Sun Jul  6 08:31:43 2025       \n",
    "+-----------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |\n",
    "|-----------------------------------------+------------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                        |               MIG M. |\n",
    "|=========================================+========================+======================|\n",
    "|   0  NVIDIA A10G                    On  |   00000000:00:1E.0 Off |                    0 |\n",
    "|  0%   45C    P0             98W /  300W |   20775MiB /  23028MiB |      0%      Default |\n",
    "|                                         |                        |                  N/A |\n",
    "+-----------------------------------------+------------------------+----------------------+\n",
    "                                                                                         \n",
    "+-----------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                              |\n",
    "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
    "|        ID   ID                                                               Usage      |\n",
    "|=========================================================================================|\n",
    "|    0   N/A  N/A             238      C   ray::RayWorkerWrapper                 20766MiB |\n",
    "+-----------------------------------------------------------------------------------------+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f274754a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "oc apply -f- << EOF\n",
    "kind: Route\n",
    "apiVersion: route.openshift.io/v1\n",
    "metadata:\n",
    "  name: ${INFERENCE_NAME}\n",
    "  labels:\n",
    "    app: isvc.${INFERENCE_NAME}-predictor\n",
    "    component: predictor\n",
    "    isvc.generation: \"1\"\n",
    "    serving.kserve.io/inferenceservice: ${INFERENCE_NAME}\n",
    "  annotations:\n",
    "    openshift.io/host.generated: \"true\"\n",
    "spec:\n",
    "  to:\n",
    "    kind: Service\n",
    "    name: ${INFERENCE_NAME}-predictor\n",
    "    weight: 100\n",
    "  port:\n",
    "    targetPort: http\n",
    "  tls:\n",
    "    termination: edge\n",
    "    insecureEdgeTerminationPolicy: Redirect\n",
    "  wildcardPolicy: None\n",
    "EOF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea2d0a8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "oc wait --for=condition=ready pod/${podName} -n $DEMO_NAMESPACE --timeout=300s\n",
    "export isvc_url=$(oc get route -n $DEMO_NAMESPACE |grep $MODEL_NAME| awk '{print $2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e84b1bd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "curl https://$isvc_url/v1/completions \\\n",
    "   -H \"Content-Type: application/json\" \\\n",
    "   -d \"{\n",
    "        \\\"model\\\": \\\"$MODEL_NAME\\\",\n",
    "        \\\"prompt\\\": \\\"What is the biggest mountain in the world?\\\",\n",
    "        \\\"max_tokens\\\": 100,\n",
    "        \\\"temperature\\\": 0\n",
    "    }\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e6c3e3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\"id\":\"cmpl-3a23f5db101f416192910105c6036cc8\",\"object\":\"text_completion\",\"created\":1751791369,\"model\":\"mistral-small\",\"choices\":[{\"index\":0,\"text\":\" The answer is not Mount Everest. The biggest mountain in the world is actually Mauna Kea in Hawaii. Mauna Kea is a dormant volcano that rises 13,796 feet (4,205 meters) above sea level, but it is also 19,680 feet (6,000 meters) tall when measured from its base on the ocean floor. This makes it the tallest mountain in the world when measured from base to peak\",\"logprobs\":null,\"finish_reason\":\"length\",\"stop_reason\":null,\"prompt_logprobs\":null}],\"usage\":{\"prompt_tokens\":10,\"total_tokens\":110,\"completion_tokens\":100,\"prompt_tokens_details\":null},\"kv_transfer_params\":null}virt:~/git/multi-node-multi-gpu-poc ‚éá main#cb26faa$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0233fcc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "oc exec -i pod/${podName} -- /bin/sh -s << EOF\n",
    "ray status\n",
    "EOF\n",
    "\n",
    "Defaulted container \"kserve-container\" out of: kserve-container, ray-tls-generator (init)\n",
    "======== Autoscaler status: 2025-07-06 08:44:21.676093 ========\n",
    "Node status\n",
    "---------------------------------------------------------------\n",
    "Active:\n",
    " 1 node_8a3ca93eb4c1f37584480f0c5611a3a46a998ce028ad0bfd8910e793\n",
    " 1 node_5e6faa16c3d7c131993ab9d79177f7a4d004b08961024a4c3a221fb3\n",
    "Pending:\n",
    " (no pending nodes)\n",
    "Recent failures:\n",
    " (no failures)\n",
    "\n",
    "Resources\n",
    "---------------------------------------------------------------\n",
    "Total Usage:\n",
    " 0.0/36.0 CPU\n",
    " 2.0/2.0 GPU (2.0 used of 2.0 reserved in placement groups)\n",
    " 0B/121.49GiB memory\n",
    " 0B/13.91GiB object_store_memory\n",
    "\n",
    "Total Constraints:\n",
    " (no request_resources() constraints)\n",
    "Total Demands:\n",
    " (no resource demands)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
