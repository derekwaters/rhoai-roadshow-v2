{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e687dc7-84a4-4761-ac03-921be2dc5b98",
   "metadata": {},
   "source": [
    "## Running a larger LLM on multiple GPU and multiple Nodes\n",
    "\n",
    "üë∑‚Äç‚ôÇÔ∏è Work In Progress üë∑‚Äç‚ôÇÔ∏è\n",
    "\n",
    "This lab has been tested to work OK - it just needs a bit more documentation, explanation and debug for the cells. \n",
    "\n",
    "If you jumped to here from Level4 notebook then carry on ! ü™è\n",
    "\n",
    "The notebook is based partly on the product documentation with some enhancements. Some useful Links.\n",
    "\n",
    "- https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/2.21/html/serving_models/serving-large-models_serving-large-models#deploying-models-using-multiple-gpu-nodes_serving-large-models\n",
    "- https://access.redhat.com/articles/6966373\n",
    "- https://github.com/rh-aiservices-bu/multi-node-multi-gpu-poc\n",
    "\n",
    "### GPU Aggregation Overview\n",
    "\n",
    "Compute workloads can benefit from using separate GPU partitions. The flexibility of GPU partitioning allows a single GPU to be shared and used by small, medium, and large-sized workloads. GPU partitions can be a valid option for executing Deep Learning workloads. An example is Deep Learning training and inferencing workflows, which utilize smaller datasets but are highly dependent on the size of the data/model, and users may need to decrease batch sizes.\n",
    "\n",
    "#### Why GPU Aggregation?\n",
    "\n",
    "Some Large Language Models (LLMs), such as Llama-3-70B and Falcon 180B, can be too large to fit into the memory of a single GPU (vRAM). Or in some cases, GPUs that would be large-enough might be difficult to obtain. If you find yourself in such a situation, it is natural to wonder whether an aggregation of multiple, smaller GPUs can be used instead of one single large GPU.\n",
    "\n",
    "Thankfully, the answer is essentially Yes. To address these challenges, we can use more advanced configurations to distribute the LLM workload across several GPUs. One option is leveraging tensor parallelism, where the LLM is split across several GPUs, with each GPU processing a portion of the model's tensors. This approach ensures efficient utilization of available resources (GPUs) across one or several workers.\n",
    "\n",
    "Some Serving Runtimes, such as vLLM, support tensor parallelism, allowing for both single-worker and multi-worker configurations (the difference whether your GPUs are all in the same machine, or are spread across machines).\n",
    "\n",
    "#### Components of GPU Aggregation\n",
    "\n",
    "GPU Aggregation is a complex topic and there are many components to consider. Fundamentally there are four core concepts to consider: \n",
    "\n",
    "* Tensor Parallelism\n",
    "* Pipeline Parallelism\n",
    "* Data Parallelism\n",
    "* Expert Parallelism\n",
    "\n",
    "<img src=\"images/gpu-aggregation.png\"\n",
    "     alt=\"GPU Aggregation\"\n",
    "     style=\"width:75%;\">\n",
    "\n",
    "### In this lab\n",
    "\n",
    "We are going to deploy a larger LLM across both our GPU enabled nodes. This needs both GPUs in full to run.\n",
    "\n",
    "#### üí° Free up GPU memory to run this exercise\n",
    "\n",
    "In this lab environment we can remove the running LLM models using the configmap mechanism that controls the model deployments using Policy-as-Code to undeploy the existing lab models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff23753d-11ce-4490-93a0-64203c152bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!oc create configmap undeploy-sno-deepseek-qwen3-vllm -n llama-serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edab51c-7da7-409b-a0b8-d23a57cc2dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!oc create configmap undeploy-llama3-2-3b -n llama-serving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e21e6d6",
   "metadata": {},
   "source": [
    "**Note:** If you need to redeploy these models then simply delete the config maps. The order matters for correct startup. Deploy Llama3 first. We have limited GPU NVRAM so we use vLLMs `gpu_memory_utilization` parameter when loading the models. This works on available GPU memory so we need to load Llama3 (gpu_memory_utilization=0.5) first then DeepSeek (gpu_memory_utilization=0.8) second.\n",
    "\n",
    "```bash\n",
    "# 1 - redeploy llama3-2-3b\n",
    "oc delete configmap undeploy-llama3-2-3b -n llama-serving\n",
    "# 2 - redeploy deepseek-qwen3\n",
    "oc delete configmap undeploy-sno-deepseek-qwen3-vllm -n llama-serving\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e783d636",
   "metadata": {},
   "source": [
    "### Configure RWX Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83c38052",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Using insecure TLS client config. Setting this option is not supported!\n",
      "\n",
      "Login successful.\n",
      "\n",
      "You have access to 116 projects, the list has been suppressed. You can list all projects with 'oc projects'\n",
      "\n",
      "Using project \"kserve-demo\".\n"
     ]
    }
   ],
   "source": [
    "!oc login -u admin -p ${ADMIN_PASSWORD} --server=https://api.sno.${BASE_DOMAIN}:6443 --insecure-skip-tls-verify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2f4524",
   "metadata": {},
   "source": [
    "Check we have the **efs-sc** storage class configured. If not - check with your cluster admin !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18d0d6ca",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME     PROVISIONER       RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE\n",
      "efs-sc   efs.csi.aws.com   Delete          Immediate           false                  3d1h\n"
     ]
    }
   ],
   "source": [
    "!oc get sc efs-sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8083a173",
   "metadata": {},
   "source": [
    "### Download Larger Model for Inference to Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2e0091-745d-4c8a-a6eb-0328e921bca1",
   "metadata": {},
   "source": [
    "For demonstration purpoeses - let's select a model that we know will not fit on our single 24Gi GPU. Lets try RedHatAI/Mistral-Small-24B-Instruct-2501-FP8-dynamic which is a good quality quantized model that has ~30Gi of safetensor weights and will also need KV cache - so will definitely not fit on our single GPU.\n",
    "\n",
    "https://huggingface.co/RedHatAI/Mistral-Small-24B-Instruct-2501-FP8-dynamic/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47bc0117",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error from server (AlreadyExists): project.project.openshift.io \"kserve-demo\" already exists\n"
     ]
    }
   ],
   "source": [
    "!oc new-project kserve-demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a454be4f",
   "metadata": {},
   "source": [
    "create ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6b03590",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MODEL_PATH=mistral-small\n"
     ]
    }
   ],
   "source": [
    "%env MODEL_PATH=mistral-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "981d5405-b15b-4485-a9b6-66afc1134fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "persistentvolumeclaim/mistral-small-pvc unchanged\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "oc apply -f- << EOF\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: PersistentVolumeClaim\n",
    "metadata:\n",
    "  name: ${MODEL_PATH}-pvc\n",
    "spec:\n",
    "  accessModes:\n",
    "    - ReadWriteMany\n",
    "  volumeMode: Filesystem\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: 30Gi\n",
    "  storageClassName: efs-sc\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111eb33f-0a74-4e18-8542-8a4dfee3bf43",
   "metadata": {},
   "source": [
    "Lets grab a YAML file that will help us download the Hugging Face model to a PVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9495d158-b223-4a0e-b3fb-ffcf07b5cf36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  1552  100  1552    0     0   4958      0 --:--:-- --:--:-- --:--:--  4958\n"
     ]
    }
   ],
   "source": [
    "!curl -o download-model-to-pvc.yaml https://raw.githubusercontent.com/eformat/rhoai-policy-collection/refs/heads/main/gitops/applications/model-download/download-model-to-pvc.yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c9b209-609d-4465-ba75-518f3ae5326e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env PVC_CLAIM_NAME=mistral-small-pvc\n",
    "%env HF_TOKEN=hf_...\n",
    "%env MODEL=RedHatAI/Mistral-Small-24B-Instruct-2501-FP8-dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b60e4ff",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod/download-model created\n"
     ]
    }
   ],
   "source": [
    "!cat download-model-to-pvc.yaml | envsubst | oc apply -f-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dde7fc3-44e2-48c3-8c18-60f6883c2c05",
   "metadata": {},
   "source": [
    "Wait until pod completes successfully ~apprx 6-8min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c569fd",
   "metadata": {},
   "source": [
    "### Create Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bd3f07-edb0-4d9c-b95e-08649caf57ed",
   "metadata": {},
   "source": [
    "RHOAI comes with the templates needed to run multinode multigpu, lets use them to create the ServingRuntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a486c199",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "oc process vllm-multinode-runtime-template -n redhat-ods-applications | oc apply -n kserve-demo -f-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cd5ac2c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: INFERENCE_NAME=mistral-small\n",
      "env: MODEL_PATH=mistrall-small\n"
     ]
    }
   ],
   "source": [
    "%env INFERENCE_NAME=mistral-small\n",
    "%env MODEL_PATH=mistrall-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f36842f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "oc apply -f- << EOF\n",
    "apiVersion: serving.kserve.io/v1beta1\n",
    "kind: InferenceService\n",
    "metadata:\n",
    "  annotations:\n",
    "    serving.kserve.io/deploymentMode: RawDeployment\n",
    "    serving.kserve.io/autoscalerClass: external\n",
    "  name: ${INFERENCE_NAME}\n",
    "spec:\n",
    "  predictor:\n",
    "    model:\n",
    "      modelFormat:\n",
    "        name: vLLM\n",
    "      runtime: vllm-multinode-runtime\n",
    "      storageUri: pvc://${PVC_CLAIM_NAME}/${MODEL_PATH}\n",
    "    workerSpec: {}\n",
    "    tolerations:\n",
    "      - effect: NoSchedule\n",
    "        key: nvidia.com/gpu\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eaed35-a722-430d-ab99-bf154aef9dc6",
   "metadata": {},
   "source": [
    "The templates have hard limits which are pretty excessive for our resources, trim them so we only set QoS to burstable i.e. set requests only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599bdf1d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "-- Serving Runtime ... prune resources HARD!\n",
    "spec:\n",
    "  containers:\n",
    "    - resources:\n",
    "        requests:\n",
    "          cpu: '1'\n",
    "          memory: 2Gi\n",
    "      readinessProbe:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55347e3-74ef-48aa-a68f-962e8c3605fc",
   "metadata": {},
   "source": [
    "Tail the logs on the inference pod, we should see the safetensor shards loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5c2ba0-15cc-4e35-a7e8-9b2bed0bf5ae",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```bash\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:25:53 [ray_distributed_executor.py:357] If certain env vars should NOT be copied to workers, add them to /tmp/.config/vllm/ray_non_carry_over_env_vars.json file\n",
    "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]pid=912) \n",
    "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:10<00:50, 10.05s/it] \n",
    "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:10<00:16,  4.24s/it] \n",
    "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:13<00:10,  3.61s/it] \n",
    "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:20<00:10,  5.01s/it] \n",
    "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:22<00:04,  4.04s/it] \n",
    "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:32<00:00,  6.14s/it] \n",
    "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:32<00:00,  5.46s/it]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a380609-dc0e-46cd-b493-668c3e9dd025",
   "metadata": {},
   "source": [
    "After some time the OpenAI API becomes ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998911c8-4d4a-4a7d-8f50-3cd62a75fe2c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```bash\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:28] Available routes are:\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /openapi.json, Methods: HEAD, GET\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /docs, Methods: HEAD, GET\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /redoc, Methods: HEAD, GET\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /health, Methods: GET\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /load, Methods: GET\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /ping, Methods: POST\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /ping, Methods: GET\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /tokenize, Methods: POST\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /detokenize, Methods: POST\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /v1/models, Methods: GET\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /version, Methods: GET\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /v1/chat/completions, Methods: POST\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /v1/completions, Methods: POST\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /v1/embeddings, Methods: POST\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /pooling, Methods: POST\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /classify, Methods: POST\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /score, Methods: POST\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /v1/score, Methods: POST\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /v1/audio/transcriptions, Methods: POST\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /rerank, Methods: POST\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /v1/rerank, Methods: POST\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /v2/rerank, Methods: POST\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /invocations, Methods: POST\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO 07-06 08:28:20 [launcher.py:36] Route: /metrics, Methods: GET\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO:     Started server process [1]\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO:     Waiting for application startup.\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO:     Application startup complete.\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm kserve-container INFO:     10.128.0.218:55526 - \"GET /metrics HTTP/1.1\" 200 OK\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbca3ec-55b9-4481-9848-3e0a6758d97f",
   "metadata": {},
   "source": [
    "Check Pod Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753caa9e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!oc get pods -o wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62317f19",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "NAME                                              READY   STATUS      RESTARTS   AGE     IP             NODE                                       NOMINATED NODE   READINESS GATES\n",
    "download-model                                    0/1     Completed   0          6h9m    10.129.0.47    ip-10-0-37-35.us-east-2.compute.internal   <none>           <none>\n",
    "mistral-small-predictor-5dbf9cbd8d-gbpbm          1/1     Running     0          6m10s   10.128.1.222   ip-10-0-40-85.us-east-2.compute.internal   <none>           <none>\n",
    "mistral-small-predictor-worker-7489767864-l6mfg   1/1     Running     0          6m10s   10.129.0.163   ip-10-0-37-35.us-east-2.compute.internal   <none>           <none>\n",
    "tools-56447bb8b-27wsl                             1/1     Running     0          6h9m    10.129.0.45    ip-10-0-37-35.us-east-2.compute.internal   <none>           <none>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b74949a-53e6-4b12-9693-ae6d1bd72901",
   "metadata": {},
   "source": [
    "We can also check nvidia-smi for GPU NVRAM usage stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353d7834",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%env DEMO_NAMESPACE=kserve-demo\n",
    "%env MODEL_NAME=mistral-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbcc612",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!podName=$(oc get pod -n $DEMO_NAMESPACE -l app=isvc.$MODEL_NAME-predictor --no-headers|cut -d' ' -f1)\n",
    "!workerPodName=$(kubectl get pod -n $DEMO_NAMESPACE -l app=isvc.$MODEL_NAME-predictor-worker --no-headers|cut -d' ' -f1)\n",
    "!oc -n $DEMO_NAMESPACE wait --for=condition=ready pod/${podName} --timeout=300s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd8e5a8-5c2a-442d-846e-dfc4194d88df",
   "metadata": {},
   "source": [
    "We can see model loaded across both of out GPU nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a10786",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!echo \"### HEAD NODE GPU Memory Size\"\n",
    "!oc -n $DEMO_NAMESPACE exec $podName -- nvidia-smi\n",
    "!echo \"### Worker NODE GPU Memory Size\"\n",
    "!oc -n $DEMO_NAMESPACE exec $workerPodName -- nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810b0c1f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "### HEAD NODE GPU Memory Size\n",
    "Defaulted container \"kserve-container\" out of: kserve-container, ray-tls-generator (init)\n",
    "Sun Jul  6 08:31:40 2025       \n",
    "+-----------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |\n",
    "|-----------------------------------------+------------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                        |               MIG M. |\n",
    "|=========================================+========================+======================|\n",
    "|   0  NVIDIA L4                      On  |   00000000:36:00.0 Off |                    0 |\n",
    "| N/A   54C    P0             35W /   72W |   20252MiB /  23034MiB |      0%      Default |\n",
    "|                                         |                        |                  N/A |\n",
    "+-----------------------------------------+------------------------+----------------------+\n",
    "                                                                                         \n",
    "+-----------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                              |\n",
    "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
    "|        ID   ID                                                               Usage      |\n",
    "|=========================================================================================|\n",
    "|    0   N/A  N/A             912      C   ray::RayWorkerWrapper                 20244MiB |\n",
    "+-----------------------------------------------------------------------------------------+\n",
    "### Worker NODE GPU Memory Size\n",
    "Defaulted container \"worker-container\" out of: worker-container, ray-tls-generator (init)\n",
    "Sun Jul  6 08:31:43 2025       \n",
    "+-----------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |\n",
    "|-----------------------------------------+------------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                        |               MIG M. |\n",
    "|=========================================+========================+======================|\n",
    "|   0  NVIDIA A10G                    On  |   00000000:00:1E.0 Off |                    0 |\n",
    "|  0%   45C    P0             98W /  300W |   20775MiB /  23028MiB |      0%      Default |\n",
    "|                                         |                        |                  N/A |\n",
    "+-----------------------------------------+------------------------+----------------------+\n",
    "                                                                                         \n",
    "+-----------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                              |\n",
    "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
    "|        ID   ID                                                               Usage      |\n",
    "|=========================================================================================|\n",
    "|    0   N/A  N/A             238      C   ray::RayWorkerWrapper                 20766MiB |\n",
    "+-----------------------------------------------------------------------------------------+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca8893e-03dd-417b-b0ca-ef0da1ddb8e3",
   "metadata": {},
   "source": [
    "Lets create a route so we can test the inference endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea8a3c3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "oc apply -f- << EOF\n",
    "kind: Route\n",
    "apiVersion: route.openshift.io/v1\n",
    "metadata:\n",
    "  name: ${INFERENCE_NAME}\n",
    "  labels:\n",
    "    app: isvc.${INFERENCE_NAME}-predictor\n",
    "    component: predictor\n",
    "    isvc.generation: \"1\"\n",
    "    serving.kserve.io/inferenceservice: ${INFERENCE_NAME}\n",
    "  annotations:\n",
    "    openshift.io/host.generated: \"true\"\n",
    "spec:\n",
    "  to:\n",
    "    kind: Service\n",
    "    name: ${INFERENCE_NAME}-predictor\n",
    "    weight: 100\n",
    "  port:\n",
    "    targetPort: http\n",
    "  tls:\n",
    "    termination: edge\n",
    "    insecureEdgeTerminationPolicy: Redirect\n",
    "  wildcardPolicy: None\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea2d0a8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "oc wait --for=condition=ready pod/${podName} -n $DEMO_NAMESPACE --timeout=300s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de2201e-afd2-4481-8c91-c419d8004273",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env isvc_url=$(oc get route -n $DEMO_NAMESPACE |grep $MODEL_NAME| awk '{print $2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7fe109-c266-476f-9c8c-a1c1d1158488",
   "metadata": {},
   "source": [
    "Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c523800",
   "metadata": {},
   "outputs": [],
   "source": [
    "curl https://$isvc_url/v1/completions \\\n",
    "   -H \"Content-Type: application/json\" \\\n",
    "   -d \"{\n",
    "        \\\"model\\\": \\\"$MODEL_NAME\\\",\n",
    "        \\\"prompt\\\": \\\"What is the biggest mountain in the world?\\\",\n",
    "        \\\"max_tokens\\\": 100,\n",
    "        \\\"temperature\\\": 0\n",
    "    }\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf66369-5f67-4022-827e-7ed0ef33d6b2",
   "metadata": {},
   "source": [
    "Mauna Kea indeed !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e6c3e3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\"id\":\"cmpl-3a23f5db101f416192910105c6036cc8\",\"object\":\"text_completion\",\"created\":1751791369,\"model\":\"mistral-small\",\"choices\":[{\"index\":0,\"text\":\" The answer is not Mount Everest. The biggest mountain in the world is actually Mauna Kea in Hawaii. Mauna Kea is a dormant volcano that rises 13,796 feet (4,205 meters) above sea level, but it is also 19,680 feet (6,000 meters) tall when measured from its base on the ocean floor. This makes it the tallest mountain in the world when measured from base to peak\",\"logprobs\":null,\"finish_reason\":\"length\",\"stop_reason\":null,\"prompt_logprobs\":null}],\"usage\":{\"prompt_tokens\":10,\"total_tokens\":110,\"completion_tokens\":100,\"prompt_tokens_details\":null},\"kv_transfer_params\":null}virt:~/git/multi-node-multi-gpu-poc ‚éá main#cb26faa$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d3d587-1877-4ae6-a277-5193d91abe9f",
   "metadata": {},
   "source": [
    "Ray is used internally by vLLM and we can check its status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb58552",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "oc exec -i pod/${podName} -- /bin/sh -s << EOF\n",
    "ray status\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0233fcc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Defaulted container \"kserve-container\" out of: kserve-container, ray-tls-generator (init)\n",
    "======== Autoscaler status: 2025-07-06 08:44:21.676093 ========\n",
    "Node status\n",
    "---------------------------------------------------------------\n",
    "Active:\n",
    " 1 node_8a3ca93eb4c1f37584480f0c5611a3a46a998ce028ad0bfd8910e793\n",
    " 1 node_5e6faa16c3d7c131993ab9d79177f7a4d004b08961024a4c3a221fb3\n",
    "Pending:\n",
    " (no pending nodes)\n",
    "Recent failures:\n",
    " (no failures)\n",
    "\n",
    "Resources\n",
    "---------------------------------------------------------------\n",
    "Total Usage:\n",
    " 0.0/36.0 CPU\n",
    " 2.0/2.0 GPU (2.0 used of 2.0 reserved in placement groups)\n",
    " 0B/121.49GiB memory\n",
    " 0B/13.91GiB object_store_memory\n",
    "\n",
    "Total Constraints:\n",
    " (no request_resources() constraints)\n",
    "Total Demands:\n",
    " (no resource demands)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
